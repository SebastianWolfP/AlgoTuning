{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50aa7fb1",
   "metadata": {},
   "source": [
    "# Optimierungsanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa91748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: Features + Target in einer Datei\n",
    "train_df = pd.read_csv(\"Data/preprocessed/train_preprocessed.csv\")\n",
    "train_X = train_df.drop(columns=[\"AdoptionSpeed\"])\n",
    "train_y = train_df[\"AdoptionSpeed\"].astype(int).values\n",
    "\n",
    "# Valid: Features und Target getrennt\n",
    "valid_X = pd.read_csv(\"Data/preprocessed/valid_preprocessed.csv\")\n",
    "# Nur die Target-Spalte laden\n",
    "valid_y = pd.read_csv(\"Data/preprocessed/valid_target.csv\")[\"AdoptionSpeed\"].astype(int).values.ravel()\n",
    "\n",
    "# Test: Features und Target getrennt\n",
    "test_X = pd.read_csv(\"Data/preprocessed/test_preprocessed.csv\")\n",
    "test_y = pd.read_csv(\"Data/preprocessed/test_target.csv\")[\"AdoptionSpeed\"].astype(int).values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc308d2",
   "metadata": {},
   "source": [
    "## 1. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50de9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombiniere Train + Valid (für RandomizedSearchCV)\n",
    "X_train = pd.concat([train_X, valid_X], axis=0)\n",
    "y_train = np.concatenate([train_y, valid_y], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55577716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softmax\",   # Multiklassifikation\n",
    "    num_class=len(np.unique(y_train)),  # Anzahl Klassen automatisch bestimmen\n",
    "    eval_metric=\"mlogloss\",\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5afc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter-Suchraum\n",
    "param_dist = {\n",
    "    \"max_depth\": np.arange(3, 11),\n",
    "    \"learning_rate\": np.linspace(0.01, 0.3, 30),\n",
    "    \"subsample\": np.linspace(0.5, 1.0, 6),\n",
    "    \"n_estimators\": np.arange(100, 1001, 100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2da2e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=[\"PetID\"])\n",
    "valid_X = valid_X.drop(columns=[\"PetID\"])\n",
    "test_X  = test_X.drop(columns=[\"PetID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "986fec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faulh\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:49:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, device=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=&#x27;mlogloss&#x27;,\n",
       "                                           feature_types=None,\n",
       "                                           feature_weights=None, gamma=None,\n",
       "                                           grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_con...\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
       "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
       "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ]),\n",
       "                                        &#x27;max_depth&#x27;: array([ 3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;n_estimators&#x27;: array([ 100,  200,  300,  400,  500,  600,  700,  800,  900, 1000]),\n",
       "                                        &#x27;subsample&#x27;: array([0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "                   random_state=42, scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, device=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=&#x27;mlogloss&#x27;,\n",
       "                                           feature_types=None,\n",
       "                                           feature_weights=None, gamma=None,\n",
       "                                           grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_con...\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
       "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
       "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ]),\n",
       "                                        &#x27;max_depth&#x27;: array([ 3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;n_estimators&#x27;: array([ 100,  200,  300,  400,  500,  600,  700,  800,  900, 1000]),\n",
       "                                        &#x27;subsample&#x27;: array([0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "                   random_state=42, scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None, num_class=5, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None, num_class=5, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, device=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric='mlogloss',\n",
       "                                           feature_types=None,\n",
       "                                           feature_weights=None, gamma=None,\n",
       "                                           grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_con...\n",
       "                   param_distributions={'learning_rate': array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
       "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
       "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ]),\n",
       "                                        'max_depth': array([ 3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        'n_estimators': array([ 100,  200,  300,  400,  500,  600,  700,  800,  900, 1000]),\n",
       "                                        'subsample': array([0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "                   random_state=42, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3663251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (16120, 21)\n",
      "Valid shape: (2845, 21)\n",
      "Test shape: (2845, 21)\n"
     ]
    }
   ],
   "source": [
    "# Alle Object-/String-Spalten finden\n",
    "object_cols = X_train.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "\n",
    "# Object-Spalten entfernen\n",
    "X_train_clean = X_train.drop(columns=object_cols)\n",
    "valid_X_clean = valid_X.drop(columns=object_cols)\n",
    "test_X_clean  = test_X.drop(columns=object_cols)\n",
    "\n",
    "# Kontrolle\n",
    "print(\"Train shape:\", X_train_clean.shape)\n",
    "print(\"Valid shape:\", valid_X_clean.shape)\n",
    "print(\"Test shape:\", test_X_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd207423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.60433\tvalid-mlogloss:1.60441\n",
      "[1]\ttrain-mlogloss:1.59949\tvalid-mlogloss:1.59968\n",
      "[2]\ttrain-mlogloss:1.59473\tvalid-mlogloss:1.59498\n",
      "[3]\ttrain-mlogloss:1.59020\tvalid-mlogloss:1.59055\n",
      "[4]\ttrain-mlogloss:1.58540\tvalid-mlogloss:1.58581\n",
      "[5]\ttrain-mlogloss:1.58072\tvalid-mlogloss:1.58113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faulh\\anaconda3\\Lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [14:55:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\ttrain-mlogloss:1.57627\tvalid-mlogloss:1.57673\n",
      "[7]\ttrain-mlogloss:1.57190\tvalid-mlogloss:1.57245\n",
      "[8]\ttrain-mlogloss:1.56734\tvalid-mlogloss:1.56794\n",
      "[9]\ttrain-mlogloss:1.56271\tvalid-mlogloss:1.56330\n",
      "[10]\ttrain-mlogloss:1.55813\tvalid-mlogloss:1.55873\n",
      "[11]\ttrain-mlogloss:1.55374\tvalid-mlogloss:1.55447\n",
      "[12]\ttrain-mlogloss:1.54939\tvalid-mlogloss:1.55019\n",
      "[13]\ttrain-mlogloss:1.54513\tvalid-mlogloss:1.54597\n",
      "[14]\ttrain-mlogloss:1.54095\tvalid-mlogloss:1.54181\n",
      "[15]\ttrain-mlogloss:1.53665\tvalid-mlogloss:1.53757\n",
      "[16]\ttrain-mlogloss:1.53249\tvalid-mlogloss:1.53344\n",
      "[17]\ttrain-mlogloss:1.52839\tvalid-mlogloss:1.52934\n",
      "[18]\ttrain-mlogloss:1.52438\tvalid-mlogloss:1.52536\n",
      "[19]\ttrain-mlogloss:1.52027\tvalid-mlogloss:1.52128\n",
      "[20]\ttrain-mlogloss:1.51628\tvalid-mlogloss:1.51736\n",
      "[21]\ttrain-mlogloss:1.51241\tvalid-mlogloss:1.51351\n",
      "[22]\ttrain-mlogloss:1.50825\tvalid-mlogloss:1.50948\n",
      "[23]\ttrain-mlogloss:1.50421\tvalid-mlogloss:1.50544\n",
      "[24]\ttrain-mlogloss:1.50021\tvalid-mlogloss:1.50140\n",
      "[25]\ttrain-mlogloss:1.49637\tvalid-mlogloss:1.49767\n",
      "[26]\ttrain-mlogloss:1.49267\tvalid-mlogloss:1.49397\n",
      "[27]\ttrain-mlogloss:1.48884\tvalid-mlogloss:1.49026\n",
      "[28]\ttrain-mlogloss:1.48509\tvalid-mlogloss:1.48652\n",
      "[29]\ttrain-mlogloss:1.48129\tvalid-mlogloss:1.48276\n",
      "[30]\ttrain-mlogloss:1.47759\tvalid-mlogloss:1.47907\n",
      "[31]\ttrain-mlogloss:1.47397\tvalid-mlogloss:1.47549\n",
      "[32]\ttrain-mlogloss:1.47034\tvalid-mlogloss:1.47191\n",
      "[33]\ttrain-mlogloss:1.46681\tvalid-mlogloss:1.46838\n",
      "[34]\ttrain-mlogloss:1.46320\tvalid-mlogloss:1.46479\n",
      "[35]\ttrain-mlogloss:1.45980\tvalid-mlogloss:1.46139\n",
      "[36]\ttrain-mlogloss:1.45619\tvalid-mlogloss:1.45786\n",
      "[37]\ttrain-mlogloss:1.45269\tvalid-mlogloss:1.45447\n",
      "[38]\ttrain-mlogloss:1.44930\tvalid-mlogloss:1.45110\n",
      "[39]\ttrain-mlogloss:1.44596\tvalid-mlogloss:1.44780\n",
      "[40]\ttrain-mlogloss:1.44269\tvalid-mlogloss:1.44450\n",
      "[41]\ttrain-mlogloss:1.43933\tvalid-mlogloss:1.44107\n",
      "[42]\ttrain-mlogloss:1.43598\tvalid-mlogloss:1.43776\n",
      "[43]\ttrain-mlogloss:1.43266\tvalid-mlogloss:1.43455\n",
      "[44]\ttrain-mlogloss:1.42926\tvalid-mlogloss:1.43117\n",
      "[45]\ttrain-mlogloss:1.42597\tvalid-mlogloss:1.42789\n",
      "[46]\ttrain-mlogloss:1.42265\tvalid-mlogloss:1.42463\n",
      "[47]\ttrain-mlogloss:1.41952\tvalid-mlogloss:1.42147\n",
      "[48]\ttrain-mlogloss:1.41629\tvalid-mlogloss:1.41826\n",
      "[49]\ttrain-mlogloss:1.41317\tvalid-mlogloss:1.41515\n",
      "[50]\ttrain-mlogloss:1.41008\tvalid-mlogloss:1.41218\n",
      "[51]\ttrain-mlogloss:1.40698\tvalid-mlogloss:1.40906\n",
      "[52]\ttrain-mlogloss:1.40383\tvalid-mlogloss:1.40596\n",
      "[53]\ttrain-mlogloss:1.40061\tvalid-mlogloss:1.40282\n",
      "[54]\ttrain-mlogloss:1.39748\tvalid-mlogloss:1.39972\n",
      "[55]\ttrain-mlogloss:1.39464\tvalid-mlogloss:1.39685\n",
      "[56]\ttrain-mlogloss:1.39162\tvalid-mlogloss:1.39384\n",
      "[57]\ttrain-mlogloss:1.38871\tvalid-mlogloss:1.39100\n",
      "[58]\ttrain-mlogloss:1.38579\tvalid-mlogloss:1.38812\n",
      "[59]\ttrain-mlogloss:1.38273\tvalid-mlogloss:1.38512\n",
      "[60]\ttrain-mlogloss:1.37980\tvalid-mlogloss:1.38221\n",
      "[61]\ttrain-mlogloss:1.37697\tvalid-mlogloss:1.37942\n",
      "[62]\ttrain-mlogloss:1.37402\tvalid-mlogloss:1.37649\n",
      "[63]\ttrain-mlogloss:1.37113\tvalid-mlogloss:1.37359\n",
      "[64]\ttrain-mlogloss:1.36834\tvalid-mlogloss:1.37084\n",
      "[65]\ttrain-mlogloss:1.36557\tvalid-mlogloss:1.36806\n",
      "[66]\ttrain-mlogloss:1.36277\tvalid-mlogloss:1.36528\n",
      "[67]\ttrain-mlogloss:1.36007\tvalid-mlogloss:1.36259\n",
      "[68]\ttrain-mlogloss:1.35734\tvalid-mlogloss:1.35987\n",
      "[69]\ttrain-mlogloss:1.35464\tvalid-mlogloss:1.35711\n",
      "[70]\ttrain-mlogloss:1.35185\tvalid-mlogloss:1.35429\n",
      "[71]\ttrain-mlogloss:1.34897\tvalid-mlogloss:1.35140\n",
      "[72]\ttrain-mlogloss:1.34626\tvalid-mlogloss:1.34878\n",
      "[73]\ttrain-mlogloss:1.34361\tvalid-mlogloss:1.34618\n",
      "[74]\ttrain-mlogloss:1.34100\tvalid-mlogloss:1.34365\n",
      "[75]\ttrain-mlogloss:1.33831\tvalid-mlogloss:1.34097\n",
      "[76]\ttrain-mlogloss:1.33566\tvalid-mlogloss:1.33837\n",
      "[77]\ttrain-mlogloss:1.33298\tvalid-mlogloss:1.33576\n",
      "[78]\ttrain-mlogloss:1.33032\tvalid-mlogloss:1.33308\n",
      "[79]\ttrain-mlogloss:1.32779\tvalid-mlogloss:1.33061\n",
      "[80]\ttrain-mlogloss:1.32524\tvalid-mlogloss:1.32819\n",
      "[81]\ttrain-mlogloss:1.32266\tvalid-mlogloss:1.32566\n",
      "[82]\ttrain-mlogloss:1.32011\tvalid-mlogloss:1.32313\n",
      "[83]\ttrain-mlogloss:1.31755\tvalid-mlogloss:1.32059\n",
      "[84]\ttrain-mlogloss:1.31505\tvalid-mlogloss:1.31808\n",
      "[85]\ttrain-mlogloss:1.31262\tvalid-mlogloss:1.31568\n",
      "[86]\ttrain-mlogloss:1.31007\tvalid-mlogloss:1.31321\n",
      "[87]\ttrain-mlogloss:1.30750\tvalid-mlogloss:1.31070\n",
      "[88]\ttrain-mlogloss:1.30514\tvalid-mlogloss:1.30831\n",
      "[89]\ttrain-mlogloss:1.30270\tvalid-mlogloss:1.30599\n",
      "[90]\ttrain-mlogloss:1.30031\tvalid-mlogloss:1.30361\n",
      "[91]\ttrain-mlogloss:1.29799\tvalid-mlogloss:1.30125\n",
      "[92]\ttrain-mlogloss:1.29570\tvalid-mlogloss:1.29901\n",
      "[93]\ttrain-mlogloss:1.29316\tvalid-mlogloss:1.29648\n",
      "[94]\ttrain-mlogloss:1.29086\tvalid-mlogloss:1.29425\n",
      "[95]\ttrain-mlogloss:1.28859\tvalid-mlogloss:1.29199\n",
      "[96]\ttrain-mlogloss:1.28628\tvalid-mlogloss:1.28973\n",
      "[97]\ttrain-mlogloss:1.28382\tvalid-mlogloss:1.28727\n",
      "[98]\ttrain-mlogloss:1.28147\tvalid-mlogloss:1.28489\n",
      "[99]\ttrain-mlogloss:1.27929\tvalid-mlogloss:1.28277\n",
      "[100]\ttrain-mlogloss:1.27706\tvalid-mlogloss:1.28049\n",
      "[101]\ttrain-mlogloss:1.27486\tvalid-mlogloss:1.27833\n",
      "[102]\ttrain-mlogloss:1.27262\tvalid-mlogloss:1.27606\n",
      "[103]\ttrain-mlogloss:1.27017\tvalid-mlogloss:1.27365\n",
      "[104]\ttrain-mlogloss:1.26794\tvalid-mlogloss:1.27139\n",
      "[105]\ttrain-mlogloss:1.26573\tvalid-mlogloss:1.26925\n",
      "[106]\ttrain-mlogloss:1.26366\tvalid-mlogloss:1.26719\n",
      "[107]\ttrain-mlogloss:1.26145\tvalid-mlogloss:1.26506\n",
      "[108]\ttrain-mlogloss:1.25922\tvalid-mlogloss:1.26281\n",
      "[109]\ttrain-mlogloss:1.25679\tvalid-mlogloss:1.26033\n",
      "[110]\ttrain-mlogloss:1.25454\tvalid-mlogloss:1.25811\n",
      "[111]\ttrain-mlogloss:1.25244\tvalid-mlogloss:1.25601\n",
      "[112]\ttrain-mlogloss:1.25024\tvalid-mlogloss:1.25376\n",
      "[113]\ttrain-mlogloss:1.24809\tvalid-mlogloss:1.25160\n",
      "[114]\ttrain-mlogloss:1.24589\tvalid-mlogloss:1.24950\n",
      "[115]\ttrain-mlogloss:1.24374\tvalid-mlogloss:1.24737\n",
      "[116]\ttrain-mlogloss:1.24172\tvalid-mlogloss:1.24541\n",
      "[117]\ttrain-mlogloss:1.23945\tvalid-mlogloss:1.24317\n",
      "[118]\ttrain-mlogloss:1.23732\tvalid-mlogloss:1.24106\n",
      "[119]\ttrain-mlogloss:1.23514\tvalid-mlogloss:1.23892\n",
      "[120]\ttrain-mlogloss:1.23317\tvalid-mlogloss:1.23691\n",
      "[121]\ttrain-mlogloss:1.23122\tvalid-mlogloss:1.23496\n",
      "[122]\ttrain-mlogloss:1.22922\tvalid-mlogloss:1.23298\n",
      "[123]\ttrain-mlogloss:1.22729\tvalid-mlogloss:1.23106\n",
      "[124]\ttrain-mlogloss:1.22531\tvalid-mlogloss:1.22912\n",
      "[125]\ttrain-mlogloss:1.22330\tvalid-mlogloss:1.22712\n",
      "[126]\ttrain-mlogloss:1.22131\tvalid-mlogloss:1.22522\n",
      "[127]\ttrain-mlogloss:1.21914\tvalid-mlogloss:1.22307\n",
      "[128]\ttrain-mlogloss:1.21708\tvalid-mlogloss:1.22104\n",
      "[129]\ttrain-mlogloss:1.21522\tvalid-mlogloss:1.21918\n",
      "[130]\ttrain-mlogloss:1.21306\tvalid-mlogloss:1.21700\n",
      "[131]\ttrain-mlogloss:1.21109\tvalid-mlogloss:1.21504\n",
      "[132]\ttrain-mlogloss:1.20923\tvalid-mlogloss:1.21315\n",
      "[133]\ttrain-mlogloss:1.20717\tvalid-mlogloss:1.21116\n",
      "[134]\ttrain-mlogloss:1.20513\tvalid-mlogloss:1.20918\n",
      "[135]\ttrain-mlogloss:1.20312\tvalid-mlogloss:1.20718\n",
      "[136]\ttrain-mlogloss:1.20121\tvalid-mlogloss:1.20531\n",
      "[137]\ttrain-mlogloss:1.19937\tvalid-mlogloss:1.20346\n",
      "[138]\ttrain-mlogloss:1.19746\tvalid-mlogloss:1.20162\n",
      "[139]\ttrain-mlogloss:1.19565\tvalid-mlogloss:1.19986\n",
      "[140]\ttrain-mlogloss:1.19368\tvalid-mlogloss:1.19793\n",
      "[141]\ttrain-mlogloss:1.19182\tvalid-mlogloss:1.19617\n",
      "[142]\ttrain-mlogloss:1.18988\tvalid-mlogloss:1.19429\n",
      "[143]\ttrain-mlogloss:1.18820\tvalid-mlogloss:1.19257\n",
      "[144]\ttrain-mlogloss:1.18651\tvalid-mlogloss:1.19088\n",
      "[145]\ttrain-mlogloss:1.18473\tvalid-mlogloss:1.18903\n",
      "[146]\ttrain-mlogloss:1.18281\tvalid-mlogloss:1.18718\n",
      "[147]\ttrain-mlogloss:1.18081\tvalid-mlogloss:1.18517\n",
      "[148]\ttrain-mlogloss:1.17888\tvalid-mlogloss:1.18326\n",
      "[149]\ttrain-mlogloss:1.17712\tvalid-mlogloss:1.18157\n",
      "[150]\ttrain-mlogloss:1.17535\tvalid-mlogloss:1.17991\n",
      "[151]\ttrain-mlogloss:1.17348\tvalid-mlogloss:1.17807\n",
      "[152]\ttrain-mlogloss:1.17190\tvalid-mlogloss:1.17652\n",
      "[153]\ttrain-mlogloss:1.17002\tvalid-mlogloss:1.17467\n",
      "[154]\ttrain-mlogloss:1.16831\tvalid-mlogloss:1.17300\n",
      "[155]\ttrain-mlogloss:1.16642\tvalid-mlogloss:1.17108\n",
      "[156]\ttrain-mlogloss:1.16462\tvalid-mlogloss:1.16929\n",
      "[157]\ttrain-mlogloss:1.16290\tvalid-mlogloss:1.16758\n",
      "[158]\ttrain-mlogloss:1.16127\tvalid-mlogloss:1.16594\n",
      "[159]\ttrain-mlogloss:1.15953\tvalid-mlogloss:1.16414\n",
      "[160]\ttrain-mlogloss:1.15788\tvalid-mlogloss:1.16252\n",
      "[161]\ttrain-mlogloss:1.15621\tvalid-mlogloss:1.16084\n",
      "[162]\ttrain-mlogloss:1.15464\tvalid-mlogloss:1.15931\n",
      "[163]\ttrain-mlogloss:1.15299\tvalid-mlogloss:1.15763\n",
      "[164]\ttrain-mlogloss:1.15142\tvalid-mlogloss:1.15609\n",
      "[165]\ttrain-mlogloss:1.14978\tvalid-mlogloss:1.15450\n",
      "[166]\ttrain-mlogloss:1.14830\tvalid-mlogloss:1.15307\n",
      "[167]\ttrain-mlogloss:1.14648\tvalid-mlogloss:1.15130\n",
      "[168]\ttrain-mlogloss:1.14477\tvalid-mlogloss:1.14956\n",
      "[169]\ttrain-mlogloss:1.14316\tvalid-mlogloss:1.14796\n",
      "[170]\ttrain-mlogloss:1.14169\tvalid-mlogloss:1.14650\n",
      "[171]\ttrain-mlogloss:1.14004\tvalid-mlogloss:1.14494\n",
      "[172]\ttrain-mlogloss:1.13846\tvalid-mlogloss:1.14338\n",
      "[173]\ttrain-mlogloss:1.13700\tvalid-mlogloss:1.14190\n",
      "[174]\ttrain-mlogloss:1.13532\tvalid-mlogloss:1.14025\n",
      "[175]\ttrain-mlogloss:1.13395\tvalid-mlogloss:1.13892\n",
      "[176]\ttrain-mlogloss:1.13225\tvalid-mlogloss:1.13716\n",
      "[177]\ttrain-mlogloss:1.13071\tvalid-mlogloss:1.13568\n",
      "[178]\ttrain-mlogloss:1.12904\tvalid-mlogloss:1.13406\n",
      "[179]\ttrain-mlogloss:1.12759\tvalid-mlogloss:1.13266\n",
      "[180]\ttrain-mlogloss:1.12605\tvalid-mlogloss:1.13108\n",
      "[181]\ttrain-mlogloss:1.12459\tvalid-mlogloss:1.12970\n",
      "[182]\ttrain-mlogloss:1.12307\tvalid-mlogloss:1.12823\n",
      "[183]\ttrain-mlogloss:1.12150\tvalid-mlogloss:1.12671\n",
      "[184]\ttrain-mlogloss:1.11996\tvalid-mlogloss:1.12522\n",
      "[185]\ttrain-mlogloss:1.11851\tvalid-mlogloss:1.12374\n",
      "[186]\ttrain-mlogloss:1.11683\tvalid-mlogloss:1.12209\n",
      "[187]\ttrain-mlogloss:1.11545\tvalid-mlogloss:1.12073\n",
      "[188]\ttrain-mlogloss:1.11379\tvalid-mlogloss:1.11916\n",
      "[189]\ttrain-mlogloss:1.11216\tvalid-mlogloss:1.11755\n",
      "[190]\ttrain-mlogloss:1.11064\tvalid-mlogloss:1.11602\n",
      "[191]\ttrain-mlogloss:1.10901\tvalid-mlogloss:1.11449\n",
      "[192]\ttrain-mlogloss:1.10744\tvalid-mlogloss:1.11293\n",
      "[193]\ttrain-mlogloss:1.10591\tvalid-mlogloss:1.11140\n",
      "[194]\ttrain-mlogloss:1.10420\tvalid-mlogloss:1.10968\n",
      "[195]\ttrain-mlogloss:1.10278\tvalid-mlogloss:1.10830\n",
      "[196]\ttrain-mlogloss:1.10125\tvalid-mlogloss:1.10679\n",
      "[197]\ttrain-mlogloss:1.09970\tvalid-mlogloss:1.10526\n",
      "[198]\ttrain-mlogloss:1.09817\tvalid-mlogloss:1.10380\n",
      "[199]\ttrain-mlogloss:1.09675\tvalid-mlogloss:1.10234\n",
      "[200]\ttrain-mlogloss:1.09542\tvalid-mlogloss:1.10101\n",
      "[201]\ttrain-mlogloss:1.09402\tvalid-mlogloss:1.09963\n",
      "[202]\ttrain-mlogloss:1.09269\tvalid-mlogloss:1.09833\n",
      "[203]\ttrain-mlogloss:1.09118\tvalid-mlogloss:1.09682\n",
      "[204]\ttrain-mlogloss:1.08968\tvalid-mlogloss:1.09532\n",
      "[205]\ttrain-mlogloss:1.08819\tvalid-mlogloss:1.09386\n",
      "[206]\ttrain-mlogloss:1.08684\tvalid-mlogloss:1.09253\n",
      "[207]\ttrain-mlogloss:1.08523\tvalid-mlogloss:1.09091\n",
      "[208]\ttrain-mlogloss:1.08383\tvalid-mlogloss:1.08957\n",
      "[209]\ttrain-mlogloss:1.08248\tvalid-mlogloss:1.08824\n",
      "[210]\ttrain-mlogloss:1.08101\tvalid-mlogloss:1.08677\n",
      "[211]\ttrain-mlogloss:1.07971\tvalid-mlogloss:1.08548\n",
      "[212]\ttrain-mlogloss:1.07838\tvalid-mlogloss:1.08414\n",
      "[213]\ttrain-mlogloss:1.07700\tvalid-mlogloss:1.08279\n",
      "[214]\ttrain-mlogloss:1.07547\tvalid-mlogloss:1.08126\n",
      "[215]\ttrain-mlogloss:1.07415\tvalid-mlogloss:1.07992\n",
      "[216]\ttrain-mlogloss:1.07284\tvalid-mlogloss:1.07865\n",
      "[217]\ttrain-mlogloss:1.07153\tvalid-mlogloss:1.07733\n",
      "[218]\ttrain-mlogloss:1.07004\tvalid-mlogloss:1.07587\n",
      "[219]\ttrain-mlogloss:1.06881\tvalid-mlogloss:1.07469\n",
      "[220]\ttrain-mlogloss:1.06761\tvalid-mlogloss:1.07358\n",
      "[221]\ttrain-mlogloss:1.06642\tvalid-mlogloss:1.07244\n",
      "[222]\ttrain-mlogloss:1.06512\tvalid-mlogloss:1.07110\n",
      "[223]\ttrain-mlogloss:1.06374\tvalid-mlogloss:1.06975\n",
      "[224]\ttrain-mlogloss:1.06244\tvalid-mlogloss:1.06844\n",
      "[225]\ttrain-mlogloss:1.06086\tvalid-mlogloss:1.06686\n",
      "[226]\ttrain-mlogloss:1.05948\tvalid-mlogloss:1.06543\n",
      "[227]\ttrain-mlogloss:1.05808\tvalid-mlogloss:1.06408\n",
      "[228]\ttrain-mlogloss:1.05683\tvalid-mlogloss:1.06283\n",
      "[229]\ttrain-mlogloss:1.05544\tvalid-mlogloss:1.06145\n",
      "[230]\ttrain-mlogloss:1.05413\tvalid-mlogloss:1.06016\n",
      "[231]\ttrain-mlogloss:1.05282\tvalid-mlogloss:1.05890\n",
      "[232]\ttrain-mlogloss:1.05143\tvalid-mlogloss:1.05748\n",
      "[233]\ttrain-mlogloss:1.05018\tvalid-mlogloss:1.05621\n",
      "[234]\ttrain-mlogloss:1.04891\tvalid-mlogloss:1.05498\n",
      "[235]\ttrain-mlogloss:1.04767\tvalid-mlogloss:1.05371\n",
      "[236]\ttrain-mlogloss:1.04651\tvalid-mlogloss:1.05262\n",
      "[237]\ttrain-mlogloss:1.04512\tvalid-mlogloss:1.05126\n",
      "[238]\ttrain-mlogloss:1.04389\tvalid-mlogloss:1.05007\n",
      "[239]\ttrain-mlogloss:1.04249\tvalid-mlogloss:1.04866\n",
      "[240]\ttrain-mlogloss:1.04130\tvalid-mlogloss:1.04743\n",
      "[241]\ttrain-mlogloss:1.04014\tvalid-mlogloss:1.04631\n",
      "[242]\ttrain-mlogloss:1.03901\tvalid-mlogloss:1.04521\n",
      "[243]\ttrain-mlogloss:1.03781\tvalid-mlogloss:1.04401\n",
      "[244]\ttrain-mlogloss:1.03654\tvalid-mlogloss:1.04276\n",
      "[245]\ttrain-mlogloss:1.03527\tvalid-mlogloss:1.04149\n",
      "[246]\ttrain-mlogloss:1.03401\tvalid-mlogloss:1.04028\n",
      "[247]\ttrain-mlogloss:1.03272\tvalid-mlogloss:1.03900\n",
      "[248]\ttrain-mlogloss:1.03146\tvalid-mlogloss:1.03773\n",
      "[249]\ttrain-mlogloss:1.03033\tvalid-mlogloss:1.03662\n",
      "[250]\ttrain-mlogloss:1.02893\tvalid-mlogloss:1.03522\n",
      "[251]\ttrain-mlogloss:1.02792\tvalid-mlogloss:1.03426\n",
      "[252]\ttrain-mlogloss:1.02669\tvalid-mlogloss:1.03303\n",
      "[253]\ttrain-mlogloss:1.02546\tvalid-mlogloss:1.03185\n",
      "[254]\ttrain-mlogloss:1.02429\tvalid-mlogloss:1.03069\n",
      "[255]\ttrain-mlogloss:1.02299\tvalid-mlogloss:1.02939\n",
      "[256]\ttrain-mlogloss:1.02185\tvalid-mlogloss:1.02827\n",
      "[257]\ttrain-mlogloss:1.02062\tvalid-mlogloss:1.02707\n",
      "[258]\ttrain-mlogloss:1.01943\tvalid-mlogloss:1.02588\n",
      "[259]\ttrain-mlogloss:1.01842\tvalid-mlogloss:1.02490\n",
      "[260]\ttrain-mlogloss:1.01726\tvalid-mlogloss:1.02379\n",
      "[261]\ttrain-mlogloss:1.01629\tvalid-mlogloss:1.02284\n",
      "[262]\ttrain-mlogloss:1.01502\tvalid-mlogloss:1.02160\n",
      "[263]\ttrain-mlogloss:1.01394\tvalid-mlogloss:1.02050\n",
      "[264]\ttrain-mlogloss:1.01291\tvalid-mlogloss:1.01946\n",
      "[265]\ttrain-mlogloss:1.01157\tvalid-mlogloss:1.01809\n",
      "[266]\ttrain-mlogloss:1.01030\tvalid-mlogloss:1.01690\n",
      "[267]\ttrain-mlogloss:1.00931\tvalid-mlogloss:1.01593\n",
      "[268]\ttrain-mlogloss:1.00809\tvalid-mlogloss:1.01468\n",
      "[269]\ttrain-mlogloss:1.00690\tvalid-mlogloss:1.01356\n",
      "[270]\ttrain-mlogloss:1.00596\tvalid-mlogloss:1.01264\n",
      "[271]\ttrain-mlogloss:1.00473\tvalid-mlogloss:1.01147\n",
      "[272]\ttrain-mlogloss:1.00362\tvalid-mlogloss:1.01043\n",
      "[273]\ttrain-mlogloss:1.00248\tvalid-mlogloss:1.00930\n",
      "[274]\ttrain-mlogloss:1.00146\tvalid-mlogloss:1.00831\n",
      "[275]\ttrain-mlogloss:1.00046\tvalid-mlogloss:1.00732\n",
      "[276]\ttrain-mlogloss:0.99943\tvalid-mlogloss:1.00632\n",
      "[277]\ttrain-mlogloss:0.99834\tvalid-mlogloss:1.00525\n",
      "[278]\ttrain-mlogloss:0.99728\tvalid-mlogloss:1.00422\n",
      "[279]\ttrain-mlogloss:0.99615\tvalid-mlogloss:1.00309\n",
      "[280]\ttrain-mlogloss:0.99512\tvalid-mlogloss:1.00200\n",
      "[281]\ttrain-mlogloss:0.99396\tvalid-mlogloss:1.00087\n",
      "[282]\ttrain-mlogloss:0.99274\tvalid-mlogloss:0.99967\n",
      "[283]\ttrain-mlogloss:0.99177\tvalid-mlogloss:0.99871\n",
      "[284]\ttrain-mlogloss:0.99077\tvalid-mlogloss:0.99771\n",
      "[285]\ttrain-mlogloss:0.98968\tvalid-mlogloss:0.99661\n",
      "[286]\ttrain-mlogloss:0.98843\tvalid-mlogloss:0.99537\n",
      "[287]\ttrain-mlogloss:0.98717\tvalid-mlogloss:0.99411\n",
      "[288]\ttrain-mlogloss:0.98623\tvalid-mlogloss:0.99313\n",
      "[289]\ttrain-mlogloss:0.98508\tvalid-mlogloss:0.99201\n",
      "[290]\ttrain-mlogloss:0.98395\tvalid-mlogloss:0.99093\n",
      "[291]\ttrain-mlogloss:0.98298\tvalid-mlogloss:0.99001\n",
      "[292]\ttrain-mlogloss:0.98179\tvalid-mlogloss:0.98890\n",
      "[293]\ttrain-mlogloss:0.98077\tvalid-mlogloss:0.98789\n",
      "[294]\ttrain-mlogloss:0.97968\tvalid-mlogloss:0.98680\n",
      "[295]\ttrain-mlogloss:0.97854\tvalid-mlogloss:0.98567\n",
      "[296]\ttrain-mlogloss:0.97755\tvalid-mlogloss:0.98473\n",
      "[297]\ttrain-mlogloss:0.97655\tvalid-mlogloss:0.98375\n",
      "[298]\ttrain-mlogloss:0.97536\tvalid-mlogloss:0.98256\n",
      "[299]\ttrain-mlogloss:0.97420\tvalid-mlogloss:0.98138\n",
      "[300]\ttrain-mlogloss:0.97304\tvalid-mlogloss:0.98022\n",
      "[301]\ttrain-mlogloss:0.97211\tvalid-mlogloss:0.97935\n",
      "[302]\ttrain-mlogloss:0.97111\tvalid-mlogloss:0.97840\n",
      "[303]\ttrain-mlogloss:0.97009\tvalid-mlogloss:0.97742\n",
      "[304]\ttrain-mlogloss:0.96910\tvalid-mlogloss:0.97640\n",
      "[305]\ttrain-mlogloss:0.96821\tvalid-mlogloss:0.97553\n",
      "[306]\ttrain-mlogloss:0.96721\tvalid-mlogloss:0.97452\n",
      "[307]\ttrain-mlogloss:0.96620\tvalid-mlogloss:0.97353\n",
      "[308]\ttrain-mlogloss:0.96528\tvalid-mlogloss:0.97263\n",
      "[309]\ttrain-mlogloss:0.96437\tvalid-mlogloss:0.97174\n",
      "[310]\ttrain-mlogloss:0.96345\tvalid-mlogloss:0.97086\n",
      "[311]\ttrain-mlogloss:0.96228\tvalid-mlogloss:0.96968\n",
      "[312]\ttrain-mlogloss:0.96109\tvalid-mlogloss:0.96851\n",
      "[313]\ttrain-mlogloss:0.96008\tvalid-mlogloss:0.96748\n",
      "[314]\ttrain-mlogloss:0.95904\tvalid-mlogloss:0.96652\n",
      "[315]\ttrain-mlogloss:0.95797\tvalid-mlogloss:0.96550\n",
      "[316]\ttrain-mlogloss:0.95690\tvalid-mlogloss:0.96443\n",
      "[317]\ttrain-mlogloss:0.95599\tvalid-mlogloss:0.96356\n",
      "[318]\ttrain-mlogloss:0.95511\tvalid-mlogloss:0.96269\n",
      "[319]\ttrain-mlogloss:0.95420\tvalid-mlogloss:0.96186\n",
      "[320]\ttrain-mlogloss:0.95326\tvalid-mlogloss:0.96092\n",
      "[321]\ttrain-mlogloss:0.95232\tvalid-mlogloss:0.96000\n",
      "[322]\ttrain-mlogloss:0.95143\tvalid-mlogloss:0.95911\n",
      "[323]\ttrain-mlogloss:0.95064\tvalid-mlogloss:0.95835\n",
      "[324]\ttrain-mlogloss:0.94978\tvalid-mlogloss:0.95750\n",
      "[325]\ttrain-mlogloss:0.94889\tvalid-mlogloss:0.95660\n",
      "[326]\ttrain-mlogloss:0.94790\tvalid-mlogloss:0.95560\n",
      "[327]\ttrain-mlogloss:0.94686\tvalid-mlogloss:0.95457\n",
      "[328]\ttrain-mlogloss:0.94607\tvalid-mlogloss:0.95379\n",
      "[329]\ttrain-mlogloss:0.94502\tvalid-mlogloss:0.95276\n",
      "[330]\ttrain-mlogloss:0.94421\tvalid-mlogloss:0.95200\n",
      "[331]\ttrain-mlogloss:0.94325\tvalid-mlogloss:0.95103\n",
      "[332]\ttrain-mlogloss:0.94236\tvalid-mlogloss:0.95019\n",
      "[333]\ttrain-mlogloss:0.94161\tvalid-mlogloss:0.94946\n",
      "[334]\ttrain-mlogloss:0.94076\tvalid-mlogloss:0.94868\n",
      "[335]\ttrain-mlogloss:0.93983\tvalid-mlogloss:0.94780\n",
      "[336]\ttrain-mlogloss:0.93874\tvalid-mlogloss:0.94672\n",
      "[337]\ttrain-mlogloss:0.93781\tvalid-mlogloss:0.94581\n",
      "[338]\ttrain-mlogloss:0.93668\tvalid-mlogloss:0.94465\n",
      "[339]\ttrain-mlogloss:0.93586\tvalid-mlogloss:0.94389\n",
      "[340]\ttrain-mlogloss:0.93499\tvalid-mlogloss:0.94303\n",
      "[341]\ttrain-mlogloss:0.93425\tvalid-mlogloss:0.94233\n",
      "[342]\ttrain-mlogloss:0.93326\tvalid-mlogloss:0.94137\n",
      "[343]\ttrain-mlogloss:0.93252\tvalid-mlogloss:0.94067\n",
      "[344]\ttrain-mlogloss:0.93162\tvalid-mlogloss:0.93979\n",
      "[345]\ttrain-mlogloss:0.93080\tvalid-mlogloss:0.93895\n",
      "[346]\ttrain-mlogloss:0.93008\tvalid-mlogloss:0.93825\n",
      "[347]\ttrain-mlogloss:0.92934\tvalid-mlogloss:0.93754\n",
      "[348]\ttrain-mlogloss:0.92845\tvalid-mlogloss:0.93662\n",
      "[349]\ttrain-mlogloss:0.92750\tvalid-mlogloss:0.93571\n",
      "[350]\ttrain-mlogloss:0.92670\tvalid-mlogloss:0.93494\n",
      "[351]\ttrain-mlogloss:0.92592\tvalid-mlogloss:0.93414\n",
      "[352]\ttrain-mlogloss:0.92507\tvalid-mlogloss:0.93332\n",
      "[353]\ttrain-mlogloss:0.92418\tvalid-mlogloss:0.93248\n",
      "[354]\ttrain-mlogloss:0.92341\tvalid-mlogloss:0.93173\n",
      "[355]\ttrain-mlogloss:0.92257\tvalid-mlogloss:0.93094\n",
      "[356]\ttrain-mlogloss:0.92165\tvalid-mlogloss:0.93008\n",
      "[357]\ttrain-mlogloss:0.92084\tvalid-mlogloss:0.92930\n",
      "[358]\ttrain-mlogloss:0.91988\tvalid-mlogloss:0.92836\n",
      "[359]\ttrain-mlogloss:0.91897\tvalid-mlogloss:0.92744\n",
      "[360]\ttrain-mlogloss:0.91794\tvalid-mlogloss:0.92643\n",
      "[361]\ttrain-mlogloss:0.91717\tvalid-mlogloss:0.92563\n",
      "[362]\ttrain-mlogloss:0.91634\tvalid-mlogloss:0.92484\n",
      "[363]\ttrain-mlogloss:0.91559\tvalid-mlogloss:0.92411\n",
      "[364]\ttrain-mlogloss:0.91473\tvalid-mlogloss:0.92327\n",
      "[365]\ttrain-mlogloss:0.91383\tvalid-mlogloss:0.92238\n",
      "[366]\ttrain-mlogloss:0.91306\tvalid-mlogloss:0.92163\n",
      "[367]\ttrain-mlogloss:0.91208\tvalid-mlogloss:0.92070\n",
      "[368]\ttrain-mlogloss:0.91129\tvalid-mlogloss:0.91994\n",
      "[369]\ttrain-mlogloss:0.91034\tvalid-mlogloss:0.91903\n",
      "[370]\ttrain-mlogloss:0.90957\tvalid-mlogloss:0.91827\n",
      "[371]\ttrain-mlogloss:0.90884\tvalid-mlogloss:0.91756\n",
      "[372]\ttrain-mlogloss:0.90804\tvalid-mlogloss:0.91678\n",
      "[373]\ttrain-mlogloss:0.90717\tvalid-mlogloss:0.91597\n",
      "[374]\ttrain-mlogloss:0.90613\tvalid-mlogloss:0.91495\n",
      "[375]\ttrain-mlogloss:0.90527\tvalid-mlogloss:0.91408\n",
      "[376]\ttrain-mlogloss:0.90445\tvalid-mlogloss:0.91330\n",
      "[377]\ttrain-mlogloss:0.90375\tvalid-mlogloss:0.91261\n",
      "[378]\ttrain-mlogloss:0.90306\tvalid-mlogloss:0.91200\n",
      "[379]\ttrain-mlogloss:0.90209\tvalid-mlogloss:0.91101\n",
      "[380]\ttrain-mlogloss:0.90136\tvalid-mlogloss:0.91027\n",
      "[381]\ttrain-mlogloss:0.90050\tvalid-mlogloss:0.90942\n",
      "[382]\ttrain-mlogloss:0.89972\tvalid-mlogloss:0.90862\n",
      "[383]\ttrain-mlogloss:0.89883\tvalid-mlogloss:0.90773\n",
      "[384]\ttrain-mlogloss:0.89792\tvalid-mlogloss:0.90676\n",
      "[385]\ttrain-mlogloss:0.89693\tvalid-mlogloss:0.90577\n",
      "[386]\ttrain-mlogloss:0.89613\tvalid-mlogloss:0.90499\n",
      "[387]\ttrain-mlogloss:0.89530\tvalid-mlogloss:0.90414\n",
      "[388]\ttrain-mlogloss:0.89430\tvalid-mlogloss:0.90316\n",
      "[389]\ttrain-mlogloss:0.89341\tvalid-mlogloss:0.90227\n",
      "[390]\ttrain-mlogloss:0.89266\tvalid-mlogloss:0.90153\n",
      "[391]\ttrain-mlogloss:0.89187\tvalid-mlogloss:0.90081\n",
      "[392]\ttrain-mlogloss:0.89107\tvalid-mlogloss:0.90002\n",
      "[393]\ttrain-mlogloss:0.89029\tvalid-mlogloss:0.89926\n",
      "[394]\ttrain-mlogloss:0.88959\tvalid-mlogloss:0.89858\n",
      "[395]\ttrain-mlogloss:0.88889\tvalid-mlogloss:0.89791\n",
      "[396]\ttrain-mlogloss:0.88810\tvalid-mlogloss:0.89718\n",
      "[397]\ttrain-mlogloss:0.88737\tvalid-mlogloss:0.89646\n",
      "[398]\ttrain-mlogloss:0.88656\tvalid-mlogloss:0.89564\n",
      "[399]\ttrain-mlogloss:0.88580\tvalid-mlogloss:0.89488\n",
      "[400]\ttrain-mlogloss:0.88480\tvalid-mlogloss:0.89392\n",
      "[401]\ttrain-mlogloss:0.88381\tvalid-mlogloss:0.89291\n",
      "[402]\ttrain-mlogloss:0.88293\tvalid-mlogloss:0.89205\n",
      "[403]\ttrain-mlogloss:0.88213\tvalid-mlogloss:0.89126\n",
      "[404]\ttrain-mlogloss:0.88128\tvalid-mlogloss:0.89039\n",
      "[405]\ttrain-mlogloss:0.88042\tvalid-mlogloss:0.88952\n",
      "[406]\ttrain-mlogloss:0.87961\tvalid-mlogloss:0.88870\n",
      "[407]\ttrain-mlogloss:0.87889\tvalid-mlogloss:0.88802\n",
      "[408]\ttrain-mlogloss:0.87796\tvalid-mlogloss:0.88708\n",
      "[409]\ttrain-mlogloss:0.87711\tvalid-mlogloss:0.88627\n",
      "[410]\ttrain-mlogloss:0.87643\tvalid-mlogloss:0.88561\n",
      "[411]\ttrain-mlogloss:0.87560\tvalid-mlogloss:0.88480\n",
      "[412]\ttrain-mlogloss:0.87495\tvalid-mlogloss:0.88412\n",
      "[413]\ttrain-mlogloss:0.87418\tvalid-mlogloss:0.88335\n",
      "[414]\ttrain-mlogloss:0.87342\tvalid-mlogloss:0.88261\n",
      "[415]\ttrain-mlogloss:0.87242\tvalid-mlogloss:0.88163\n",
      "[416]\ttrain-mlogloss:0.87162\tvalid-mlogloss:0.88086\n",
      "[417]\ttrain-mlogloss:0.87091\tvalid-mlogloss:0.88019\n",
      "[418]\ttrain-mlogloss:0.87022\tvalid-mlogloss:0.87952\n",
      "[419]\ttrain-mlogloss:0.86935\tvalid-mlogloss:0.87867\n",
      "[420]\ttrain-mlogloss:0.86865\tvalid-mlogloss:0.87795\n",
      "[421]\ttrain-mlogloss:0.86780\tvalid-mlogloss:0.87711\n",
      "[422]\ttrain-mlogloss:0.86718\tvalid-mlogloss:0.87653\n",
      "[423]\ttrain-mlogloss:0.86638\tvalid-mlogloss:0.87572\n",
      "[424]\ttrain-mlogloss:0.86560\tvalid-mlogloss:0.87496\n",
      "[425]\ttrain-mlogloss:0.86478\tvalid-mlogloss:0.87414\n",
      "[426]\ttrain-mlogloss:0.86397\tvalid-mlogloss:0.87338\n",
      "[427]\ttrain-mlogloss:0.86307\tvalid-mlogloss:0.87251\n",
      "[428]\ttrain-mlogloss:0.86244\tvalid-mlogloss:0.87192\n",
      "[429]\ttrain-mlogloss:0.86168\tvalid-mlogloss:0.87120\n",
      "[430]\ttrain-mlogloss:0.86077\tvalid-mlogloss:0.87028\n",
      "[431]\ttrain-mlogloss:0.86007\tvalid-mlogloss:0.86960\n",
      "[432]\ttrain-mlogloss:0.85920\tvalid-mlogloss:0.86873\n",
      "[433]\ttrain-mlogloss:0.85830\tvalid-mlogloss:0.86786\n",
      "[434]\ttrain-mlogloss:0.85756\tvalid-mlogloss:0.86714\n",
      "[435]\ttrain-mlogloss:0.85674\tvalid-mlogloss:0.86635\n",
      "[436]\ttrain-mlogloss:0.85579\tvalid-mlogloss:0.86539\n",
      "[437]\ttrain-mlogloss:0.85499\tvalid-mlogloss:0.86462\n",
      "[438]\ttrain-mlogloss:0.85428\tvalid-mlogloss:0.86391\n",
      "[439]\ttrain-mlogloss:0.85343\tvalid-mlogloss:0.86306\n",
      "[440]\ttrain-mlogloss:0.85276\tvalid-mlogloss:0.86240\n",
      "[441]\ttrain-mlogloss:0.85216\tvalid-mlogloss:0.86182\n",
      "[442]\ttrain-mlogloss:0.85121\tvalid-mlogloss:0.86090\n",
      "[443]\ttrain-mlogloss:0.85048\tvalid-mlogloss:0.86016\n",
      "[444]\ttrain-mlogloss:0.84981\tvalid-mlogloss:0.85945\n",
      "[445]\ttrain-mlogloss:0.84927\tvalid-mlogloss:0.85891\n",
      "[446]\ttrain-mlogloss:0.84857\tvalid-mlogloss:0.85825\n",
      "[447]\ttrain-mlogloss:0.84770\tvalid-mlogloss:0.85741\n",
      "[448]\ttrain-mlogloss:0.84705\tvalid-mlogloss:0.85679\n",
      "[449]\ttrain-mlogloss:0.84632\tvalid-mlogloss:0.85607\n",
      "[450]\ttrain-mlogloss:0.84570\tvalid-mlogloss:0.85546\n",
      "[451]\ttrain-mlogloss:0.84487\tvalid-mlogloss:0.85462\n",
      "[452]\ttrain-mlogloss:0.84421\tvalid-mlogloss:0.85396\n",
      "[453]\ttrain-mlogloss:0.84340\tvalid-mlogloss:0.85319\n",
      "[454]\ttrain-mlogloss:0.84260\tvalid-mlogloss:0.85236\n",
      "[455]\ttrain-mlogloss:0.84172\tvalid-mlogloss:0.85155\n",
      "[456]\ttrain-mlogloss:0.84113\tvalid-mlogloss:0.85093\n",
      "[457]\ttrain-mlogloss:0.84042\tvalid-mlogloss:0.85022\n",
      "[458]\ttrain-mlogloss:0.83975\tvalid-mlogloss:0.84957\n",
      "[459]\ttrain-mlogloss:0.83914\tvalid-mlogloss:0.84899\n",
      "[460]\ttrain-mlogloss:0.83837\tvalid-mlogloss:0.84820\n",
      "[461]\ttrain-mlogloss:0.83760\tvalid-mlogloss:0.84744\n",
      "[462]\ttrain-mlogloss:0.83674\tvalid-mlogloss:0.84663\n",
      "[463]\ttrain-mlogloss:0.83607\tvalid-mlogloss:0.84593\n",
      "[464]\ttrain-mlogloss:0.83552\tvalid-mlogloss:0.84541\n",
      "[465]\ttrain-mlogloss:0.83494\tvalid-mlogloss:0.84484\n",
      "[466]\ttrain-mlogloss:0.83432\tvalid-mlogloss:0.84421\n",
      "[467]\ttrain-mlogloss:0.83364\tvalid-mlogloss:0.84357\n",
      "[468]\ttrain-mlogloss:0.83315\tvalid-mlogloss:0.84310\n",
      "[469]\ttrain-mlogloss:0.83228\tvalid-mlogloss:0.84226\n",
      "[470]\ttrain-mlogloss:0.83150\tvalid-mlogloss:0.84155\n",
      "[471]\ttrain-mlogloss:0.83069\tvalid-mlogloss:0.84078\n",
      "[472]\ttrain-mlogloss:0.83004\tvalid-mlogloss:0.84017\n",
      "[473]\ttrain-mlogloss:0.82935\tvalid-mlogloss:0.83954\n",
      "[474]\ttrain-mlogloss:0.82878\tvalid-mlogloss:0.83897\n",
      "[475]\ttrain-mlogloss:0.82812\tvalid-mlogloss:0.83832\n",
      "[476]\ttrain-mlogloss:0.82725\tvalid-mlogloss:0.83749\n",
      "[477]\ttrain-mlogloss:0.82663\tvalid-mlogloss:0.83686\n",
      "[478]\ttrain-mlogloss:0.82589\tvalid-mlogloss:0.83610\n",
      "[479]\ttrain-mlogloss:0.82514\tvalid-mlogloss:0.83535\n",
      "[480]\ttrain-mlogloss:0.82454\tvalid-mlogloss:0.83477\n",
      "[481]\ttrain-mlogloss:0.82387\tvalid-mlogloss:0.83411\n",
      "[482]\ttrain-mlogloss:0.82321\tvalid-mlogloss:0.83348\n",
      "[483]\ttrain-mlogloss:0.82262\tvalid-mlogloss:0.83288\n",
      "[484]\ttrain-mlogloss:0.82201\tvalid-mlogloss:0.83229\n",
      "[485]\ttrain-mlogloss:0.82147\tvalid-mlogloss:0.83177\n",
      "[486]\ttrain-mlogloss:0.82084\tvalid-mlogloss:0.83111\n",
      "[487]\ttrain-mlogloss:0.82023\tvalid-mlogloss:0.83054\n",
      "[488]\ttrain-mlogloss:0.81961\tvalid-mlogloss:0.82998\n",
      "[489]\ttrain-mlogloss:0.81885\tvalid-mlogloss:0.82925\n",
      "[490]\ttrain-mlogloss:0.81815\tvalid-mlogloss:0.82857\n",
      "[491]\ttrain-mlogloss:0.81738\tvalid-mlogloss:0.82785\n",
      "[492]\ttrain-mlogloss:0.81667\tvalid-mlogloss:0.82711\n",
      "[493]\ttrain-mlogloss:0.81599\tvalid-mlogloss:0.82644\n",
      "[494]\ttrain-mlogloss:0.81526\tvalid-mlogloss:0.82575\n",
      "[495]\ttrain-mlogloss:0.81465\tvalid-mlogloss:0.82518\n",
      "[496]\ttrain-mlogloss:0.81394\tvalid-mlogloss:0.82449\n",
      "[497]\ttrain-mlogloss:0.81335\tvalid-mlogloss:0.82389\n",
      "[498]\ttrain-mlogloss:0.81271\tvalid-mlogloss:0.82325\n",
      "[499]\ttrain-mlogloss:0.81202\tvalid-mlogloss:0.82254\n",
      "[500]\ttrain-mlogloss:0.81152\tvalid-mlogloss:0.82203\n",
      "[501]\ttrain-mlogloss:0.81077\tvalid-mlogloss:0.82128\n",
      "[502]\ttrain-mlogloss:0.81018\tvalid-mlogloss:0.82071\n",
      "[503]\ttrain-mlogloss:0.80957\tvalid-mlogloss:0.82016\n",
      "[504]\ttrain-mlogloss:0.80906\tvalid-mlogloss:0.81967\n",
      "[505]\ttrain-mlogloss:0.80847\tvalid-mlogloss:0.81907\n",
      "[506]\ttrain-mlogloss:0.80774\tvalid-mlogloss:0.81832\n",
      "[507]\ttrain-mlogloss:0.80712\tvalid-mlogloss:0.81771\n",
      "[508]\ttrain-mlogloss:0.80646\tvalid-mlogloss:0.81703\n",
      "[509]\ttrain-mlogloss:0.80571\tvalid-mlogloss:0.81628\n",
      "[510]\ttrain-mlogloss:0.80510\tvalid-mlogloss:0.81564\n",
      "[511]\ttrain-mlogloss:0.80451\tvalid-mlogloss:0.81504\n",
      "[512]\ttrain-mlogloss:0.80404\tvalid-mlogloss:0.81458\n",
      "[513]\ttrain-mlogloss:0.80339\tvalid-mlogloss:0.81398\n",
      "[514]\ttrain-mlogloss:0.80268\tvalid-mlogloss:0.81326\n",
      "[515]\ttrain-mlogloss:0.80193\tvalid-mlogloss:0.81251\n",
      "[516]\ttrain-mlogloss:0.80120\tvalid-mlogloss:0.81178\n",
      "[517]\ttrain-mlogloss:0.80046\tvalid-mlogloss:0.81104\n",
      "[518]\ttrain-mlogloss:0.79960\tvalid-mlogloss:0.81015\n",
      "[519]\ttrain-mlogloss:0.79896\tvalid-mlogloss:0.80954\n",
      "[520]\ttrain-mlogloss:0.79837\tvalid-mlogloss:0.80898\n",
      "[521]\ttrain-mlogloss:0.79759\tvalid-mlogloss:0.80817\n",
      "[522]\ttrain-mlogloss:0.79698\tvalid-mlogloss:0.80755\n",
      "[523]\ttrain-mlogloss:0.79630\tvalid-mlogloss:0.80687\n",
      "[524]\ttrain-mlogloss:0.79582\tvalid-mlogloss:0.80639\n",
      "[525]\ttrain-mlogloss:0.79523\tvalid-mlogloss:0.80583\n",
      "[526]\ttrain-mlogloss:0.79463\tvalid-mlogloss:0.80525\n",
      "[527]\ttrain-mlogloss:0.79402\tvalid-mlogloss:0.80467\n",
      "[528]\ttrain-mlogloss:0.79337\tvalid-mlogloss:0.80404\n",
      "[529]\ttrain-mlogloss:0.79282\tvalid-mlogloss:0.80348\n",
      "[530]\ttrain-mlogloss:0.79202\tvalid-mlogloss:0.80268\n",
      "[531]\ttrain-mlogloss:0.79150\tvalid-mlogloss:0.80217\n",
      "[532]\ttrain-mlogloss:0.79091\tvalid-mlogloss:0.80159\n",
      "[533]\ttrain-mlogloss:0.79032\tvalid-mlogloss:0.80098\n",
      "[534]\ttrain-mlogloss:0.78959\tvalid-mlogloss:0.80025\n",
      "[535]\ttrain-mlogloss:0.78888\tvalid-mlogloss:0.79953\n",
      "[536]\ttrain-mlogloss:0.78806\tvalid-mlogloss:0.79872\n",
      "[537]\ttrain-mlogloss:0.78723\tvalid-mlogloss:0.79786\n",
      "[538]\ttrain-mlogloss:0.78653\tvalid-mlogloss:0.79715\n",
      "[539]\ttrain-mlogloss:0.78594\tvalid-mlogloss:0.79656\n",
      "[540]\ttrain-mlogloss:0.78531\tvalid-mlogloss:0.79593\n",
      "[541]\ttrain-mlogloss:0.78475\tvalid-mlogloss:0.79534\n",
      "[542]\ttrain-mlogloss:0.78387\tvalid-mlogloss:0.79441\n",
      "[543]\ttrain-mlogloss:0.78323\tvalid-mlogloss:0.79379\n",
      "[544]\ttrain-mlogloss:0.78248\tvalid-mlogloss:0.79304\n",
      "[545]\ttrain-mlogloss:0.78200\tvalid-mlogloss:0.79260\n",
      "[546]\ttrain-mlogloss:0.78133\tvalid-mlogloss:0.79195\n",
      "[547]\ttrain-mlogloss:0.78074\tvalid-mlogloss:0.79136\n",
      "[548]\ttrain-mlogloss:0.78009\tvalid-mlogloss:0.79074\n",
      "[549]\ttrain-mlogloss:0.77941\tvalid-mlogloss:0.79008\n",
      "[550]\ttrain-mlogloss:0.77883\tvalid-mlogloss:0.78951\n",
      "[551]\ttrain-mlogloss:0.77828\tvalid-mlogloss:0.78896\n",
      "[552]\ttrain-mlogloss:0.77779\tvalid-mlogloss:0.78846\n",
      "[553]\ttrain-mlogloss:0.77706\tvalid-mlogloss:0.78778\n",
      "[554]\ttrain-mlogloss:0.77658\tvalid-mlogloss:0.78730\n",
      "[555]\ttrain-mlogloss:0.77597\tvalid-mlogloss:0.78672\n",
      "[556]\ttrain-mlogloss:0.77552\tvalid-mlogloss:0.78629\n",
      "[557]\ttrain-mlogloss:0.77483\tvalid-mlogloss:0.78563\n",
      "[558]\ttrain-mlogloss:0.77422\tvalid-mlogloss:0.78504\n",
      "[559]\ttrain-mlogloss:0.77363\tvalid-mlogloss:0.78444\n",
      "[560]\ttrain-mlogloss:0.77314\tvalid-mlogloss:0.78398\n",
      "[561]\ttrain-mlogloss:0.77255\tvalid-mlogloss:0.78344\n",
      "[562]\ttrain-mlogloss:0.77179\tvalid-mlogloss:0.78269\n",
      "[563]\ttrain-mlogloss:0.77108\tvalid-mlogloss:0.78197\n",
      "[564]\ttrain-mlogloss:0.77027\tvalid-mlogloss:0.78116\n",
      "[565]\ttrain-mlogloss:0.76963\tvalid-mlogloss:0.78049\n",
      "[566]\ttrain-mlogloss:0.76883\tvalid-mlogloss:0.77964\n",
      "[567]\ttrain-mlogloss:0.76809\tvalid-mlogloss:0.77887\n",
      "[568]\ttrain-mlogloss:0.76747\tvalid-mlogloss:0.77823\n",
      "[569]\ttrain-mlogloss:0.76669\tvalid-mlogloss:0.77751\n",
      "[570]\ttrain-mlogloss:0.76618\tvalid-mlogloss:0.77701\n",
      "[571]\ttrain-mlogloss:0.76555\tvalid-mlogloss:0.77637\n",
      "[572]\ttrain-mlogloss:0.76486\tvalid-mlogloss:0.77570\n",
      "[573]\ttrain-mlogloss:0.76416\tvalid-mlogloss:0.77502\n",
      "[574]\ttrain-mlogloss:0.76358\tvalid-mlogloss:0.77446\n",
      "[575]\ttrain-mlogloss:0.76308\tvalid-mlogloss:0.77398\n",
      "[576]\ttrain-mlogloss:0.76238\tvalid-mlogloss:0.77332\n",
      "[577]\ttrain-mlogloss:0.76185\tvalid-mlogloss:0.77281\n",
      "[578]\ttrain-mlogloss:0.76116\tvalid-mlogloss:0.77213\n",
      "[579]\ttrain-mlogloss:0.76050\tvalid-mlogloss:0.77151\n",
      "[580]\ttrain-mlogloss:0.75978\tvalid-mlogloss:0.77082\n",
      "[581]\ttrain-mlogloss:0.75925\tvalid-mlogloss:0.77031\n",
      "[582]\ttrain-mlogloss:0.75857\tvalid-mlogloss:0.76967\n",
      "[583]\ttrain-mlogloss:0.75815\tvalid-mlogloss:0.76927\n",
      "[584]\ttrain-mlogloss:0.75747\tvalid-mlogloss:0.76863\n",
      "[585]\ttrain-mlogloss:0.75702\tvalid-mlogloss:0.76819\n",
      "[586]\ttrain-mlogloss:0.75629\tvalid-mlogloss:0.76747\n",
      "[587]\ttrain-mlogloss:0.75561\tvalid-mlogloss:0.76682\n",
      "[588]\ttrain-mlogloss:0.75512\tvalid-mlogloss:0.76634\n",
      "[589]\ttrain-mlogloss:0.75463\tvalid-mlogloss:0.76583\n",
      "[590]\ttrain-mlogloss:0.75412\tvalid-mlogloss:0.76533\n",
      "[591]\ttrain-mlogloss:0.75363\tvalid-mlogloss:0.76482\n",
      "[592]\ttrain-mlogloss:0.75317\tvalid-mlogloss:0.76437\n",
      "[593]\ttrain-mlogloss:0.75267\tvalid-mlogloss:0.76387\n",
      "[594]\ttrain-mlogloss:0.75209\tvalid-mlogloss:0.76331\n",
      "[595]\ttrain-mlogloss:0.75143\tvalid-mlogloss:0.76267\n",
      "[596]\ttrain-mlogloss:0.75088\tvalid-mlogloss:0.76213\n",
      "[597]\ttrain-mlogloss:0.75062\tvalid-mlogloss:0.76188\n",
      "[598]\ttrain-mlogloss:0.75003\tvalid-mlogloss:0.76131\n",
      "[599]\ttrain-mlogloss:0.74949\tvalid-mlogloss:0.76074\n",
      "[600]\ttrain-mlogloss:0.74902\tvalid-mlogloss:0.76025\n",
      "[601]\ttrain-mlogloss:0.74846\tvalid-mlogloss:0.75972\n",
      "[602]\ttrain-mlogloss:0.74787\tvalid-mlogloss:0.75912\n",
      "[603]\ttrain-mlogloss:0.74724\tvalid-mlogloss:0.75848\n",
      "[604]\ttrain-mlogloss:0.74675\tvalid-mlogloss:0.75799\n",
      "[605]\ttrain-mlogloss:0.74624\tvalid-mlogloss:0.75749\n",
      "[606]\ttrain-mlogloss:0.74580\tvalid-mlogloss:0.75709\n",
      "[607]\ttrain-mlogloss:0.74536\tvalid-mlogloss:0.75667\n",
      "[608]\ttrain-mlogloss:0.74487\tvalid-mlogloss:0.75617\n",
      "[609]\ttrain-mlogloss:0.74426\tvalid-mlogloss:0.75559\n",
      "[610]\ttrain-mlogloss:0.74350\tvalid-mlogloss:0.75483\n",
      "[611]\ttrain-mlogloss:0.74288\tvalid-mlogloss:0.75422\n",
      "[612]\ttrain-mlogloss:0.74231\tvalid-mlogloss:0.75365\n",
      "[613]\ttrain-mlogloss:0.74156\tvalid-mlogloss:0.75290\n",
      "[614]\ttrain-mlogloss:0.74095\tvalid-mlogloss:0.75231\n",
      "[615]\ttrain-mlogloss:0.74049\tvalid-mlogloss:0.75184\n",
      "[616]\ttrain-mlogloss:0.74010\tvalid-mlogloss:0.75149\n",
      "[617]\ttrain-mlogloss:0.73964\tvalid-mlogloss:0.75105\n",
      "[618]\ttrain-mlogloss:0.73911\tvalid-mlogloss:0.75053\n",
      "[619]\ttrain-mlogloss:0.73869\tvalid-mlogloss:0.75011\n",
      "[620]\ttrain-mlogloss:0.73826\tvalid-mlogloss:0.74970\n",
      "[621]\ttrain-mlogloss:0.73758\tvalid-mlogloss:0.74901\n",
      "[622]\ttrain-mlogloss:0.73688\tvalid-mlogloss:0.74829\n",
      "[623]\ttrain-mlogloss:0.73641\tvalid-mlogloss:0.74782\n",
      "[624]\ttrain-mlogloss:0.73596\tvalid-mlogloss:0.74734\n",
      "[625]\ttrain-mlogloss:0.73534\tvalid-mlogloss:0.74673\n",
      "[626]\ttrain-mlogloss:0.73461\tvalid-mlogloss:0.74599\n",
      "[627]\ttrain-mlogloss:0.73400\tvalid-mlogloss:0.74541\n",
      "[628]\ttrain-mlogloss:0.73355\tvalid-mlogloss:0.74495\n",
      "[629]\ttrain-mlogloss:0.73284\tvalid-mlogloss:0.74423\n",
      "[630]\ttrain-mlogloss:0.73244\tvalid-mlogloss:0.74385\n",
      "[631]\ttrain-mlogloss:0.73194\tvalid-mlogloss:0.74335\n",
      "[632]\ttrain-mlogloss:0.73149\tvalid-mlogloss:0.74289\n",
      "[633]\ttrain-mlogloss:0.73086\tvalid-mlogloss:0.74225\n",
      "[634]\ttrain-mlogloss:0.73021\tvalid-mlogloss:0.74157\n",
      "[635]\ttrain-mlogloss:0.72965\tvalid-mlogloss:0.74101\n",
      "[636]\ttrain-mlogloss:0.72916\tvalid-mlogloss:0.74054\n",
      "[637]\ttrain-mlogloss:0.72850\tvalid-mlogloss:0.73988\n",
      "[638]\ttrain-mlogloss:0.72810\tvalid-mlogloss:0.73952\n",
      "[639]\ttrain-mlogloss:0.72739\tvalid-mlogloss:0.73882\n",
      "[640]\ttrain-mlogloss:0.72680\tvalid-mlogloss:0.73821\n",
      "[641]\ttrain-mlogloss:0.72619\tvalid-mlogloss:0.73759\n",
      "[642]\ttrain-mlogloss:0.72561\tvalid-mlogloss:0.73705\n",
      "[643]\ttrain-mlogloss:0.72510\tvalid-mlogloss:0.73659\n",
      "[644]\ttrain-mlogloss:0.72460\tvalid-mlogloss:0.73608\n",
      "[645]\ttrain-mlogloss:0.72409\tvalid-mlogloss:0.73557\n",
      "[646]\ttrain-mlogloss:0.72350\tvalid-mlogloss:0.73500\n",
      "[647]\ttrain-mlogloss:0.72314\tvalid-mlogloss:0.73465\n",
      "[648]\ttrain-mlogloss:0.72237\tvalid-mlogloss:0.73389\n",
      "[649]\ttrain-mlogloss:0.72189\tvalid-mlogloss:0.73340\n",
      "[650]\ttrain-mlogloss:0.72137\tvalid-mlogloss:0.73292\n",
      "[651]\ttrain-mlogloss:0.72095\tvalid-mlogloss:0.73248\n",
      "[652]\ttrain-mlogloss:0.72036\tvalid-mlogloss:0.73188\n",
      "[653]\ttrain-mlogloss:0.71993\tvalid-mlogloss:0.73147\n",
      "[654]\ttrain-mlogloss:0.71946\tvalid-mlogloss:0.73102\n",
      "[655]\ttrain-mlogloss:0.71870\tvalid-mlogloss:0.73028\n",
      "[656]\ttrain-mlogloss:0.71805\tvalid-mlogloss:0.72967\n",
      "[657]\ttrain-mlogloss:0.71748\tvalid-mlogloss:0.72910\n",
      "[658]\ttrain-mlogloss:0.71683\tvalid-mlogloss:0.72844\n",
      "[659]\ttrain-mlogloss:0.71623\tvalid-mlogloss:0.72785\n",
      "[660]\ttrain-mlogloss:0.71568\tvalid-mlogloss:0.72731\n",
      "[661]\ttrain-mlogloss:0.71505\tvalid-mlogloss:0.72668\n",
      "[662]\ttrain-mlogloss:0.71462\tvalid-mlogloss:0.72627\n",
      "[663]\ttrain-mlogloss:0.71389\tvalid-mlogloss:0.72554\n",
      "[664]\ttrain-mlogloss:0.71333\tvalid-mlogloss:0.72500\n",
      "[665]\ttrain-mlogloss:0.71274\tvalid-mlogloss:0.72445\n",
      "[666]\ttrain-mlogloss:0.71235\tvalid-mlogloss:0.72407\n",
      "[667]\ttrain-mlogloss:0.71175\tvalid-mlogloss:0.72352\n",
      "[668]\ttrain-mlogloss:0.71107\tvalid-mlogloss:0.72287\n",
      "[669]\ttrain-mlogloss:0.71038\tvalid-mlogloss:0.72217\n",
      "[670]\ttrain-mlogloss:0.70982\tvalid-mlogloss:0.72165\n",
      "[671]\ttrain-mlogloss:0.70915\tvalid-mlogloss:0.72097\n",
      "[672]\ttrain-mlogloss:0.70849\tvalid-mlogloss:0.72032\n",
      "[673]\ttrain-mlogloss:0.70799\tvalid-mlogloss:0.71983\n",
      "[674]\ttrain-mlogloss:0.70747\tvalid-mlogloss:0.71931\n",
      "[675]\ttrain-mlogloss:0.70710\tvalid-mlogloss:0.71894\n",
      "[676]\ttrain-mlogloss:0.70652\tvalid-mlogloss:0.71836\n",
      "[677]\ttrain-mlogloss:0.70591\tvalid-mlogloss:0.71777\n",
      "[678]\ttrain-mlogloss:0.70545\tvalid-mlogloss:0.71729\n",
      "[679]\ttrain-mlogloss:0.70488\tvalid-mlogloss:0.71673\n",
      "[680]\ttrain-mlogloss:0.70434\tvalid-mlogloss:0.71620\n",
      "[681]\ttrain-mlogloss:0.70388\tvalid-mlogloss:0.71575\n",
      "[682]\ttrain-mlogloss:0.70355\tvalid-mlogloss:0.71542\n",
      "[683]\ttrain-mlogloss:0.70302\tvalid-mlogloss:0.71488\n",
      "[684]\ttrain-mlogloss:0.70255\tvalid-mlogloss:0.71444\n",
      "[685]\ttrain-mlogloss:0.70203\tvalid-mlogloss:0.71394\n",
      "[686]\ttrain-mlogloss:0.70139\tvalid-mlogloss:0.71329\n",
      "[687]\ttrain-mlogloss:0.70111\tvalid-mlogloss:0.71302\n",
      "[688]\ttrain-mlogloss:0.70057\tvalid-mlogloss:0.71247\n",
      "[689]\ttrain-mlogloss:0.69998\tvalid-mlogloss:0.71190\n",
      "[690]\ttrain-mlogloss:0.69944\tvalid-mlogloss:0.71134\n",
      "[691]\ttrain-mlogloss:0.69902\tvalid-mlogloss:0.71091\n",
      "[692]\ttrain-mlogloss:0.69834\tvalid-mlogloss:0.71022\n",
      "[693]\ttrain-mlogloss:0.69767\tvalid-mlogloss:0.70954\n",
      "[694]\ttrain-mlogloss:0.69719\tvalid-mlogloss:0.70909\n",
      "[695]\ttrain-mlogloss:0.69653\tvalid-mlogloss:0.70845\n",
      "[696]\ttrain-mlogloss:0.69589\tvalid-mlogloss:0.70780\n",
      "[697]\ttrain-mlogloss:0.69517\tvalid-mlogloss:0.70708\n",
      "[698]\ttrain-mlogloss:0.69463\tvalid-mlogloss:0.70655\n",
      "[699]\ttrain-mlogloss:0.69404\tvalid-mlogloss:0.70597\n",
      "[700]\ttrain-mlogloss:0.69338\tvalid-mlogloss:0.70531\n",
      "[701]\ttrain-mlogloss:0.69280\tvalid-mlogloss:0.70472\n",
      "[702]\ttrain-mlogloss:0.69248\tvalid-mlogloss:0.70437\n",
      "[703]\ttrain-mlogloss:0.69212\tvalid-mlogloss:0.70401\n",
      "[704]\ttrain-mlogloss:0.69160\tvalid-mlogloss:0.70349\n",
      "[705]\ttrain-mlogloss:0.69136\tvalid-mlogloss:0.70325\n",
      "[706]\ttrain-mlogloss:0.69080\tvalid-mlogloss:0.70268\n",
      "[707]\ttrain-mlogloss:0.69025\tvalid-mlogloss:0.70215\n",
      "[708]\ttrain-mlogloss:0.68973\tvalid-mlogloss:0.70163\n",
      "[709]\ttrain-mlogloss:0.68920\tvalid-mlogloss:0.70110\n",
      "[710]\ttrain-mlogloss:0.68861\tvalid-mlogloss:0.70050\n",
      "[711]\ttrain-mlogloss:0.68806\tvalid-mlogloss:0.69999\n",
      "[712]\ttrain-mlogloss:0.68753\tvalid-mlogloss:0.69948\n",
      "[713]\ttrain-mlogloss:0.68701\tvalid-mlogloss:0.69900\n",
      "[714]\ttrain-mlogloss:0.68649\tvalid-mlogloss:0.69848\n",
      "[715]\ttrain-mlogloss:0.68621\tvalid-mlogloss:0.69820\n",
      "[716]\ttrain-mlogloss:0.68568\tvalid-mlogloss:0.69771\n",
      "[717]\ttrain-mlogloss:0.68515\tvalid-mlogloss:0.69717\n",
      "[718]\ttrain-mlogloss:0.68471\tvalid-mlogloss:0.69671\n",
      "[719]\ttrain-mlogloss:0.68434\tvalid-mlogloss:0.69634\n",
      "[720]\ttrain-mlogloss:0.68386\tvalid-mlogloss:0.69587\n",
      "[721]\ttrain-mlogloss:0.68343\tvalid-mlogloss:0.69543\n",
      "[722]\ttrain-mlogloss:0.68293\tvalid-mlogloss:0.69494\n",
      "[723]\ttrain-mlogloss:0.68245\tvalid-mlogloss:0.69444\n",
      "[724]\ttrain-mlogloss:0.68202\tvalid-mlogloss:0.69402\n",
      "[725]\ttrain-mlogloss:0.68137\tvalid-mlogloss:0.69339\n",
      "[726]\ttrain-mlogloss:0.68095\tvalid-mlogloss:0.69299\n",
      "[727]\ttrain-mlogloss:0.68042\tvalid-mlogloss:0.69251\n",
      "[728]\ttrain-mlogloss:0.68002\tvalid-mlogloss:0.69209\n",
      "[729]\ttrain-mlogloss:0.67953\tvalid-mlogloss:0.69162\n",
      "[730]\ttrain-mlogloss:0.67898\tvalid-mlogloss:0.69110\n",
      "[731]\ttrain-mlogloss:0.67858\tvalid-mlogloss:0.69072\n",
      "[732]\ttrain-mlogloss:0.67816\tvalid-mlogloss:0.69029\n",
      "[733]\ttrain-mlogloss:0.67766\tvalid-mlogloss:0.68978\n",
      "[734]\ttrain-mlogloss:0.67706\tvalid-mlogloss:0.68918\n",
      "[735]\ttrain-mlogloss:0.67655\tvalid-mlogloss:0.68871\n",
      "[736]\ttrain-mlogloss:0.67595\tvalid-mlogloss:0.68813\n",
      "[737]\ttrain-mlogloss:0.67537\tvalid-mlogloss:0.68757\n",
      "[738]\ttrain-mlogloss:0.67493\tvalid-mlogloss:0.68716\n",
      "[739]\ttrain-mlogloss:0.67455\tvalid-mlogloss:0.68677\n",
      "[740]\ttrain-mlogloss:0.67407\tvalid-mlogloss:0.68634\n",
      "[741]\ttrain-mlogloss:0.67353\tvalid-mlogloss:0.68583\n",
      "[742]\ttrain-mlogloss:0.67302\tvalid-mlogloss:0.68532\n",
      "[743]\ttrain-mlogloss:0.67244\tvalid-mlogloss:0.68470\n",
      "[744]\ttrain-mlogloss:0.67183\tvalid-mlogloss:0.68411\n",
      "[745]\ttrain-mlogloss:0.67114\tvalid-mlogloss:0.68342\n",
      "[746]\ttrain-mlogloss:0.67076\tvalid-mlogloss:0.68304\n",
      "[747]\ttrain-mlogloss:0.67016\tvalid-mlogloss:0.68245\n",
      "[748]\ttrain-mlogloss:0.66986\tvalid-mlogloss:0.68213\n",
      "[749]\ttrain-mlogloss:0.66935\tvalid-mlogloss:0.68164\n",
      "[750]\ttrain-mlogloss:0.66877\tvalid-mlogloss:0.68108\n",
      "[751]\ttrain-mlogloss:0.66844\tvalid-mlogloss:0.68076\n",
      "[752]\ttrain-mlogloss:0.66812\tvalid-mlogloss:0.68046\n",
      "[753]\ttrain-mlogloss:0.66756\tvalid-mlogloss:0.67989\n",
      "[754]\ttrain-mlogloss:0.66694\tvalid-mlogloss:0.67924\n",
      "[755]\ttrain-mlogloss:0.66643\tvalid-mlogloss:0.67872\n",
      "[756]\ttrain-mlogloss:0.66580\tvalid-mlogloss:0.67809\n",
      "[757]\ttrain-mlogloss:0.66537\tvalid-mlogloss:0.67771\n",
      "[758]\ttrain-mlogloss:0.66501\tvalid-mlogloss:0.67736\n",
      "[759]\ttrain-mlogloss:0.66451\tvalid-mlogloss:0.67686\n",
      "[760]\ttrain-mlogloss:0.66419\tvalid-mlogloss:0.67655\n",
      "[761]\ttrain-mlogloss:0.66386\tvalid-mlogloss:0.67622\n",
      "[762]\ttrain-mlogloss:0.66354\tvalid-mlogloss:0.67589\n",
      "[763]\ttrain-mlogloss:0.66312\tvalid-mlogloss:0.67549\n",
      "[764]\ttrain-mlogloss:0.66272\tvalid-mlogloss:0.67512\n",
      "[765]\ttrain-mlogloss:0.66232\tvalid-mlogloss:0.67475\n",
      "[766]\ttrain-mlogloss:0.66187\tvalid-mlogloss:0.67431\n",
      "[767]\ttrain-mlogloss:0.66133\tvalid-mlogloss:0.67375\n",
      "[768]\ttrain-mlogloss:0.66080\tvalid-mlogloss:0.67320\n",
      "[769]\ttrain-mlogloss:0.66037\tvalid-mlogloss:0.67279\n",
      "[770]\ttrain-mlogloss:0.65993\tvalid-mlogloss:0.67236\n",
      "[771]\ttrain-mlogloss:0.65955\tvalid-mlogloss:0.67196\n",
      "[772]\ttrain-mlogloss:0.65919\tvalid-mlogloss:0.67162\n",
      "[773]\ttrain-mlogloss:0.65870\tvalid-mlogloss:0.67116\n",
      "[774]\ttrain-mlogloss:0.65814\tvalid-mlogloss:0.67061\n",
      "[775]\ttrain-mlogloss:0.65758\tvalid-mlogloss:0.67006\n",
      "[776]\ttrain-mlogloss:0.65705\tvalid-mlogloss:0.66955\n",
      "[777]\ttrain-mlogloss:0.65662\tvalid-mlogloss:0.66909\n",
      "[778]\ttrain-mlogloss:0.65603\tvalid-mlogloss:0.66849\n",
      "[779]\ttrain-mlogloss:0.65553\tvalid-mlogloss:0.66798\n",
      "[780]\ttrain-mlogloss:0.65507\tvalid-mlogloss:0.66754\n",
      "[781]\ttrain-mlogloss:0.65465\tvalid-mlogloss:0.66713\n",
      "[782]\ttrain-mlogloss:0.65410\tvalid-mlogloss:0.66660\n",
      "[783]\ttrain-mlogloss:0.65345\tvalid-mlogloss:0.66592\n",
      "[784]\ttrain-mlogloss:0.65296\tvalid-mlogloss:0.66542\n",
      "[785]\ttrain-mlogloss:0.65261\tvalid-mlogloss:0.66509\n",
      "[786]\ttrain-mlogloss:0.65221\tvalid-mlogloss:0.66471\n",
      "[787]\ttrain-mlogloss:0.65177\tvalid-mlogloss:0.66429\n",
      "[788]\ttrain-mlogloss:0.65137\tvalid-mlogloss:0.66390\n",
      "[789]\ttrain-mlogloss:0.65108\tvalid-mlogloss:0.66362\n",
      "[790]\ttrain-mlogloss:0.65053\tvalid-mlogloss:0.66306\n",
      "[791]\ttrain-mlogloss:0.64995\tvalid-mlogloss:0.66247\n",
      "[792]\ttrain-mlogloss:0.64938\tvalid-mlogloss:0.66190\n",
      "[793]\ttrain-mlogloss:0.64883\tvalid-mlogloss:0.66137\n",
      "[794]\ttrain-mlogloss:0.64821\tvalid-mlogloss:0.66075\n",
      "[795]\ttrain-mlogloss:0.64778\tvalid-mlogloss:0.66032\n",
      "[796]\ttrain-mlogloss:0.64722\tvalid-mlogloss:0.65975\n",
      "[797]\ttrain-mlogloss:0.64682\tvalid-mlogloss:0.65933\n",
      "[798]\ttrain-mlogloss:0.64626\tvalid-mlogloss:0.65878\n",
      "[799]\ttrain-mlogloss:0.64572\tvalid-mlogloss:0.65825\n",
      "[800]\ttrain-mlogloss:0.64513\tvalid-mlogloss:0.65767\n",
      "[801]\ttrain-mlogloss:0.64471\tvalid-mlogloss:0.65722\n",
      "[802]\ttrain-mlogloss:0.64439\tvalid-mlogloss:0.65689\n",
      "[803]\ttrain-mlogloss:0.64396\tvalid-mlogloss:0.65644\n",
      "[804]\ttrain-mlogloss:0.64345\tvalid-mlogloss:0.65593\n",
      "[805]\ttrain-mlogloss:0.64289\tvalid-mlogloss:0.65535\n",
      "[806]\ttrain-mlogloss:0.64242\tvalid-mlogloss:0.65491\n",
      "[807]\ttrain-mlogloss:0.64205\tvalid-mlogloss:0.65456\n",
      "[808]\ttrain-mlogloss:0.64156\tvalid-mlogloss:0.65406\n",
      "[809]\ttrain-mlogloss:0.64114\tvalid-mlogloss:0.65368\n",
      "[810]\ttrain-mlogloss:0.64074\tvalid-mlogloss:0.65327\n",
      "[811]\ttrain-mlogloss:0.64018\tvalid-mlogloss:0.65273\n",
      "[812]\ttrain-mlogloss:0.63989\tvalid-mlogloss:0.65243\n",
      "[813]\ttrain-mlogloss:0.63946\tvalid-mlogloss:0.65201\n",
      "[814]\ttrain-mlogloss:0.63892\tvalid-mlogloss:0.65149\n",
      "[815]\ttrain-mlogloss:0.63844\tvalid-mlogloss:0.65101\n",
      "[816]\ttrain-mlogloss:0.63797\tvalid-mlogloss:0.65056\n",
      "[817]\ttrain-mlogloss:0.63744\tvalid-mlogloss:0.65005\n",
      "[818]\ttrain-mlogloss:0.63701\tvalid-mlogloss:0.64963\n",
      "[819]\ttrain-mlogloss:0.63656\tvalid-mlogloss:0.64918\n",
      "[820]\ttrain-mlogloss:0.63598\tvalid-mlogloss:0.64862\n",
      "[821]\ttrain-mlogloss:0.63553\tvalid-mlogloss:0.64816\n",
      "[822]\ttrain-mlogloss:0.63499\tvalid-mlogloss:0.64761\n",
      "[823]\ttrain-mlogloss:0.63446\tvalid-mlogloss:0.64708\n",
      "[824]\ttrain-mlogloss:0.63398\tvalid-mlogloss:0.64664\n",
      "[825]\ttrain-mlogloss:0.63344\tvalid-mlogloss:0.64609\n",
      "[826]\ttrain-mlogloss:0.63291\tvalid-mlogloss:0.64558\n",
      "[827]\ttrain-mlogloss:0.63237\tvalid-mlogloss:0.64503\n",
      "[828]\ttrain-mlogloss:0.63188\tvalid-mlogloss:0.64456\n",
      "[829]\ttrain-mlogloss:0.63158\tvalid-mlogloss:0.64424\n",
      "[830]\ttrain-mlogloss:0.63107\tvalid-mlogloss:0.64373\n",
      "[831]\ttrain-mlogloss:0.63062\tvalid-mlogloss:0.64329\n",
      "[832]\ttrain-mlogloss:0.63007\tvalid-mlogloss:0.64275\n",
      "[833]\ttrain-mlogloss:0.62951\tvalid-mlogloss:0.64221\n",
      "[834]\ttrain-mlogloss:0.62909\tvalid-mlogloss:0.64176\n",
      "[835]\ttrain-mlogloss:0.62869\tvalid-mlogloss:0.64136\n",
      "[836]\ttrain-mlogloss:0.62811\tvalid-mlogloss:0.64078\n",
      "[837]\ttrain-mlogloss:0.62770\tvalid-mlogloss:0.64035\n",
      "[838]\ttrain-mlogloss:0.62735\tvalid-mlogloss:0.63998\n",
      "[839]\ttrain-mlogloss:0.62683\tvalid-mlogloss:0.63944\n",
      "[840]\ttrain-mlogloss:0.62643\tvalid-mlogloss:0.63906\n",
      "[841]\ttrain-mlogloss:0.62584\tvalid-mlogloss:0.63846\n",
      "[842]\ttrain-mlogloss:0.62534\tvalid-mlogloss:0.63795\n",
      "[843]\ttrain-mlogloss:0.62485\tvalid-mlogloss:0.63747\n",
      "[844]\ttrain-mlogloss:0.62443\tvalid-mlogloss:0.63708\n",
      "[845]\ttrain-mlogloss:0.62403\tvalid-mlogloss:0.63668\n",
      "[846]\ttrain-mlogloss:0.62366\tvalid-mlogloss:0.63631\n",
      "[847]\ttrain-mlogloss:0.62322\tvalid-mlogloss:0.63585\n",
      "[848]\ttrain-mlogloss:0.62280\tvalid-mlogloss:0.63542\n",
      "[849]\ttrain-mlogloss:0.62233\tvalid-mlogloss:0.63497\n",
      "[850]\ttrain-mlogloss:0.62202\tvalid-mlogloss:0.63466\n",
      "[851]\ttrain-mlogloss:0.62154\tvalid-mlogloss:0.63416\n",
      "[852]\ttrain-mlogloss:0.62109\tvalid-mlogloss:0.63373\n",
      "[853]\ttrain-mlogloss:0.62063\tvalid-mlogloss:0.63331\n",
      "[854]\ttrain-mlogloss:0.62009\tvalid-mlogloss:0.63276\n",
      "[855]\ttrain-mlogloss:0.61959\tvalid-mlogloss:0.63225\n",
      "[856]\ttrain-mlogloss:0.61923\tvalid-mlogloss:0.63189\n",
      "[857]\ttrain-mlogloss:0.61872\tvalid-mlogloss:0.63138\n",
      "[858]\ttrain-mlogloss:0.61814\tvalid-mlogloss:0.63074\n",
      "[859]\ttrain-mlogloss:0.61764\tvalid-mlogloss:0.63023\n",
      "[860]\ttrain-mlogloss:0.61712\tvalid-mlogloss:0.62973\n",
      "[861]\ttrain-mlogloss:0.61671\tvalid-mlogloss:0.62933\n",
      "[862]\ttrain-mlogloss:0.61624\tvalid-mlogloss:0.62891\n",
      "[863]\ttrain-mlogloss:0.61576\tvalid-mlogloss:0.62844\n",
      "[864]\ttrain-mlogloss:0.61526\tvalid-mlogloss:0.62796\n",
      "[865]\ttrain-mlogloss:0.61483\tvalid-mlogloss:0.62753\n",
      "[866]\ttrain-mlogloss:0.61435\tvalid-mlogloss:0.62707\n",
      "[867]\ttrain-mlogloss:0.61391\tvalid-mlogloss:0.62662\n",
      "[868]\ttrain-mlogloss:0.61349\tvalid-mlogloss:0.62618\n",
      "[869]\ttrain-mlogloss:0.61318\tvalid-mlogloss:0.62587\n",
      "[870]\ttrain-mlogloss:0.61274\tvalid-mlogloss:0.62546\n",
      "[871]\ttrain-mlogloss:0.61238\tvalid-mlogloss:0.62509\n",
      "[872]\ttrain-mlogloss:0.61192\tvalid-mlogloss:0.62465\n",
      "[873]\ttrain-mlogloss:0.61150\tvalid-mlogloss:0.62424\n",
      "[874]\ttrain-mlogloss:0.61102\tvalid-mlogloss:0.62374\n",
      "[875]\ttrain-mlogloss:0.61057\tvalid-mlogloss:0.62328\n",
      "[876]\ttrain-mlogloss:0.61012\tvalid-mlogloss:0.62284\n",
      "[877]\ttrain-mlogloss:0.60974\tvalid-mlogloss:0.62249\n",
      "[878]\ttrain-mlogloss:0.60931\tvalid-mlogloss:0.62206\n",
      "[879]\ttrain-mlogloss:0.60883\tvalid-mlogloss:0.62160\n",
      "[880]\ttrain-mlogloss:0.60834\tvalid-mlogloss:0.62112\n",
      "[881]\ttrain-mlogloss:0.60805\tvalid-mlogloss:0.62085\n",
      "[882]\ttrain-mlogloss:0.60777\tvalid-mlogloss:0.62056\n",
      "[883]\ttrain-mlogloss:0.60715\tvalid-mlogloss:0.61995\n",
      "[884]\ttrain-mlogloss:0.60669\tvalid-mlogloss:0.61946\n",
      "[885]\ttrain-mlogloss:0.60621\tvalid-mlogloss:0.61899\n",
      "[886]\ttrain-mlogloss:0.60583\tvalid-mlogloss:0.61863\n",
      "[887]\ttrain-mlogloss:0.60541\tvalid-mlogloss:0.61821\n",
      "[888]\ttrain-mlogloss:0.60490\tvalid-mlogloss:0.61771\n",
      "[889]\ttrain-mlogloss:0.60444\tvalid-mlogloss:0.61725\n",
      "[890]\ttrain-mlogloss:0.60409\tvalid-mlogloss:0.61689\n",
      "[891]\ttrain-mlogloss:0.60372\tvalid-mlogloss:0.61652\n",
      "[892]\ttrain-mlogloss:0.60325\tvalid-mlogloss:0.61607\n",
      "[893]\ttrain-mlogloss:0.60286\tvalid-mlogloss:0.61566\n",
      "[894]\ttrain-mlogloss:0.60240\tvalid-mlogloss:0.61522\n",
      "[895]\ttrain-mlogloss:0.60182\tvalid-mlogloss:0.61466\n",
      "[896]\ttrain-mlogloss:0.60132\tvalid-mlogloss:0.61417\n",
      "[897]\ttrain-mlogloss:0.60093\tvalid-mlogloss:0.61379\n",
      "[898]\ttrain-mlogloss:0.60051\tvalid-mlogloss:0.61337\n",
      "[899]\ttrain-mlogloss:0.60016\tvalid-mlogloss:0.61299\n",
      "[900]\ttrain-mlogloss:0.59985\tvalid-mlogloss:0.61268\n",
      "[901]\ttrain-mlogloss:0.59948\tvalid-mlogloss:0.61232\n",
      "[902]\ttrain-mlogloss:0.59892\tvalid-mlogloss:0.61175\n",
      "[903]\ttrain-mlogloss:0.59850\tvalid-mlogloss:0.61133\n",
      "[904]\ttrain-mlogloss:0.59811\tvalid-mlogloss:0.61094\n",
      "[905]\ttrain-mlogloss:0.59777\tvalid-mlogloss:0.61061\n",
      "[906]\ttrain-mlogloss:0.59731\tvalid-mlogloss:0.61016\n",
      "[907]\ttrain-mlogloss:0.59692\tvalid-mlogloss:0.60977\n",
      "[908]\ttrain-mlogloss:0.59648\tvalid-mlogloss:0.60933\n",
      "[909]\ttrain-mlogloss:0.59613\tvalid-mlogloss:0.60900\n",
      "[910]\ttrain-mlogloss:0.59569\tvalid-mlogloss:0.60855\n",
      "[911]\ttrain-mlogloss:0.59535\tvalid-mlogloss:0.60821\n",
      "[912]\ttrain-mlogloss:0.59492\tvalid-mlogloss:0.60779\n",
      "[913]\ttrain-mlogloss:0.59447\tvalid-mlogloss:0.60731\n",
      "[914]\ttrain-mlogloss:0.59405\tvalid-mlogloss:0.60690\n",
      "[915]\ttrain-mlogloss:0.59372\tvalid-mlogloss:0.60657\n",
      "[916]\ttrain-mlogloss:0.59332\tvalid-mlogloss:0.60616\n",
      "[917]\ttrain-mlogloss:0.59295\tvalid-mlogloss:0.60577\n",
      "[918]\ttrain-mlogloss:0.59250\tvalid-mlogloss:0.60532\n",
      "[919]\ttrain-mlogloss:0.59203\tvalid-mlogloss:0.60483\n",
      "[920]\ttrain-mlogloss:0.59178\tvalid-mlogloss:0.60461\n",
      "[921]\ttrain-mlogloss:0.59137\tvalid-mlogloss:0.60419\n",
      "[922]\ttrain-mlogloss:0.59087\tvalid-mlogloss:0.60367\n",
      "[923]\ttrain-mlogloss:0.59039\tvalid-mlogloss:0.60320\n",
      "[924]\ttrain-mlogloss:0.59004\tvalid-mlogloss:0.60288\n",
      "[925]\ttrain-mlogloss:0.58961\tvalid-mlogloss:0.60246\n",
      "[926]\ttrain-mlogloss:0.58910\tvalid-mlogloss:0.60194\n",
      "[927]\ttrain-mlogloss:0.58867\tvalid-mlogloss:0.60153\n",
      "[928]\ttrain-mlogloss:0.58829\tvalid-mlogloss:0.60118\n",
      "[929]\ttrain-mlogloss:0.58786\tvalid-mlogloss:0.60073\n",
      "[930]\ttrain-mlogloss:0.58743\tvalid-mlogloss:0.60030\n",
      "[931]\ttrain-mlogloss:0.58706\tvalid-mlogloss:0.59993\n",
      "[932]\ttrain-mlogloss:0.58667\tvalid-mlogloss:0.59952\n",
      "[933]\ttrain-mlogloss:0.58622\tvalid-mlogloss:0.59909\n",
      "[934]\ttrain-mlogloss:0.58591\tvalid-mlogloss:0.59879\n",
      "[935]\ttrain-mlogloss:0.58549\tvalid-mlogloss:0.59835\n",
      "[936]\ttrain-mlogloss:0.58513\tvalid-mlogloss:0.59797\n",
      "[937]\ttrain-mlogloss:0.58458\tvalid-mlogloss:0.59743\n",
      "[938]\ttrain-mlogloss:0.58422\tvalid-mlogloss:0.59706\n",
      "[939]\ttrain-mlogloss:0.58403\tvalid-mlogloss:0.59689\n",
      "[940]\ttrain-mlogloss:0.58347\tvalid-mlogloss:0.59635\n",
      "[941]\ttrain-mlogloss:0.58298\tvalid-mlogloss:0.59588\n",
      "[942]\ttrain-mlogloss:0.58258\tvalid-mlogloss:0.59550\n",
      "[943]\ttrain-mlogloss:0.58227\tvalid-mlogloss:0.59517\n",
      "[944]\ttrain-mlogloss:0.58203\tvalid-mlogloss:0.59492\n",
      "[945]\ttrain-mlogloss:0.58152\tvalid-mlogloss:0.59441\n",
      "[946]\ttrain-mlogloss:0.58105\tvalid-mlogloss:0.59392\n",
      "[947]\ttrain-mlogloss:0.58054\tvalid-mlogloss:0.59341\n",
      "[948]\ttrain-mlogloss:0.58018\tvalid-mlogloss:0.59306\n",
      "[949]\ttrain-mlogloss:0.57977\tvalid-mlogloss:0.59262\n",
      "[950]\ttrain-mlogloss:0.57944\tvalid-mlogloss:0.59228\n",
      "[951]\ttrain-mlogloss:0.57914\tvalid-mlogloss:0.59200\n",
      "[952]\ttrain-mlogloss:0.57886\tvalid-mlogloss:0.59172\n",
      "[953]\ttrain-mlogloss:0.57847\tvalid-mlogloss:0.59133\n",
      "[954]\ttrain-mlogloss:0.57804\tvalid-mlogloss:0.59089\n",
      "[955]\ttrain-mlogloss:0.57747\tvalid-mlogloss:0.59034\n",
      "[956]\ttrain-mlogloss:0.57700\tvalid-mlogloss:0.58986\n",
      "[957]\ttrain-mlogloss:0.57656\tvalid-mlogloss:0.58942\n",
      "[958]\ttrain-mlogloss:0.57615\tvalid-mlogloss:0.58903\n",
      "[959]\ttrain-mlogloss:0.57577\tvalid-mlogloss:0.58868\n",
      "[960]\ttrain-mlogloss:0.57535\tvalid-mlogloss:0.58828\n",
      "[961]\ttrain-mlogloss:0.57494\tvalid-mlogloss:0.58789\n",
      "[962]\ttrain-mlogloss:0.57458\tvalid-mlogloss:0.58754\n",
      "[963]\ttrain-mlogloss:0.57408\tvalid-mlogloss:0.58703\n",
      "[964]\ttrain-mlogloss:0.57375\tvalid-mlogloss:0.58669\n",
      "[965]\ttrain-mlogloss:0.57335\tvalid-mlogloss:0.58630\n",
      "[966]\ttrain-mlogloss:0.57312\tvalid-mlogloss:0.58606\n",
      "[967]\ttrain-mlogloss:0.57256\tvalid-mlogloss:0.58548\n",
      "[968]\ttrain-mlogloss:0.57207\tvalid-mlogloss:0.58500\n",
      "[969]\ttrain-mlogloss:0.57182\tvalid-mlogloss:0.58474\n",
      "[970]\ttrain-mlogloss:0.57142\tvalid-mlogloss:0.58434\n",
      "[971]\ttrain-mlogloss:0.57103\tvalid-mlogloss:0.58392\n",
      "[972]\ttrain-mlogloss:0.57058\tvalid-mlogloss:0.58346\n",
      "[973]\ttrain-mlogloss:0.57016\tvalid-mlogloss:0.58304\n",
      "[974]\ttrain-mlogloss:0.56979\tvalid-mlogloss:0.58266\n",
      "[975]\ttrain-mlogloss:0.56945\tvalid-mlogloss:0.58234\n",
      "[976]\ttrain-mlogloss:0.56909\tvalid-mlogloss:0.58199\n",
      "[977]\ttrain-mlogloss:0.56871\tvalid-mlogloss:0.58162\n",
      "[978]\ttrain-mlogloss:0.56833\tvalid-mlogloss:0.58123\n",
      "[979]\ttrain-mlogloss:0.56789\tvalid-mlogloss:0.58076\n",
      "[980]\ttrain-mlogloss:0.56750\tvalid-mlogloss:0.58036\n",
      "[981]\ttrain-mlogloss:0.56700\tvalid-mlogloss:0.57988\n",
      "[982]\ttrain-mlogloss:0.56676\tvalid-mlogloss:0.57961\n",
      "[983]\ttrain-mlogloss:0.56642\tvalid-mlogloss:0.57927\n",
      "[984]\ttrain-mlogloss:0.56599\tvalid-mlogloss:0.57885\n",
      "[985]\ttrain-mlogloss:0.56560\tvalid-mlogloss:0.57847\n",
      "[986]\ttrain-mlogloss:0.56525\tvalid-mlogloss:0.57812\n",
      "[987]\ttrain-mlogloss:0.56481\tvalid-mlogloss:0.57766\n",
      "[988]\ttrain-mlogloss:0.56437\tvalid-mlogloss:0.57723\n",
      "[989]\ttrain-mlogloss:0.56384\tvalid-mlogloss:0.57671\n",
      "[990]\ttrain-mlogloss:0.56333\tvalid-mlogloss:0.57617\n",
      "[991]\ttrain-mlogloss:0.56289\tvalid-mlogloss:0.57572\n",
      "[992]\ttrain-mlogloss:0.56242\tvalid-mlogloss:0.57525\n",
      "[993]\ttrain-mlogloss:0.56193\tvalid-mlogloss:0.57476\n",
      "[994]\ttrain-mlogloss:0.56157\tvalid-mlogloss:0.57440\n",
      "[995]\ttrain-mlogloss:0.56115\tvalid-mlogloss:0.57400\n",
      "[996]\ttrain-mlogloss:0.56079\tvalid-mlogloss:0.57364\n",
      "[997]\ttrain-mlogloss:0.56037\tvalid-mlogloss:0.57323\n",
      "[998]\ttrain-mlogloss:0.55995\tvalid-mlogloss:0.57281\n",
      "[999]\ttrain-mlogloss:0.55959\tvalid-mlogloss:0.57248\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# DMatrix für XGBoost erstellen\n",
    "dtrain = xgb.DMatrix(X_train_clean, label=y_train)\n",
    "dvalid = xgb.DMatrix(valid_X_clean, label=valid_y)\n",
    "\n",
    "# Parameter für XGBoost\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": len(np.unique(y_train)),\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    **best_params  # aus RandomizedSearchCV\n",
    "}\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "\n",
    "# Training mit Early Stopping\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=30,\n",
    "    verbose_eval=True\n",
    ")\n",
    "\n",
    "# Vorhersage auf Testset\n",
    "dtest = xgb.DMatrix(test_X_clean)\n",
    "preds = bst.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a525b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beste Parameter\n",
    "print(\"Beste Parameter:\", random_search.best_params_)\n",
    "print(\"Bester CV-Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finale Bewertung auf Test\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(test_X)\n",
    "\n",
    "print(\"Accuracy (Test):\", accuracy_score(test_y, y_pred))\n",
    "print(\"Classification Report (Test):\\n\", classification_report(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "603a47d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faulh\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:53:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 0.6}\n",
      "[0]\ttrain-mlogloss:1.57894\tvalid-mlogloss:1.59310\n",
      "[1]\ttrain-mlogloss:1.55076\tvalid-mlogloss:1.57877\n",
      "[2]\ttrain-mlogloss:1.51975\tvalid-mlogloss:1.56239\n",
      "[3]\ttrain-mlogloss:1.48526\tvalid-mlogloss:1.54132\n",
      "[4]\ttrain-mlogloss:1.45865\tvalid-mlogloss:1.52830\n",
      "[5]\ttrain-mlogloss:1.43703\tvalid-mlogloss:1.51852\n",
      "[6]\ttrain-mlogloss:1.41807\tvalid-mlogloss:1.51047\n",
      "[7]\ttrain-mlogloss:1.39845\tvalid-mlogloss:1.50211\n",
      "[8]\ttrain-mlogloss:1.37928\tvalid-mlogloss:1.49448\n",
      "[9]\ttrain-mlogloss:1.36075\tvalid-mlogloss:1.48630\n",
      "[10]\ttrain-mlogloss:1.34076\tvalid-mlogloss:1.47812\n",
      "[11]\ttrain-mlogloss:1.32685\tvalid-mlogloss:1.47331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faulh\\anaconda3\\Lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [17:53:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\ttrain-mlogloss:1.31332\tvalid-mlogloss:1.46850\n",
      "[13]\ttrain-mlogloss:1.29896\tvalid-mlogloss:1.46345\n",
      "[14]\ttrain-mlogloss:1.28452\tvalid-mlogloss:1.45827\n",
      "[15]\ttrain-mlogloss:1.27035\tvalid-mlogloss:1.45348\n",
      "[16]\ttrain-mlogloss:1.25574\tvalid-mlogloss:1.44701\n",
      "[17]\ttrain-mlogloss:1.24568\tvalid-mlogloss:1.44470\n",
      "[18]\ttrain-mlogloss:1.23483\tvalid-mlogloss:1.44169\n",
      "[19]\ttrain-mlogloss:1.22124\tvalid-mlogloss:1.43731\n",
      "[20]\ttrain-mlogloss:1.21262\tvalid-mlogloss:1.43520\n",
      "[21]\ttrain-mlogloss:1.20204\tvalid-mlogloss:1.43211\n",
      "[22]\ttrain-mlogloss:1.19242\tvalid-mlogloss:1.42936\n",
      "[23]\ttrain-mlogloss:1.18309\tvalid-mlogloss:1.42800\n",
      "[24]\ttrain-mlogloss:1.17197\tvalid-mlogloss:1.42410\n",
      "[25]\ttrain-mlogloss:1.16252\tvalid-mlogloss:1.42058\n",
      "[26]\ttrain-mlogloss:1.15350\tvalid-mlogloss:1.41784\n",
      "[27]\ttrain-mlogloss:1.14734\tvalid-mlogloss:1.41757\n",
      "[28]\ttrain-mlogloss:1.13843\tvalid-mlogloss:1.41389\n",
      "[29]\ttrain-mlogloss:1.13014\tvalid-mlogloss:1.41266\n",
      "[30]\ttrain-mlogloss:1.12333\tvalid-mlogloss:1.41176\n",
      "[31]\ttrain-mlogloss:1.11743\tvalid-mlogloss:1.41079\n",
      "[32]\ttrain-mlogloss:1.10985\tvalid-mlogloss:1.40980\n",
      "[33]\ttrain-mlogloss:1.10429\tvalid-mlogloss:1.40914\n",
      "[34]\ttrain-mlogloss:1.09745\tvalid-mlogloss:1.40802\n",
      "[35]\ttrain-mlogloss:1.09063\tvalid-mlogloss:1.40581\n",
      "[36]\ttrain-mlogloss:1.08461\tvalid-mlogloss:1.40548\n",
      "[37]\ttrain-mlogloss:1.07948\tvalid-mlogloss:1.40467\n",
      "[38]\ttrain-mlogloss:1.07439\tvalid-mlogloss:1.40442\n",
      "[39]\ttrain-mlogloss:1.06977\tvalid-mlogloss:1.40512\n",
      "[40]\ttrain-mlogloss:1.06337\tvalid-mlogloss:1.40388\n",
      "[41]\ttrain-mlogloss:1.05735\tvalid-mlogloss:1.40332\n",
      "[42]\ttrain-mlogloss:1.05256\tvalid-mlogloss:1.40259\n",
      "[43]\ttrain-mlogloss:1.04653\tvalid-mlogloss:1.39981\n",
      "[44]\ttrain-mlogloss:1.04222\tvalid-mlogloss:1.39922\n",
      "[45]\ttrain-mlogloss:1.03704\tvalid-mlogloss:1.39866\n",
      "[46]\ttrain-mlogloss:1.03169\tvalid-mlogloss:1.39812\n",
      "[47]\ttrain-mlogloss:1.02566\tvalid-mlogloss:1.39847\n",
      "[48]\ttrain-mlogloss:1.02066\tvalid-mlogloss:1.39794\n",
      "[49]\ttrain-mlogloss:1.01638\tvalid-mlogloss:1.39746\n",
      "[50]\ttrain-mlogloss:1.01296\tvalid-mlogloss:1.39730\n",
      "[51]\ttrain-mlogloss:1.00764\tvalid-mlogloss:1.39669\n",
      "[52]\ttrain-mlogloss:1.00520\tvalid-mlogloss:1.39613\n",
      "[53]\ttrain-mlogloss:1.00119\tvalid-mlogloss:1.39572\n",
      "[54]\ttrain-mlogloss:0.99800\tvalid-mlogloss:1.39538\n",
      "[55]\ttrain-mlogloss:0.99197\tvalid-mlogloss:1.39431\n",
      "[56]\ttrain-mlogloss:0.98738\tvalid-mlogloss:1.39343\n",
      "[57]\ttrain-mlogloss:0.98280\tvalid-mlogloss:1.39312\n",
      "[58]\ttrain-mlogloss:0.97801\tvalid-mlogloss:1.39329\n",
      "[59]\ttrain-mlogloss:0.97472\tvalid-mlogloss:1.39346\n",
      "[60]\ttrain-mlogloss:0.96996\tvalid-mlogloss:1.39192\n",
      "[61]\ttrain-mlogloss:0.96561\tvalid-mlogloss:1.39117\n",
      "[62]\ttrain-mlogloss:0.96299\tvalid-mlogloss:1.39090\n",
      "[63]\ttrain-mlogloss:0.95867\tvalid-mlogloss:1.38990\n",
      "[64]\ttrain-mlogloss:0.95401\tvalid-mlogloss:1.38987\n",
      "[65]\ttrain-mlogloss:0.95028\tvalid-mlogloss:1.38988\n",
      "[66]\ttrain-mlogloss:0.94553\tvalid-mlogloss:1.38953\n",
      "[67]\ttrain-mlogloss:0.93879\tvalid-mlogloss:1.38767\n",
      "[68]\ttrain-mlogloss:0.93632\tvalid-mlogloss:1.38784\n",
      "[69]\ttrain-mlogloss:0.93200\tvalid-mlogloss:1.38766\n",
      "[70]\ttrain-mlogloss:0.92891\tvalid-mlogloss:1.38787\n",
      "[71]\ttrain-mlogloss:0.92303\tvalid-mlogloss:1.38767\n",
      "[72]\ttrain-mlogloss:0.91913\tvalid-mlogloss:1.38801\n",
      "[73]\ttrain-mlogloss:0.91593\tvalid-mlogloss:1.38825\n",
      "[74]\ttrain-mlogloss:0.91302\tvalid-mlogloss:1.38864\n",
      "[75]\ttrain-mlogloss:0.91024\tvalid-mlogloss:1.38888\n",
      "[76]\ttrain-mlogloss:0.90612\tvalid-mlogloss:1.38899\n",
      "[77]\ttrain-mlogloss:0.90085\tvalid-mlogloss:1.38951\n",
      "[78]\ttrain-mlogloss:0.89751\tvalid-mlogloss:1.38970\n",
      "[79]\ttrain-mlogloss:0.89439\tvalid-mlogloss:1.38970\n",
      "[80]\ttrain-mlogloss:0.89009\tvalid-mlogloss:1.38945\n",
      "[81]\ttrain-mlogloss:0.88679\tvalid-mlogloss:1.38972\n",
      "[82]\ttrain-mlogloss:0.88293\tvalid-mlogloss:1.38886\n",
      "[83]\ttrain-mlogloss:0.87864\tvalid-mlogloss:1.38835\n",
      "[84]\ttrain-mlogloss:0.87406\tvalid-mlogloss:1.38775\n",
      "[85]\ttrain-mlogloss:0.87106\tvalid-mlogloss:1.38842\n",
      "[86]\ttrain-mlogloss:0.86684\tvalid-mlogloss:1.38952\n",
      "[87]\ttrain-mlogloss:0.86307\tvalid-mlogloss:1.38980\n",
      "[88]\ttrain-mlogloss:0.85848\tvalid-mlogloss:1.38984\n",
      "[89]\ttrain-mlogloss:0.85588\tvalid-mlogloss:1.38979\n",
      "[90]\ttrain-mlogloss:0.85304\tvalid-mlogloss:1.39009\n",
      "[91]\ttrain-mlogloss:0.84681\tvalid-mlogloss:1.38985\n",
      "[92]\ttrain-mlogloss:0.84144\tvalid-mlogloss:1.38944\n",
      "[93]\ttrain-mlogloss:0.83892\tvalid-mlogloss:1.38883\n",
      "[94]\ttrain-mlogloss:0.83540\tvalid-mlogloss:1.38870\n",
      "[95]\ttrain-mlogloss:0.83139\tvalid-mlogloss:1.38829\n",
      "[96]\ttrain-mlogloss:0.82736\tvalid-mlogloss:1.38802\n",
      "[97]\ttrain-mlogloss:0.82419\tvalid-mlogloss:1.38690\n",
      "[98]\ttrain-mlogloss:0.82162\tvalid-mlogloss:1.38718\n",
      "[99]\ttrain-mlogloss:0.81773\tvalid-mlogloss:1.38815\n",
      "[100]\ttrain-mlogloss:0.81501\tvalid-mlogloss:1.38843\n",
      "[101]\ttrain-mlogloss:0.81149\tvalid-mlogloss:1.38773\n",
      "[102]\ttrain-mlogloss:0.80908\tvalid-mlogloss:1.38766\n",
      "[103]\ttrain-mlogloss:0.80562\tvalid-mlogloss:1.38805\n",
      "[104]\ttrain-mlogloss:0.80195\tvalid-mlogloss:1.38832\n",
      "[105]\ttrain-mlogloss:0.79942\tvalid-mlogloss:1.38828\n",
      "[106]\ttrain-mlogloss:0.79755\tvalid-mlogloss:1.38788\n",
      "[107]\ttrain-mlogloss:0.79305\tvalid-mlogloss:1.38707\n",
      "[108]\ttrain-mlogloss:0.78951\tvalid-mlogloss:1.38732\n",
      "[109]\ttrain-mlogloss:0.78714\tvalid-mlogloss:1.38762\n",
      "[110]\ttrain-mlogloss:0.78457\tvalid-mlogloss:1.38734\n",
      "[111]\ttrain-mlogloss:0.78285\tvalid-mlogloss:1.38729\n",
      "[112]\ttrain-mlogloss:0.77906\tvalid-mlogloss:1.38750\n",
      "[113]\ttrain-mlogloss:0.77659\tvalid-mlogloss:1.38786\n",
      "[114]\ttrain-mlogloss:0.77160\tvalid-mlogloss:1.38705\n",
      "[115]\ttrain-mlogloss:0.76812\tvalid-mlogloss:1.38705\n",
      "[116]\ttrain-mlogloss:0.76390\tvalid-mlogloss:1.38692\n",
      "[117]\ttrain-mlogloss:0.76090\tvalid-mlogloss:1.38675\n",
      "[118]\ttrain-mlogloss:0.75809\tvalid-mlogloss:1.38713\n",
      "[119]\ttrain-mlogloss:0.75422\tvalid-mlogloss:1.38723\n",
      "[120]\ttrain-mlogloss:0.74998\tvalid-mlogloss:1.38692\n",
      "[121]\ttrain-mlogloss:0.74724\tvalid-mlogloss:1.38737\n",
      "[122]\ttrain-mlogloss:0.74385\tvalid-mlogloss:1.38685\n",
      "[123]\ttrain-mlogloss:0.74126\tvalid-mlogloss:1.38729\n",
      "[124]\ttrain-mlogloss:0.73760\tvalid-mlogloss:1.38684\n",
      "[125]\ttrain-mlogloss:0.73543\tvalid-mlogloss:1.38666\n",
      "[126]\ttrain-mlogloss:0.73205\tvalid-mlogloss:1.38687\n",
      "[127]\ttrain-mlogloss:0.72963\tvalid-mlogloss:1.38776\n",
      "[128]\ttrain-mlogloss:0.72568\tvalid-mlogloss:1.38759\n",
      "[129]\ttrain-mlogloss:0.72329\tvalid-mlogloss:1.38793\n",
      "[130]\ttrain-mlogloss:0.72013\tvalid-mlogloss:1.38872\n",
      "[131]\ttrain-mlogloss:0.71849\tvalid-mlogloss:1.38862\n",
      "[132]\ttrain-mlogloss:0.71533\tvalid-mlogloss:1.38871\n",
      "[133]\ttrain-mlogloss:0.71149\tvalid-mlogloss:1.38913\n",
      "[134]\ttrain-mlogloss:0.71014\tvalid-mlogloss:1.38929\n",
      "[135]\ttrain-mlogloss:0.70738\tvalid-mlogloss:1.38925\n",
      "[136]\ttrain-mlogloss:0.70448\tvalid-mlogloss:1.38932\n",
      "[137]\ttrain-mlogloss:0.70158\tvalid-mlogloss:1.38921\n",
      "[138]\ttrain-mlogloss:0.69912\tvalid-mlogloss:1.38978\n",
      "[139]\ttrain-mlogloss:0.69642\tvalid-mlogloss:1.38992\n",
      "[140]\ttrain-mlogloss:0.69328\tvalid-mlogloss:1.39105\n",
      "[141]\ttrain-mlogloss:0.69125\tvalid-mlogloss:1.39093\n",
      "[142]\ttrain-mlogloss:0.68963\tvalid-mlogloss:1.39134\n",
      "[143]\ttrain-mlogloss:0.68751\tvalid-mlogloss:1.39175\n",
      "[144]\ttrain-mlogloss:0.68626\tvalid-mlogloss:1.39173\n",
      "[145]\ttrain-mlogloss:0.68391\tvalid-mlogloss:1.39282\n",
      "[146]\ttrain-mlogloss:0.68164\tvalid-mlogloss:1.39311\n",
      "[147]\ttrain-mlogloss:0.67902\tvalid-mlogloss:1.39312\n",
      "[148]\ttrain-mlogloss:0.67567\tvalid-mlogloss:1.39283\n",
      "[149]\ttrain-mlogloss:0.67377\tvalid-mlogloss:1.39283\n",
      "[150]\ttrain-mlogloss:0.67062\tvalid-mlogloss:1.39220\n",
      "[151]\ttrain-mlogloss:0.66820\tvalid-mlogloss:1.39208\n",
      "[152]\ttrain-mlogloss:0.66616\tvalid-mlogloss:1.39285\n",
      "[153]\ttrain-mlogloss:0.66351\tvalid-mlogloss:1.39291\n",
      "[154]\ttrain-mlogloss:0.66131\tvalid-mlogloss:1.39266\n",
      "[155]\ttrain-mlogloss:0.65830\tvalid-mlogloss:1.39257\n",
      "Test Accuracy: 0.42601054481546574\n",
      "Test F1: 0.4206088848884405\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ======================\n",
    "# 1. Daten laden\n",
    "# ======================\n",
    "train_df = pd.read_csv(\"Data/preprocessed/train_preprocessed.csv\")\n",
    "X_train = train_df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y_train = train_df[\"AdoptionSpeed\"].astype(int).values\n",
    "\n",
    "valid_X = pd.read_csv(\"Data/preprocessed/valid_preprocessed.csv\")\n",
    "valid_y = pd.read_csv(\"Data/preprocessed/valid_target.csv\")[\"AdoptionSpeed\"].astype(int).values\n",
    "\n",
    "test_X = pd.read_csv(\"Data/preprocessed/test_preprocessed.csv\")\n",
    "test_y = pd.read_csv(\"Data/preprocessed/test_target.csv\")[\"AdoptionSpeed\"].astype(int).values\n",
    "\n",
    "# ======================\n",
    "# 2. Object-Spalten entfernen\n",
    "# ======================\n",
    "object_cols = X_train.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "X_train_clean = X_train.drop(columns=object_cols)\n",
    "valid_X_clean = valid_X.drop(columns=object_cols)\n",
    "test_X_clean  = test_X.drop(columns=object_cols)\n",
    "\n",
    "# ======================\n",
    "# 3. RandomizedSearchCV für Hyperparameter\n",
    "# ======================\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=len(np.unique(y_train)),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Beispielparameter-Raum (anpassen)\n",
    "param_dist = {\n",
    "    \"max_depth\": [3, 4, 5, 6, 7],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"n_estimators\": [100, 200, 300, 500],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"gamma\": [0, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_clean, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "\n",
    "# ======================\n",
    "# 4. Bestes Modell mit xgb.train + Early Stopping\n",
    "# ======================\n",
    "# DMatrix erstellen\n",
    "dtrain = xgb.DMatrix(X_train_clean, label=y_train)\n",
    "dvalid = xgb.DMatrix(valid_X_clean, label=valid_y)\n",
    "dtest  = xgb.DMatrix(test_X_clean)\n",
    "\n",
    "# Parameter für xgb.train vorbereiten\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": len(np.unique(y_train)),\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    **best_params\n",
    "}\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=30,\n",
    "    verbose_eval=True\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 5. Vorhersage & Evaluation\n",
    "# ======================\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "accuracy = accuracy_score(test_y, preds)\n",
    "f1 = f1_score(test_y, preds, average=\"weighted\")\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Test F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9cf0c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faulh\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:57:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.2}\n",
      "[0]\ttrain-mlogloss:1.55158\tvalid-mlogloss:1.57061\n",
      "[1]\ttrain-mlogloss:1.50842\tvalid-mlogloss:1.54281\n",
      "[2]\ttrain-mlogloss:1.47339\tvalid-mlogloss:1.52276\n",
      "[3]\ttrain-mlogloss:1.44512\tvalid-mlogloss:1.50792\n",
      "[4]\ttrain-mlogloss:1.41739\tvalid-mlogloss:1.49283\n",
      "[5]\ttrain-mlogloss:1.39469\tvalid-mlogloss:1.48297\n",
      "[6]\ttrain-mlogloss:1.37314\tvalid-mlogloss:1.47396\n",
      "[7]\ttrain-mlogloss:1.35310\tvalid-mlogloss:1.46573\n",
      "[8]\ttrain-mlogloss:1.33382\tvalid-mlogloss:1.45896\n",
      "[9]\ttrain-mlogloss:1.31719\tvalid-mlogloss:1.45359\n",
      "[10]\ttrain-mlogloss:1.30261\tvalid-mlogloss:1.44688\n",
      "[11]\ttrain-mlogloss:1.28727\tvalid-mlogloss:1.44168\n",
      "[12]\ttrain-mlogloss:1.27282\tvalid-mlogloss:1.43670\n",
      "[13]\ttrain-mlogloss:1.25907\tvalid-mlogloss:1.43324\n",
      "[14]\ttrain-mlogloss:1.24500\tvalid-mlogloss:1.42892\n",
      "[15]\ttrain-mlogloss:1.23161\tvalid-mlogloss:1.42504\n",
      "[16]\ttrain-mlogloss:1.22087\tvalid-mlogloss:1.42136\n",
      "[17]\ttrain-mlogloss:1.20971\tvalid-mlogloss:1.41891\n",
      "[18]\ttrain-mlogloss:1.19833\tvalid-mlogloss:1.41579\n",
      "[19]\ttrain-mlogloss:1.18807\tvalid-mlogloss:1.41372\n",
      "[20]\ttrain-mlogloss:1.17785\tvalid-mlogloss:1.41095\n",
      "[21]\ttrain-mlogloss:1.16942\tvalid-mlogloss:1.40901\n",
      "[22]\ttrain-mlogloss:1.16080\tvalid-mlogloss:1.40785\n",
      "[23]\ttrain-mlogloss:1.15305\tvalid-mlogloss:1.40646\n",
      "[24]\ttrain-mlogloss:1.14363\tvalid-mlogloss:1.40464\n",
      "[25]\ttrain-mlogloss:1.13433\tvalid-mlogloss:1.40303\n",
      "[26]\ttrain-mlogloss:1.12752\tvalid-mlogloss:1.40287\n",
      "[27]\ttrain-mlogloss:1.11914\tvalid-mlogloss:1.40240\n",
      "[28]\ttrain-mlogloss:1.11384\tvalid-mlogloss:1.40208\n",
      "[29]\ttrain-mlogloss:1.10470\tvalid-mlogloss:1.40009\n",
      "[30]\ttrain-mlogloss:1.09885\tvalid-mlogloss:1.40016\n",
      "[31]\ttrain-mlogloss:1.09338\tvalid-mlogloss:1.39965\n",
      "[32]\ttrain-mlogloss:1.08723\tvalid-mlogloss:1.39856\n",
      "[33]\ttrain-mlogloss:1.08268\tvalid-mlogloss:1.39827\n",
      "[34]\ttrain-mlogloss:1.07652\tvalid-mlogloss:1.39747\n",
      "[35]\ttrain-mlogloss:1.07117\tvalid-mlogloss:1.39704\n",
      "[36]\ttrain-mlogloss:1.06513\tvalid-mlogloss:1.39717\n",
      "[37]\ttrain-mlogloss:1.05829\tvalid-mlogloss:1.39657\n",
      "[38]\ttrain-mlogloss:1.05416\tvalid-mlogloss:1.39676\n",
      "[39]\ttrain-mlogloss:1.04959\tvalid-mlogloss:1.39676\n",
      "[40]\ttrain-mlogloss:1.04376\tvalid-mlogloss:1.39554\n",
      "[41]\ttrain-mlogloss:1.04051\tvalid-mlogloss:1.39540\n",
      "[42]\ttrain-mlogloss:1.03474\tvalid-mlogloss:1.39477\n",
      "[43]\ttrain-mlogloss:1.03099\tvalid-mlogloss:1.39494\n",
      "[44]\ttrain-mlogloss:1.02517\tvalid-mlogloss:1.39458\n",
      "[45]\ttrain-mlogloss:1.01865\tvalid-mlogloss:1.39380\n",
      "[46]\ttrain-mlogloss:1.01287\tvalid-mlogloss:1.39355\n",
      "[47]\ttrain-mlogloss:1.00871\tvalid-mlogloss:1.39440\n",
      "[48]\ttrain-mlogloss:1.00584\tvalid-mlogloss:1.39454\n",
      "[49]\ttrain-mlogloss:1.00113\tvalid-mlogloss:1.39439\n",
      "[50]\ttrain-mlogloss:0.99479\tvalid-mlogloss:1.39416\n",
      "[51]\ttrain-mlogloss:0.98882\tvalid-mlogloss:1.39491\n",
      "[52]\ttrain-mlogloss:0.98597\tvalid-mlogloss:1.39449\n",
      "[53]\ttrain-mlogloss:0.98299\tvalid-mlogloss:1.39509\n",
      "[54]\ttrain-mlogloss:0.97722\tvalid-mlogloss:1.39505\n",
      "[55]\ttrain-mlogloss:0.97258\tvalid-mlogloss:1.39511\n",
      "[56]\ttrain-mlogloss:0.96585\tvalid-mlogloss:1.39399\n",
      "[57]\ttrain-mlogloss:0.95816\tvalid-mlogloss:1.39327\n",
      "[58]\ttrain-mlogloss:0.95417\tvalid-mlogloss:1.39193\n",
      "[59]\ttrain-mlogloss:0.94829\tvalid-mlogloss:1.39179\n",
      "[60]\ttrain-mlogloss:0.94516\tvalid-mlogloss:1.39234\n",
      "[61]\ttrain-mlogloss:0.94123\tvalid-mlogloss:1.39246\n",
      "[62]\ttrain-mlogloss:0.93228\tvalid-mlogloss:1.39286\n",
      "[63]\ttrain-mlogloss:0.92881\tvalid-mlogloss:1.39216\n",
      "[64]\ttrain-mlogloss:0.92467\tvalid-mlogloss:1.39213\n",
      "[65]\ttrain-mlogloss:0.91807\tvalid-mlogloss:1.39140\n",
      "[66]\ttrain-mlogloss:0.91212\tvalid-mlogloss:1.39002\n",
      "[67]\ttrain-mlogloss:0.90746\tvalid-mlogloss:1.38926\n",
      "[68]\ttrain-mlogloss:0.90168\tvalid-mlogloss:1.38976\n",
      "[69]\ttrain-mlogloss:0.89580\tvalid-mlogloss:1.39011\n",
      "[70]\ttrain-mlogloss:0.89156\tvalid-mlogloss:1.39028\n",
      "[71]\ttrain-mlogloss:0.88784\tvalid-mlogloss:1.38983\n",
      "[72]\ttrain-mlogloss:0.88257\tvalid-mlogloss:1.38883\n",
      "[73]\ttrain-mlogloss:0.87810\tvalid-mlogloss:1.38864\n",
      "[74]\ttrain-mlogloss:0.87356\tvalid-mlogloss:1.38800\n",
      "[75]\ttrain-mlogloss:0.86962\tvalid-mlogloss:1.38808\n",
      "[76]\ttrain-mlogloss:0.86702\tvalid-mlogloss:1.38820\n",
      "[77]\ttrain-mlogloss:0.86236\tvalid-mlogloss:1.38838\n",
      "[78]\ttrain-mlogloss:0.85996\tvalid-mlogloss:1.38902\n",
      "[79]\ttrain-mlogloss:0.85722\tvalid-mlogloss:1.38920\n",
      "[80]\ttrain-mlogloss:0.85143\tvalid-mlogloss:1.38976\n",
      "[81]\ttrain-mlogloss:0.84823\tvalid-mlogloss:1.38980\n",
      "[82]\ttrain-mlogloss:0.84451\tvalid-mlogloss:1.38906\n",
      "[83]\ttrain-mlogloss:0.83934\tvalid-mlogloss:1.39036\n",
      "[84]\ttrain-mlogloss:0.83391\tvalid-mlogloss:1.39072\n",
      "[85]\ttrain-mlogloss:0.83076\tvalid-mlogloss:1.39099\n",
      "[86]\ttrain-mlogloss:0.82614\tvalid-mlogloss:1.39026\n",
      "[87]\ttrain-mlogloss:0.82196\tvalid-mlogloss:1.39015\n",
      "[88]\ttrain-mlogloss:0.81506\tvalid-mlogloss:1.38972\n",
      "[89]\ttrain-mlogloss:0.81087\tvalid-mlogloss:1.38965\n",
      "[90]\ttrain-mlogloss:0.80560\tvalid-mlogloss:1.38997\n",
      "[91]\ttrain-mlogloss:0.80287\tvalid-mlogloss:1.39008\n",
      "[92]\ttrain-mlogloss:0.80005\tvalid-mlogloss:1.39078\n",
      "[93]\ttrain-mlogloss:0.79645\tvalid-mlogloss:1.39067\n",
      "[94]\ttrain-mlogloss:0.79361\tvalid-mlogloss:1.39088\n",
      "[95]\ttrain-mlogloss:0.78828\tvalid-mlogloss:1.39030\n",
      "[96]\ttrain-mlogloss:0.78499\tvalid-mlogloss:1.38997\n",
      "[97]\ttrain-mlogloss:0.77943\tvalid-mlogloss:1.38971\n",
      "[98]\ttrain-mlogloss:0.77388\tvalid-mlogloss:1.38917\n",
      "[99]\ttrain-mlogloss:0.76981\tvalid-mlogloss:1.38828\n",
      "[100]\ttrain-mlogloss:0.76593\tvalid-mlogloss:1.38851\n",
      "[101]\ttrain-mlogloss:0.76141\tvalid-mlogloss:1.38939\n",
      "[102]\ttrain-mlogloss:0.75777\tvalid-mlogloss:1.38933\n",
      "[103]\ttrain-mlogloss:0.75406\tvalid-mlogloss:1.38953\n",
      "Test Accuracy: 0.4253075571177504\n",
      "Test F1: 0.4200506767324677\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ======================\n",
    "# 1. Daten laden\n",
    "# ======================\n",
    "train_df = pd.read_csv(\"Data/preprocessed/train_preprocessed.csv\")\n",
    "X_train = train_df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y_train = train_df[\"AdoptionSpeed\"].astype(int).values\n",
    "\n",
    "valid_X = pd.read_csv(\"Data/preprocessed/valid_preprocessed.csv\")\n",
    "valid_y = pd.read_csv(\"Data/preprocessed/valid_target.csv\")[\"AdoptionSpeed\"].astype(int).values\n",
    "\n",
    "test_X = pd.read_csv(\"Data/preprocessed/test_preprocessed.csv\")\n",
    "test_y = pd.read_csv(\"Data/preprocessed/test_target.csv\")[\"AdoptionSpeed\"].astype(int).values\n",
    "\n",
    "# ======================\n",
    "# 2. Object-Spalten entfernen\n",
    "# ======================\n",
    "object_cols = X_train.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "X_train_clean = X_train.drop(columns=object_cols)\n",
    "valid_X_clean = valid_X.drop(columns=object_cols)\n",
    "test_X_clean  = test_X.drop(columns=object_cols)\n",
    "\n",
    "# ======================\n",
    "# 3. RandomizedSearchCV für ausgewählte Hyperparameter\n",
    "# ======================\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=len(np.unique(y_train)),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"max_depth\": [3, 4, 5, 6, 7],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"n_estimators\": [100, 200, 300, 500]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_clean, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "\n",
    "# ======================\n",
    "# 4. Bestes Modell mit xgb.train + Early Stopping\n",
    "# ======================\n",
    "dtrain = xgb.DMatrix(X_train_clean, label=y_train)\n",
    "dvalid = xgb.DMatrix(valid_X_clean, label=valid_y)\n",
    "dtest  = xgb.DMatrix(test_X_clean)\n",
    "\n",
    "# Parameter vorbereiten (early_stopping_rounds wird hier nicht in params übergeben!)\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": len(np.unique(y_train)),\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"max_depth\": best_params[\"max_depth\"],\n",
    "    \"learning_rate\": best_params[\"learning_rate\"],\n",
    "    \"subsample\": best_params[\"subsample\"]\n",
    "}\n",
    "\n",
    "num_boost_round = best_params[\"n_estimators\"]\n",
    "early_stopping_rounds = 30  # Hier gezielt gesetzt\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    verbose_eval=True\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 5. Vorhersage & Evaluation\n",
    "# ======================\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "accuracy = accuracy_score(test_y, preds)\n",
    "f1 = f1_score(test_y, preds, average=\"weighted\")\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Test F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ad79b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faulh\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:28:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'subsample': 0.9, 'n_estimators': 300, 'max_depth': 14, 'learning_rate': 0.02}\n",
      "[0]\ttrain-mlogloss:1.59323\tvalid-mlogloss:1.60357\n",
      "[1]\ttrain-mlogloss:1.57779\tvalid-mlogloss:1.59795\n",
      "[2]\ttrain-mlogloss:1.56276\tvalid-mlogloss:1.59241\n",
      "[3]\ttrain-mlogloss:1.54771\tvalid-mlogloss:1.58688\n",
      "[4]\ttrain-mlogloss:1.53276\tvalid-mlogloss:1.58157\n",
      "[5]\ttrain-mlogloss:1.51821\tvalid-mlogloss:1.57625\n",
      "[6]\ttrain-mlogloss:1.50369\tvalid-mlogloss:1.57097\n",
      "[7]\ttrain-mlogloss:1.48932\tvalid-mlogloss:1.56607\n",
      "[8]\ttrain-mlogloss:1.47514\tvalid-mlogloss:1.56125\n",
      "[9]\ttrain-mlogloss:1.46216\tvalid-mlogloss:1.55653\n",
      "[10]\ttrain-mlogloss:1.44933\tvalid-mlogloss:1.55212\n",
      "[11]\ttrain-mlogloss:1.43587\tvalid-mlogloss:1.54774\n",
      "[12]\ttrain-mlogloss:1.42261\tvalid-mlogloss:1.54367\n",
      "[13]\ttrain-mlogloss:1.41057\tvalid-mlogloss:1.53984\n",
      "[14]\ttrain-mlogloss:1.39822\tvalid-mlogloss:1.53585\n",
      "[15]\ttrain-mlogloss:1.38595\tvalid-mlogloss:1.53193\n",
      "[16]\ttrain-mlogloss:1.37359\tvalid-mlogloss:1.52790\n",
      "[17]\ttrain-mlogloss:1.36150\tvalid-mlogloss:1.52386\n",
      "[18]\ttrain-mlogloss:1.34911\tvalid-mlogloss:1.51990\n",
      "[19]\ttrain-mlogloss:1.33829\tvalid-mlogloss:1.51632\n",
      "[20]\ttrain-mlogloss:1.32675\tvalid-mlogloss:1.51277\n",
      "[21]\ttrain-mlogloss:1.31543\tvalid-mlogloss:1.50913\n",
      "[22]\ttrain-mlogloss:1.30409\tvalid-mlogloss:1.50574\n",
      "[23]\ttrain-mlogloss:1.29276\tvalid-mlogloss:1.50263\n",
      "[24]\ttrain-mlogloss:1.28211\tvalid-mlogloss:1.49958\n",
      "[25]\ttrain-mlogloss:1.27159\tvalid-mlogloss:1.49637\n",
      "[26]\ttrain-mlogloss:1.26101\tvalid-mlogloss:1.49299\n",
      "[27]\ttrain-mlogloss:1.25061\tvalid-mlogloss:1.49012\n",
      "[28]\ttrain-mlogloss:1.24059\tvalid-mlogloss:1.48721\n",
      "[29]\ttrain-mlogloss:1.23043\tvalid-mlogloss:1.48451\n",
      "[30]\ttrain-mlogloss:1.22070\tvalid-mlogloss:1.48155\n",
      "[31]\ttrain-mlogloss:1.21060\tvalid-mlogloss:1.47908\n",
      "[32]\ttrain-mlogloss:1.20079\tvalid-mlogloss:1.47623\n",
      "[33]\ttrain-mlogloss:1.19105\tvalid-mlogloss:1.47361\n",
      "[34]\ttrain-mlogloss:1.18174\tvalid-mlogloss:1.47086\n",
      "[35]\ttrain-mlogloss:1.17262\tvalid-mlogloss:1.46842\n",
      "[36]\ttrain-mlogloss:1.16356\tvalid-mlogloss:1.46580\n",
      "[37]\ttrain-mlogloss:1.15453\tvalid-mlogloss:1.46354\n",
      "[38]\ttrain-mlogloss:1.14591\tvalid-mlogloss:1.46134\n",
      "[39]\ttrain-mlogloss:1.13725\tvalid-mlogloss:1.45882\n",
      "[40]\ttrain-mlogloss:1.12873\tvalid-mlogloss:1.45663\n",
      "[41]\ttrain-mlogloss:1.12020\tvalid-mlogloss:1.45442\n",
      "[42]\ttrain-mlogloss:1.11184\tvalid-mlogloss:1.45247\n",
      "[43]\ttrain-mlogloss:1.10367\tvalid-mlogloss:1.45036\n",
      "[44]\ttrain-mlogloss:1.09500\tvalid-mlogloss:1.44814\n",
      "[45]\ttrain-mlogloss:1.08672\tvalid-mlogloss:1.44621\n",
      "[46]\ttrain-mlogloss:1.07872\tvalid-mlogloss:1.44433\n",
      "[47]\ttrain-mlogloss:1.07079\tvalid-mlogloss:1.44240\n",
      "[48]\ttrain-mlogloss:1.06280\tvalid-mlogloss:1.44050\n",
      "[49]\ttrain-mlogloss:1.05454\tvalid-mlogloss:1.43878\n",
      "[50]\ttrain-mlogloss:1.04705\tvalid-mlogloss:1.43689\n",
      "[51]\ttrain-mlogloss:1.03942\tvalid-mlogloss:1.43512\n",
      "[52]\ttrain-mlogloss:1.03238\tvalid-mlogloss:1.43350\n",
      "[53]\ttrain-mlogloss:1.02513\tvalid-mlogloss:1.43214\n",
      "[54]\ttrain-mlogloss:1.01851\tvalid-mlogloss:1.43058\n",
      "[55]\ttrain-mlogloss:1.01125\tvalid-mlogloss:1.42887\n",
      "[56]\ttrain-mlogloss:1.00464\tvalid-mlogloss:1.42734\n",
      "[57]\ttrain-mlogloss:0.99789\tvalid-mlogloss:1.42572\n",
      "[58]\ttrain-mlogloss:0.99123\tvalid-mlogloss:1.42422\n",
      "[59]\ttrain-mlogloss:0.98394\tvalid-mlogloss:1.42252\n",
      "[60]\ttrain-mlogloss:0.97742\tvalid-mlogloss:1.42128\n",
      "[61]\ttrain-mlogloss:0.97077\tvalid-mlogloss:1.41984\n",
      "[62]\ttrain-mlogloss:0.96418\tvalid-mlogloss:1.41813\n",
      "[63]\ttrain-mlogloss:0.95783\tvalid-mlogloss:1.41675\n",
      "[64]\ttrain-mlogloss:0.95154\tvalid-mlogloss:1.41541\n",
      "[65]\ttrain-mlogloss:0.94546\tvalid-mlogloss:1.41427\n",
      "[66]\ttrain-mlogloss:0.93949\tvalid-mlogloss:1.41300\n",
      "[67]\ttrain-mlogloss:0.93341\tvalid-mlogloss:1.41174\n",
      "[68]\ttrain-mlogloss:0.92770\tvalid-mlogloss:1.41063\n",
      "[69]\ttrain-mlogloss:0.92157\tvalid-mlogloss:1.40927\n",
      "[70]\ttrain-mlogloss:0.91514\tvalid-mlogloss:1.40798\n",
      "[71]\ttrain-mlogloss:0.90939\tvalid-mlogloss:1.40671\n",
      "[72]\ttrain-mlogloss:0.90375\tvalid-mlogloss:1.40557\n",
      "[73]\ttrain-mlogloss:0.89843\tvalid-mlogloss:1.40448\n",
      "[74]\ttrain-mlogloss:0.89286\tvalid-mlogloss:1.40347\n",
      "[75]\ttrain-mlogloss:0.88721\tvalid-mlogloss:1.40242\n",
      "[76]\ttrain-mlogloss:0.88181\tvalid-mlogloss:1.40140\n",
      "[77]\ttrain-mlogloss:0.87618\tvalid-mlogloss:1.40037\n",
      "[78]\ttrain-mlogloss:0.87094\tvalid-mlogloss:1.39921\n",
      "[79]\ttrain-mlogloss:0.86557\tvalid-mlogloss:1.39803\n",
      "[80]\ttrain-mlogloss:0.86036\tvalid-mlogloss:1.39700\n",
      "[81]\ttrain-mlogloss:0.85516\tvalid-mlogloss:1.39605\n",
      "[82]\ttrain-mlogloss:0.84931\tvalid-mlogloss:1.39501\n",
      "[83]\ttrain-mlogloss:0.84376\tvalid-mlogloss:1.39390\n",
      "[84]\ttrain-mlogloss:0.83892\tvalid-mlogloss:1.39294\n",
      "[85]\ttrain-mlogloss:0.83406\tvalid-mlogloss:1.39224\n",
      "[86]\ttrain-mlogloss:0.82857\tvalid-mlogloss:1.39126\n",
      "[87]\ttrain-mlogloss:0.82313\tvalid-mlogloss:1.39070\n",
      "[88]\ttrain-mlogloss:0.81832\tvalid-mlogloss:1.38983\n",
      "[89]\ttrain-mlogloss:0.81345\tvalid-mlogloss:1.38890\n",
      "[90]\ttrain-mlogloss:0.80870\tvalid-mlogloss:1.38794\n",
      "[91]\ttrain-mlogloss:0.80409\tvalid-mlogloss:1.38684\n",
      "[92]\ttrain-mlogloss:0.79950\tvalid-mlogloss:1.38607\n",
      "[93]\ttrain-mlogloss:0.79468\tvalid-mlogloss:1.38523\n",
      "[94]\ttrain-mlogloss:0.78983\tvalid-mlogloss:1.38460\n",
      "[95]\ttrain-mlogloss:0.78509\tvalid-mlogloss:1.38367\n",
      "[96]\ttrain-mlogloss:0.78097\tvalid-mlogloss:1.38279\n",
      "[97]\ttrain-mlogloss:0.77664\tvalid-mlogloss:1.38201\n",
      "[98]\ttrain-mlogloss:0.77205\tvalid-mlogloss:1.38130\n",
      "[99]\ttrain-mlogloss:0.76801\tvalid-mlogloss:1.38060\n",
      "[100]\ttrain-mlogloss:0.76348\tvalid-mlogloss:1.38014\n",
      "[101]\ttrain-mlogloss:0.75901\tvalid-mlogloss:1.37944\n",
      "[102]\ttrain-mlogloss:0.75466\tvalid-mlogloss:1.37872\n",
      "[103]\ttrain-mlogloss:0.75044\tvalid-mlogloss:1.37806\n",
      "[104]\ttrain-mlogloss:0.74595\tvalid-mlogloss:1.37751\n",
      "[105]\ttrain-mlogloss:0.74152\tvalid-mlogloss:1.37692\n",
      "[106]\ttrain-mlogloss:0.73746\tvalid-mlogloss:1.37616\n",
      "[107]\ttrain-mlogloss:0.73312\tvalid-mlogloss:1.37553\n",
      "[108]\ttrain-mlogloss:0.72882\tvalid-mlogloss:1.37492\n",
      "[109]\ttrain-mlogloss:0.72493\tvalid-mlogloss:1.37414\n",
      "[110]\ttrain-mlogloss:0.72081\tvalid-mlogloss:1.37359\n",
      "[111]\ttrain-mlogloss:0.71688\tvalid-mlogloss:1.37297\n",
      "[112]\ttrain-mlogloss:0.71291\tvalid-mlogloss:1.37237\n",
      "[113]\ttrain-mlogloss:0.70897\tvalid-mlogloss:1.37176\n",
      "[114]\ttrain-mlogloss:0.70523\tvalid-mlogloss:1.37111\n",
      "[115]\ttrain-mlogloss:0.70155\tvalid-mlogloss:1.37065\n",
      "[116]\ttrain-mlogloss:0.69782\tvalid-mlogloss:1.37014\n",
      "[117]\ttrain-mlogloss:0.69383\tvalid-mlogloss:1.36957\n",
      "[118]\ttrain-mlogloss:0.69025\tvalid-mlogloss:1.36902\n",
      "[119]\ttrain-mlogloss:0.68675\tvalid-mlogloss:1.36861\n",
      "[120]\ttrain-mlogloss:0.68324\tvalid-mlogloss:1.36819\n",
      "[121]\ttrain-mlogloss:0.67989\tvalid-mlogloss:1.36776\n",
      "[122]\ttrain-mlogloss:0.67638\tvalid-mlogloss:1.36729\n",
      "[123]\ttrain-mlogloss:0.67282\tvalid-mlogloss:1.36679\n",
      "[124]\ttrain-mlogloss:0.66934\tvalid-mlogloss:1.36628\n",
      "[125]\ttrain-mlogloss:0.66621\tvalid-mlogloss:1.36566\n",
      "[126]\ttrain-mlogloss:0.66308\tvalid-mlogloss:1.36515\n",
      "[127]\ttrain-mlogloss:0.65951\tvalid-mlogloss:1.36472\n",
      "[128]\ttrain-mlogloss:0.65613\tvalid-mlogloss:1.36438\n",
      "[129]\ttrain-mlogloss:0.65306\tvalid-mlogloss:1.36382\n",
      "[130]\ttrain-mlogloss:0.65001\tvalid-mlogloss:1.36341\n",
      "[131]\ttrain-mlogloss:0.64681\tvalid-mlogloss:1.36316\n",
      "[132]\ttrain-mlogloss:0.64384\tvalid-mlogloss:1.36269\n",
      "[133]\ttrain-mlogloss:0.64041\tvalid-mlogloss:1.36232\n",
      "[134]\ttrain-mlogloss:0.63726\tvalid-mlogloss:1.36201\n",
      "[135]\ttrain-mlogloss:0.63419\tvalid-mlogloss:1.36151\n",
      "[136]\ttrain-mlogloss:0.63111\tvalid-mlogloss:1.36123\n",
      "[137]\ttrain-mlogloss:0.62767\tvalid-mlogloss:1.36091\n",
      "[138]\ttrain-mlogloss:0.62445\tvalid-mlogloss:1.36051\n",
      "[139]\ttrain-mlogloss:0.62173\tvalid-mlogloss:1.36017\n",
      "[140]\ttrain-mlogloss:0.61861\tvalid-mlogloss:1.35977\n",
      "[141]\ttrain-mlogloss:0.61528\tvalid-mlogloss:1.35936\n",
      "[142]\ttrain-mlogloss:0.61247\tvalid-mlogloss:1.35906\n",
      "[143]\ttrain-mlogloss:0.60916\tvalid-mlogloss:1.35852\n",
      "[144]\ttrain-mlogloss:0.60633\tvalid-mlogloss:1.35834\n",
      "[145]\ttrain-mlogloss:0.60358\tvalid-mlogloss:1.35794\n",
      "[146]\ttrain-mlogloss:0.60041\tvalid-mlogloss:1.35757\n",
      "[147]\ttrain-mlogloss:0.59759\tvalid-mlogloss:1.35725\n",
      "[148]\ttrain-mlogloss:0.59492\tvalid-mlogloss:1.35689\n",
      "[149]\ttrain-mlogloss:0.59202\tvalid-mlogloss:1.35648\n",
      "[150]\ttrain-mlogloss:0.58905\tvalid-mlogloss:1.35633\n",
      "[151]\ttrain-mlogloss:0.58620\tvalid-mlogloss:1.35608\n",
      "[152]\ttrain-mlogloss:0.58342\tvalid-mlogloss:1.35570\n",
      "[153]\ttrain-mlogloss:0.58093\tvalid-mlogloss:1.35555\n",
      "[154]\ttrain-mlogloss:0.57788\tvalid-mlogloss:1.35537\n",
      "[155]\ttrain-mlogloss:0.57554\tvalid-mlogloss:1.35525\n",
      "[156]\ttrain-mlogloss:0.57300\tvalid-mlogloss:1.35487\n",
      "[157]\ttrain-mlogloss:0.57056\tvalid-mlogloss:1.35459\n",
      "[158]\ttrain-mlogloss:0.56813\tvalid-mlogloss:1.35443\n",
      "[159]\ttrain-mlogloss:0.56558\tvalid-mlogloss:1.35422\n",
      "[160]\ttrain-mlogloss:0.56325\tvalid-mlogloss:1.35376\n",
      "[161]\ttrain-mlogloss:0.56076\tvalid-mlogloss:1.35367\n",
      "[162]\ttrain-mlogloss:0.55805\tvalid-mlogloss:1.35347\n",
      "[163]\ttrain-mlogloss:0.55573\tvalid-mlogloss:1.35327\n",
      "[164]\ttrain-mlogloss:0.55282\tvalid-mlogloss:1.35323\n",
      "[165]\ttrain-mlogloss:0.55056\tvalid-mlogloss:1.35299\n",
      "[166]\ttrain-mlogloss:0.54839\tvalid-mlogloss:1.35285\n",
      "[167]\ttrain-mlogloss:0.54595\tvalid-mlogloss:1.35264\n",
      "[168]\ttrain-mlogloss:0.54357\tvalid-mlogloss:1.35245\n",
      "[169]\ttrain-mlogloss:0.54135\tvalid-mlogloss:1.35241\n",
      "[170]\ttrain-mlogloss:0.53929\tvalid-mlogloss:1.35227\n",
      "[171]\ttrain-mlogloss:0.53663\tvalid-mlogloss:1.35209\n",
      "[172]\ttrain-mlogloss:0.53465\tvalid-mlogloss:1.35193\n",
      "[173]\ttrain-mlogloss:0.53236\tvalid-mlogloss:1.35171\n",
      "[174]\ttrain-mlogloss:0.52989\tvalid-mlogloss:1.35150\n",
      "[175]\ttrain-mlogloss:0.52794\tvalid-mlogloss:1.35153\n",
      "[176]\ttrain-mlogloss:0.52528\tvalid-mlogloss:1.35138\n",
      "[177]\ttrain-mlogloss:0.52309\tvalid-mlogloss:1.35106\n",
      "[178]\ttrain-mlogloss:0.52110\tvalid-mlogloss:1.35095\n",
      "[179]\ttrain-mlogloss:0.51901\tvalid-mlogloss:1.35080\n",
      "[180]\ttrain-mlogloss:0.51676\tvalid-mlogloss:1.35057\n",
      "[181]\ttrain-mlogloss:0.51490\tvalid-mlogloss:1.35034\n",
      "[182]\ttrain-mlogloss:0.51291\tvalid-mlogloss:1.35036\n",
      "[183]\ttrain-mlogloss:0.51054\tvalid-mlogloss:1.34999\n",
      "[184]\ttrain-mlogloss:0.50840\tvalid-mlogloss:1.34991\n",
      "[185]\ttrain-mlogloss:0.50650\tvalid-mlogloss:1.34972\n",
      "[186]\ttrain-mlogloss:0.50459\tvalid-mlogloss:1.34944\n",
      "[187]\ttrain-mlogloss:0.50260\tvalid-mlogloss:1.34937\n",
      "[188]\ttrain-mlogloss:0.50035\tvalid-mlogloss:1.34926\n",
      "[189]\ttrain-mlogloss:0.49849\tvalid-mlogloss:1.34911\n",
      "[190]\ttrain-mlogloss:0.49648\tvalid-mlogloss:1.34874\n",
      "[191]\ttrain-mlogloss:0.49437\tvalid-mlogloss:1.34859\n",
      "[192]\ttrain-mlogloss:0.49214\tvalid-mlogloss:1.34842\n",
      "[193]\ttrain-mlogloss:0.48997\tvalid-mlogloss:1.34834\n",
      "[194]\ttrain-mlogloss:0.48751\tvalid-mlogloss:1.34813\n",
      "[195]\ttrain-mlogloss:0.48559\tvalid-mlogloss:1.34813\n",
      "[196]\ttrain-mlogloss:0.48387\tvalid-mlogloss:1.34806\n",
      "[197]\ttrain-mlogloss:0.48181\tvalid-mlogloss:1.34788\n",
      "[198]\ttrain-mlogloss:0.47975\tvalid-mlogloss:1.34783\n",
      "[199]\ttrain-mlogloss:0.47790\tvalid-mlogloss:1.34760\n",
      "[200]\ttrain-mlogloss:0.47594\tvalid-mlogloss:1.34747\n",
      "[201]\ttrain-mlogloss:0.47420\tvalid-mlogloss:1.34733\n",
      "[202]\ttrain-mlogloss:0.47237\tvalid-mlogloss:1.34727\n",
      "[203]\ttrain-mlogloss:0.47040\tvalid-mlogloss:1.34725\n",
      "[204]\ttrain-mlogloss:0.46895\tvalid-mlogloss:1.34711\n",
      "[205]\ttrain-mlogloss:0.46724\tvalid-mlogloss:1.34709\n",
      "[206]\ttrain-mlogloss:0.46565\tvalid-mlogloss:1.34700\n",
      "[207]\ttrain-mlogloss:0.46375\tvalid-mlogloss:1.34694\n",
      "[208]\ttrain-mlogloss:0.46218\tvalid-mlogloss:1.34688\n",
      "[209]\ttrain-mlogloss:0.46040\tvalid-mlogloss:1.34689\n",
      "[210]\ttrain-mlogloss:0.45869\tvalid-mlogloss:1.34681\n",
      "[211]\ttrain-mlogloss:0.45682\tvalid-mlogloss:1.34680\n",
      "[212]\ttrain-mlogloss:0.45507\tvalid-mlogloss:1.34670\n",
      "[213]\ttrain-mlogloss:0.45335\tvalid-mlogloss:1.34654\n",
      "[214]\ttrain-mlogloss:0.45173\tvalid-mlogloss:1.34642\n",
      "[215]\ttrain-mlogloss:0.44971\tvalid-mlogloss:1.34632\n",
      "[216]\ttrain-mlogloss:0.44800\tvalid-mlogloss:1.34619\n",
      "[217]\ttrain-mlogloss:0.44644\tvalid-mlogloss:1.34624\n",
      "[218]\ttrain-mlogloss:0.44463\tvalid-mlogloss:1.34605\n",
      "[219]\ttrain-mlogloss:0.44287\tvalid-mlogloss:1.34601\n",
      "[220]\ttrain-mlogloss:0.44125\tvalid-mlogloss:1.34604\n",
      "[221]\ttrain-mlogloss:0.43957\tvalid-mlogloss:1.34600\n",
      "[222]\ttrain-mlogloss:0.43783\tvalid-mlogloss:1.34602\n",
      "[223]\ttrain-mlogloss:0.43637\tvalid-mlogloss:1.34605\n",
      "[224]\ttrain-mlogloss:0.43498\tvalid-mlogloss:1.34606\n",
      "[225]\ttrain-mlogloss:0.43353\tvalid-mlogloss:1.34607\n",
      "[226]\ttrain-mlogloss:0.43213\tvalid-mlogloss:1.34606\n",
      "[227]\ttrain-mlogloss:0.43101\tvalid-mlogloss:1.34619\n",
      "[228]\ttrain-mlogloss:0.42944\tvalid-mlogloss:1.34601\n",
      "[229]\ttrain-mlogloss:0.42798\tvalid-mlogloss:1.34586\n",
      "[230]\ttrain-mlogloss:0.42637\tvalid-mlogloss:1.34573\n",
      "[231]\ttrain-mlogloss:0.42465\tvalid-mlogloss:1.34570\n",
      "[232]\ttrain-mlogloss:0.42305\tvalid-mlogloss:1.34576\n",
      "[233]\ttrain-mlogloss:0.42134\tvalid-mlogloss:1.34578\n",
      "[234]\ttrain-mlogloss:0.41981\tvalid-mlogloss:1.34569\n",
      "[235]\ttrain-mlogloss:0.41842\tvalid-mlogloss:1.34568\n",
      "[236]\ttrain-mlogloss:0.41705\tvalid-mlogloss:1.34558\n",
      "[237]\ttrain-mlogloss:0.41576\tvalid-mlogloss:1.34562\n",
      "[238]\ttrain-mlogloss:0.41442\tvalid-mlogloss:1.34548\n",
      "[239]\ttrain-mlogloss:0.41312\tvalid-mlogloss:1.34538\n",
      "[240]\ttrain-mlogloss:0.41185\tvalid-mlogloss:1.34535\n",
      "[241]\ttrain-mlogloss:0.41048\tvalid-mlogloss:1.34514\n",
      "[242]\ttrain-mlogloss:0.40905\tvalid-mlogloss:1.34516\n",
      "[243]\ttrain-mlogloss:0.40778\tvalid-mlogloss:1.34512\n",
      "[244]\ttrain-mlogloss:0.40629\tvalid-mlogloss:1.34511\n",
      "[245]\ttrain-mlogloss:0.40497\tvalid-mlogloss:1.34512\n",
      "[246]\ttrain-mlogloss:0.40364\tvalid-mlogloss:1.34515\n",
      "[247]\ttrain-mlogloss:0.40219\tvalid-mlogloss:1.34508\n",
      "[248]\ttrain-mlogloss:0.40080\tvalid-mlogloss:1.34504\n",
      "[249]\ttrain-mlogloss:0.39941\tvalid-mlogloss:1.34514\n",
      "[250]\ttrain-mlogloss:0.39795\tvalid-mlogloss:1.34519\n",
      "[251]\ttrain-mlogloss:0.39661\tvalid-mlogloss:1.34512\n",
      "[252]\ttrain-mlogloss:0.39530\tvalid-mlogloss:1.34504\n",
      "[253]\ttrain-mlogloss:0.39415\tvalid-mlogloss:1.34500\n",
      "[254]\ttrain-mlogloss:0.39317\tvalid-mlogloss:1.34504\n",
      "[255]\ttrain-mlogloss:0.39171\tvalid-mlogloss:1.34528\n",
      "[256]\ttrain-mlogloss:0.39049\tvalid-mlogloss:1.34532\n",
      "[257]\ttrain-mlogloss:0.38925\tvalid-mlogloss:1.34527\n",
      "[258]\ttrain-mlogloss:0.38823\tvalid-mlogloss:1.34522\n",
      "[259]\ttrain-mlogloss:0.38706\tvalid-mlogloss:1.34520\n",
      "[260]\ttrain-mlogloss:0.38567\tvalid-mlogloss:1.34527\n",
      "[261]\ttrain-mlogloss:0.38435\tvalid-mlogloss:1.34520\n",
      "[262]\ttrain-mlogloss:0.38267\tvalid-mlogloss:1.34528\n",
      "[263]\ttrain-mlogloss:0.38152\tvalid-mlogloss:1.34516\n",
      "[264]\ttrain-mlogloss:0.38019\tvalid-mlogloss:1.34524\n",
      "[265]\ttrain-mlogloss:0.37878\tvalid-mlogloss:1.34530\n",
      "[266]\ttrain-mlogloss:0.37731\tvalid-mlogloss:1.34532\n",
      "[267]\ttrain-mlogloss:0.37604\tvalid-mlogloss:1.34537\n",
      "[268]\ttrain-mlogloss:0.37478\tvalid-mlogloss:1.34537\n",
      "[269]\ttrain-mlogloss:0.37353\tvalid-mlogloss:1.34544\n",
      "[270]\ttrain-mlogloss:0.37193\tvalid-mlogloss:1.34549\n",
      "[271]\ttrain-mlogloss:0.37064\tvalid-mlogloss:1.34551\n",
      "[272]\ttrain-mlogloss:0.36982\tvalid-mlogloss:1.34546\n",
      "[273]\ttrain-mlogloss:0.36859\tvalid-mlogloss:1.34550\n",
      "[274]\ttrain-mlogloss:0.36740\tvalid-mlogloss:1.34546\n",
      "[275]\ttrain-mlogloss:0.36630\tvalid-mlogloss:1.34552\n",
      "[276]\ttrain-mlogloss:0.36516\tvalid-mlogloss:1.34558\n",
      "[277]\ttrain-mlogloss:0.36419\tvalid-mlogloss:1.34552\n",
      "[278]\ttrain-mlogloss:0.36311\tvalid-mlogloss:1.34551\n",
      "[279]\ttrain-mlogloss:0.36195\tvalid-mlogloss:1.34561\n",
      "[280]\ttrain-mlogloss:0.36079\tvalid-mlogloss:1.34573\n",
      "[281]\ttrain-mlogloss:0.35927\tvalid-mlogloss:1.34560\n",
      "[282]\ttrain-mlogloss:0.35825\tvalid-mlogloss:1.34567\n",
      "Test Accuracy: 0.443585237258348\n",
      "Test F1: 0.43763269401585664\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ======================\n",
    "# 1. Daten laden\n",
    "# ======================\n",
    "train_df = pd.read_csv(\"Data/preprocessed/train_preprocessed.csv\")\n",
    "X_train = train_df.drop(columns=[\"AdoptionSpeed\"])\n",
    "y_train = train_df[\"AdoptionSpeed\"].astype(int).values\n",
    "\n",
    "valid_X = pd.read_csv(\"Data/preprocessed/valid_preprocessed.csv\")\n",
    "valid_y = pd.read_csv(\"Data/preprocessed/valid_target.csv\")[\"AdoptionSpeed\"].astype(int).values\n",
    "\n",
    "test_X = pd.read_csv(\"Data/preprocessed/test_preprocessed.csv\")\n",
    "test_y = pd.read_csv(\"Data/preprocessed/test_target.csv\")[\"AdoptionSpeed\"].astype(int).values\n",
    "\n",
    "# ======================\n",
    "# 2. Object-Spalten entfernen\n",
    "# ======================\n",
    "object_cols = X_train.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "X_train_clean = X_train.drop(columns=object_cols)\n",
    "valid_X_clean = valid_X.drop(columns=object_cols)\n",
    "test_X_clean  = test_X.drop(columns=object_cols)\n",
    "\n",
    "# ======================\n",
    "# 3. RandomizedSearchCV für ausgewählte Hyperparameter\n",
    "# ======================\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=len(np.unique(y_train)),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"max_depth\": [2, 4, 6, 8, 10, 12, 14, 16],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1, 0.09],\n",
    "    \"subsample\": [0.5, 0.7, 0.9, 1.0],\n",
    "    \"n_estimators\": [100, 300, 500, 800, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_clean, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "\n",
    "# ======================\n",
    "# 4. Bestes Modell mit xgb.train + Early Stopping\n",
    "# ======================\n",
    "dtrain = xgb.DMatrix(X_train_clean, label=y_train)\n",
    "dvalid = xgb.DMatrix(valid_X_clean, label=valid_y)\n",
    "dtest  = xgb.DMatrix(test_X_clean)\n",
    "\n",
    "# Parameter vorbereiten (early_stopping_rounds wird hier nicht in params übergeben!)\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": len(np.unique(y_train)),\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"max_depth\": best_params[\"max_depth\"],\n",
    "    \"learning_rate\": best_params[\"learning_rate\"],\n",
    "    \"subsample\": best_params[\"subsample\"]\n",
    "}\n",
    "\n",
    "num_boost_round = best_params[\"n_estimators\"]\n",
    "early_stopping_rounds = 30  # Hier gezielt gesetzt\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    verbose_eval=True\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 5. Vorhersage & Evaluation\n",
    "# ======================\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "accuracy = accuracy_score(test_y, preds)\n",
    "f1 = f1_score(test_y, preds, average=\"weighted\")\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Test F1:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
