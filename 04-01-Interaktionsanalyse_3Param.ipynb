{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e8c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74e35b6d",
   "metadata": {},
   "source": [
    "# Hypothese 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6cc14",
   "metadata": {},
   "source": [
    "## Hypothese 4.1 -- pre post Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965188e",
   "metadata": {},
   "source": [
    "### 3 Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250b692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: [0 1 2]\n",
      "Valid classes: [0 1 2]\n",
      "Test classes: [0 1 2]\n",
      "Original Training Set: (13275, 22)\n",
      "Oversampled Training Set: (21810, 22)\n",
      "Class distribution after oversampling: [7270 7270 7270]\n",
      "Final Training Set: (21810, 22), Validation Set: (2845, 22), Test Set: (2845, 22)\n",
      "Teste 180 Parameterkombinationen\n",
      "Starte Native XGBoost Grid Search...\n",
      "[  1/180] max_depth= 3, lr=0.100, subsample=1.0 → F1: 0.5517±0.012\n",
      "[  2/180] max_depth= 3, lr=0.100, subsample=0.9 → F1: 0.5539±0.010\n",
      "[  3/180] max_depth= 3, lr=0.100, subsample=0.8 → F1: 0.5555±0.010\n",
      "[  4/180] max_depth= 3, lr=0.100, subsample=0.7 → F1: 0.5573±0.010\n",
      "[  5/180] max_depth= 3, lr=0.100, subsample=0.6 → F1: 0.5556±0.010\n",
      "[  6/180] max_depth= 3, lr=0.100, subsample=0.5 → F1: 0.5577±0.012\n",
      "[  7/180] max_depth= 3, lr=0.070, subsample=1.0 → F1: 0.5396±0.013\n",
      "[  8/180] max_depth= 3, lr=0.070, subsample=0.9 → F1: 0.5421±0.012\n",
      "[  9/180] max_depth= 3, lr=0.070, subsample=0.8 → F1: 0.5404±0.012\n",
      "[ 10/180] max_depth= 3, lr=0.070, subsample=0.7 → F1: 0.5421±0.012\n",
      "[ 11/180] max_depth= 3, lr=0.070, subsample=0.6 → F1: 0.5421±0.011\n",
      "[ 12/180] max_depth= 3, lr=0.070, subsample=0.5 → F1: 0.5451±0.013\n",
      "[ 13/180] max_depth= 3, lr=0.050, subsample=1.0 → F1: 0.5232±0.009\n",
      "[ 14/180] max_depth= 3, lr=0.050, subsample=0.9 → F1: 0.5274±0.012\n",
      "[ 15/180] max_depth= 3, lr=0.050, subsample=0.8 → F1: 0.5290±0.012\n",
      "[ 16/180] max_depth= 3, lr=0.050, subsample=0.7 → F1: 0.5289±0.012\n",
      "[ 17/180] max_depth= 3, lr=0.050, subsample=0.6 → F1: 0.5302±0.011\n",
      "[ 18/180] max_depth= 3, lr=0.050, subsample=0.5 → F1: 0.5315±0.012\n",
      "[ 19/180] max_depth= 3, lr=0.030, subsample=1.0 → F1: 0.5087±0.010\n",
      "[ 20/180] max_depth= 3, lr=0.030, subsample=0.9 → F1: 0.5111±0.008\n",
      "[ 21/180] max_depth= 3, lr=0.030, subsample=0.8 → F1: 0.5107±0.008\n",
      "[ 22/180] max_depth= 3, lr=0.030, subsample=0.7 → F1: 0.5122±0.009\n",
      "[ 23/180] max_depth= 3, lr=0.030, subsample=0.6 → F1: 0.5117±0.010\n",
      "[ 24/180] max_depth= 3, lr=0.030, subsample=0.5 → F1: 0.5122±0.010\n",
      "[ 25/180] max_depth= 3, lr=0.010, subsample=1.0 → F1: 0.4850±0.012\n",
      "[ 26/180] max_depth= 3, lr=0.010, subsample=0.9 → F1: 0.4830±0.011\n",
      "[ 27/180] max_depth= 3, lr=0.010, subsample=0.8 → F1: 0.4832±0.011\n",
      "[ 28/180] max_depth= 3, lr=0.010, subsample=0.7 → F1: 0.4856±0.010\n",
      "[ 29/180] max_depth= 3, lr=0.010, subsample=0.6 → F1: 0.4840±0.009\n",
      "[ 30/180] max_depth= 3, lr=0.010, subsample=0.5 → F1: 0.4839±0.011\n",
      "[ 31/180] max_depth= 5, lr=0.100, subsample=1.0 → F1: 0.6293±0.006\n",
      "[ 32/180] max_depth= 5, lr=0.100, subsample=0.9 → F1: 0.6384±0.007\n",
      "[ 33/180] max_depth= 5, lr=0.100, subsample=0.8 → F1: 0.6406±0.007\n",
      "[ 34/180] max_depth= 5, lr=0.100, subsample=0.7 → F1: 0.6405±0.007\n",
      "[ 35/180] max_depth= 5, lr=0.100, subsample=0.6 → F1: 0.6410±0.008\n",
      "[ 36/180] max_depth= 5, lr=0.100, subsample=0.5 → F1: 0.6366±0.006\n",
      "[ 37/180] max_depth= 5, lr=0.070, subsample=1.0 → F1: 0.6073±0.010\n",
      "[ 38/180] max_depth= 5, lr=0.070, subsample=0.9 → F1: 0.6138±0.010\n",
      "[ 39/180] max_depth= 5, lr=0.070, subsample=0.8 → F1: 0.6168±0.006\n",
      "[ 40/180] max_depth= 5, lr=0.070, subsample=0.7 → F1: 0.6191±0.006\n",
      "[ 41/180] max_depth= 5, lr=0.070, subsample=0.6 → F1: 0.6161±0.009\n",
      "[ 42/180] max_depth= 5, lr=0.070, subsample=0.5 → F1: 0.6155±0.010\n",
      "[ 43/180] max_depth= 5, lr=0.050, subsample=1.0 → F1: 0.5883±0.012\n",
      "[ 44/180] max_depth= 5, lr=0.050, subsample=0.9 → F1: 0.5953±0.012\n",
      "[ 45/180] max_depth= 5, lr=0.050, subsample=0.8 → F1: 0.5977±0.010\n",
      "[ 46/180] max_depth= 5, lr=0.050, subsample=0.7 → F1: 0.6001±0.008\n",
      "[ 47/180] max_depth= 5, lr=0.050, subsample=0.6 → F1: 0.6005±0.006\n",
      "[ 48/180] max_depth= 5, lr=0.050, subsample=0.5 → F1: 0.6006±0.011\n",
      "[ 49/180] max_depth= 5, lr=0.030, subsample=1.0 → F1: 0.5637±0.008\n",
      "[ 50/180] max_depth= 5, lr=0.030, subsample=0.9 → F1: 0.5699±0.009\n",
      "[ 51/180] max_depth= 5, lr=0.030, subsample=0.8 → F1: 0.5727±0.011\n",
      "[ 52/180] max_depth= 5, lr=0.030, subsample=0.7 → F1: 0.5737±0.010\n",
      "[ 53/180] max_depth= 5, lr=0.030, subsample=0.6 → F1: 0.5768±0.011\n",
      "[ 54/180] max_depth= 5, lr=0.030, subsample=0.5 → F1: 0.5763±0.012\n",
      "[ 55/180] max_depth= 5, lr=0.010, subsample=1.0 → F1: 0.5279±0.012\n",
      "[ 56/180] max_depth= 5, lr=0.010, subsample=0.9 → F1: 0.5297±0.013\n",
      "[ 57/180] max_depth= 5, lr=0.010, subsample=0.8 → F1: 0.5313±0.011\n",
      "[ 58/180] max_depth= 5, lr=0.010, subsample=0.7 → F1: 0.5353±0.010\n",
      "[ 59/180] max_depth= 5, lr=0.010, subsample=0.6 → F1: 0.5402±0.010\n",
      "[ 60/180] max_depth= 5, lr=0.010, subsample=0.5 → F1: 0.5418±0.008\n",
      "[ 61/180] max_depth= 7, lr=0.100, subsample=1.0 → F1: 0.7143±0.005\n",
      "[ 62/180] max_depth= 7, lr=0.100, subsample=0.9 → F1: 0.7291±0.004\n",
      "[ 63/180] max_depth= 7, lr=0.100, subsample=0.8 → F1: 0.7272±0.005\n",
      "[ 64/180] max_depth= 7, lr=0.100, subsample=0.7 → F1: 0.7309±0.006\n",
      "[ 65/180] max_depth= 7, lr=0.100, subsample=0.6 → F1: 0.7285±0.005\n",
      "[ 66/180] max_depth= 7, lr=0.100, subsample=0.5 → F1: 0.7263±0.004\n",
      "[ 67/180] max_depth= 7, lr=0.070, subsample=1.0 → F1: 0.6870±0.006\n",
      "[ 68/180] max_depth= 7, lr=0.070, subsample=0.9 → F1: 0.6988±0.004\n",
      "[ 69/180] max_depth= 7, lr=0.070, subsample=0.8 → F1: 0.7029±0.004\n",
      "[ 70/180] max_depth= 7, lr=0.070, subsample=0.7 → F1: 0.7028±0.006\n",
      "[ 71/180] max_depth= 7, lr=0.070, subsample=0.6 → F1: 0.7033±0.004\n",
      "[ 72/180] max_depth= 7, lr=0.070, subsample=0.5 → F1: 0.6970±0.005\n",
      "[ 73/180] max_depth= 7, lr=0.050, subsample=1.0 → F1: 0.6635±0.005\n",
      "[ 74/180] max_depth= 7, lr=0.050, subsample=0.9 → F1: 0.6741±0.006\n",
      "[ 75/180] max_depth= 7, lr=0.050, subsample=0.8 → F1: 0.6778±0.005\n",
      "[ 76/180] max_depth= 7, lr=0.050, subsample=0.7 → F1: 0.6774±0.003\n",
      "[ 77/180] max_depth= 7, lr=0.050, subsample=0.6 → F1: 0.6776±0.003\n",
      "[ 78/180] max_depth= 7, lr=0.050, subsample=0.5 → F1: 0.6742±0.008\n",
      "[ 79/180] max_depth= 7, lr=0.030, subsample=1.0 → F1: 0.6260±0.007\n",
      "[ 80/180] max_depth= 7, lr=0.030, subsample=0.9 → F1: 0.6354±0.008\n",
      "[ 81/180] max_depth= 7, lr=0.030, subsample=0.8 → F1: 0.6415±0.006\n",
      "[ 82/180] max_depth= 7, lr=0.030, subsample=0.7 → F1: 0.6405±0.008\n",
      "[ 83/180] max_depth= 7, lr=0.030, subsample=0.6 → F1: 0.6438±0.006\n",
      "[ 84/180] max_depth= 7, lr=0.030, subsample=0.5 → F1: 0.6421±0.006\n",
      "[ 85/180] max_depth= 7, lr=0.010, subsample=1.0 → F1: 0.5851±0.011\n",
      "[ 86/180] max_depth= 7, lr=0.010, subsample=0.9 → F1: 0.5940±0.010\n",
      "[ 87/180] max_depth= 7, lr=0.010, subsample=0.8 → F1: 0.5986±0.008\n",
      "[ 88/180] max_depth= 7, lr=0.010, subsample=0.7 → F1: 0.5994±0.008\n",
      "[ 89/180] max_depth= 7, lr=0.010, subsample=0.6 → F1: 0.6023±0.008\n",
      "[ 90/180] max_depth= 7, lr=0.010, subsample=0.5 → F1: 0.6046±0.009\n",
      "[ 91/180] max_depth= 9, lr=0.100, subsample=1.0 → F1: 0.7859±0.005\n",
      "[ 92/180] max_depth= 9, lr=0.100, subsample=0.9 → F1: 0.8007±0.004\n",
      "[ 93/180] max_depth= 9, lr=0.100, subsample=0.8 → F1: 0.8017±0.003\n",
      "[ 94/180] max_depth= 9, lr=0.100, subsample=0.7 → F1: 0.8021±0.002\n",
      "[ 95/180] max_depth= 9, lr=0.100, subsample=0.6 → F1: 0.7974±0.002\n",
      "[ 96/180] max_depth= 9, lr=0.100, subsample=0.5 → F1: 0.7932±0.003\n",
      "[ 97/180] max_depth= 9, lr=0.070, subsample=1.0 → F1: 0.7612±0.004\n",
      "[ 98/180] max_depth= 9, lr=0.070, subsample=0.9 → F1: 0.7786±0.003\n",
      "[ 99/180] max_depth= 9, lr=0.070, subsample=0.8 "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Daten laden\n",
    "df = pd.read_csv(\"Data/preprocessed/combined_preprocessed.csv\")\n",
    "\n",
    "# Labels von 5 → 3 Klassen mappen\n",
    "def map_labels(x):\n",
    "    if x == 0:\n",
    "        return 0   # sehr schnell adoptiert\n",
    "    elif x == 4:\n",
    "        return 2   # gar nicht adoptiert\n",
    "    else:\n",
    "        return 1   # mittlere Geschwindigkeiten (1,2,3)\n",
    "\n",
    "df['target'] = df['AdoptionSpeed'].map(map_labels)\n",
    "\n",
    "# Features & Labels trennen\n",
    "X = df.drop(columns=['AdoptionSpeed', 'target'])\n",
    "y = df['target'].astype(int)\n",
    "\n",
    "# Stratified Split: Train / Temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Stratified Split: Temp → Valid / Test\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train classes:\", np.unique(y_train))\n",
    "print(\"Valid classes:\", np.unique(y_valid))\n",
    "print(\"Test classes:\", np.unique(y_test))\n",
    "\n",
    "# Kategorische Variablen in Kategorie-Typ umwandeln\n",
    "for df_ in [X_train, X_valid, X_test]:\n",
    "    for col in df_.select_dtypes(include=[\"object\"]).columns:\n",
    "        df_[col] = df_[col].astype(\"category\")\n",
    "\n",
    "# Oversampling nur auf Train\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original Training Set: {X_train.shape}\")\n",
    "print(f\"Oversampled Training Set: {X_train_res.shape}\")\n",
    "print(f\"Class distribution after oversampling: {np.bincount(y_train_res)}\")\n",
    "\n",
    "# Kategorische Spalten encodieren\n",
    "cat_cols = X_train_res.select_dtypes(include=[\"category\"]).columns\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    X_train_res[cat_cols] = oe.fit_transform(X_train_res[cat_cols])\n",
    "    X_valid[cat_cols] = oe.transform(X_valid[cat_cols])\n",
    "    X_test[cat_cols] = oe.transform(X_test[cat_cols])\n",
    "\n",
    "print(f\"Final Training Set: {X_train_res.shape}, Validation Set: {X_valid.shape}, Test Set: {X_test.shape}\")\n",
    "\n",
    "# HYPOTHESE 4: max_depth × learning_rate × subsample\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9, 11, 15],\n",
    "    'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01], \n",
    "    'subsample': [1.0, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "}\n",
    "\n",
    "total_combinations = len(param_grid['max_depth']) * len(param_grid['learning_rate']) * len(param_grid['subsample'])\n",
    "print(f\"Teste {total_combinations} Parameterkombinationen\")\n",
    "\n",
    "def evaluate_params_native(max_depth, learning_rate, subsample, X_train, y_train):\n",
    "    \"\"\"Evaluiert Parameter mit nativem XGBoost (ohne sklearn)\"\"\"\n",
    "    \n",
    "    # Manual 5-Fold Cross Validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        dtrain_fold = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dval_fold = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 3,\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'max_depth': max_depth,\n",
    "            'learning_rate': learning_rate,\n",
    "            'subsample': subsample,\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Train model\n",
    "        model = xgb.train(params, dtrain_fold, num_boost_round=100, verbose_eval=False)\n",
    "        \n",
    "        # Predict and calculate F1\n",
    "        y_pred = model.predict(dval_fold)\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample,\n",
    "        'f1_mean': np.mean(f1_scores),\n",
    "        'f1_std': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "# Kombiniere Training + Validation für Cross-Validation\n",
    "X_train_val = pd.concat([X_train_res, X_valid])\n",
    "y_train_val = pd.concat([y_train_res, y_valid])\n",
    "\n",
    "print(\"Starte Native XGBoost Grid Search...\")\n",
    "start_time = time.time()\n",
    "\n",
    "results = []\n",
    "for i, (max_depth, learning_rate, subsample) in enumerate(\n",
    "    product(param_grid['max_depth'], \n",
    "            param_grid['learning_rate'], \n",
    "            param_grid['subsample']), 1):\n",
    "    \n",
    "    print(f\"[{i:3d}/{total_combinations}] max_depth={max_depth:2d}, lr={learning_rate:.3f}, subsample={subsample:.1f}\", end=\" \")\n",
    "    \n",
    "    result = evaluate_params_native(max_depth, learning_rate, subsample, X_train_val, y_train_val)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"→ F1: {result['f1_mean']:.4f}±{result['f1_std']:.3f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Ergebnisse analysieren\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nNative XGBoost Grid Search abgeschlossen in {total_time/60:.1f} Minuten\")\n",
    "\n",
    "# Beste Konfiguration finden\n",
    "best_result = results_df.loc[results_df['f1_mean'].idxmax()]\n",
    "print(f\"Best parameters: {{'max_depth': {best_result['max_depth']}, 'learning_rate': {best_result['learning_rate']:.3f}, 'subsample': {best_result['subsample']:.1f}}}\")\n",
    "print(f\"Best CV score: {best_result['f1_mean']:.4f}\")\n",
    "\n",
    "# Bestes Modell auf Test Set evaluieren (native XGBoost)\n",
    "dtrain_final = xgb.DMatrix(X_train_val, label=y_train_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "best_params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'max_depth': int(best_result['max_depth']),\n",
    "    'learning_rate': best_result['learning_rate'],\n",
    "    'subsample': best_result['subsample'],\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "best_model = xgb.train(best_params, dtrain_final, num_boost_round=100, verbose_eval=False)\n",
    "y_test_pred = best_model.predict(dtest)\n",
    "\n",
    "# Test Metriken\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion Matrix (für 3 Klassen)\n",
    "labels_all = [0, 1, 2]\n",
    "cm = confusion_matrix(y_test, y_test_pred, labels=labels_all)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Sehr schnell (0)\", \"Mittel (1-3)\", \"Gar nicht (4)\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Best Model (3 Klassen)\")\n",
    "plt.show()\n",
    "\n",
    "# Top 10 Konfigurationen\n",
    "print(f\"\\nTop 10 Konfigurationen:\")\n",
    "print(\"=\"*90)\n",
    "top_10 = results_df.nlargest(10, 'f1_mean')\n",
    "\n",
    "for idx, row in top_10.iterrows():\n",
    "    print(f\"F1: {row['f1_mean']:.4f}±{row['f1_std']:.3f} | \"\n",
    "          f\"max_depth={int(row['max_depth']):2d}, lr={row['learning_rate']:.3f}, subsample={row['subsample']:.1f}\")\n",
    "\n",
    "# Klassen-Verteilung anzeigen\n",
    "print(f\"\\nKlassen-Verteilung im Test Set:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"Klasse {cls}: {count} Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28a94e",
   "metadata": {},
   "source": [
    "## Hypothese 4.2 -- post Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f610a",
   "metadata": {},
   "source": [
    "### 3 Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01097f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Daten laden (bereits preprocessed)\n",
    "print(\"Lade vorverarbeitete Daten...\")\n",
    "\n",
    "X_train_res = pd.read_csv(\"Data/preprocessed/X_train_res.csv\")\n",
    "y_train_res = pd.read_csv(\"Data/preprocessed/y_train_res.csv\").iloc[:, 0]\n",
    "\n",
    "X_valid = pd.read_csv(\"Data/preprocessed/X_valid_enc.csv\")\n",
    "y_valid = pd.read_csv(\"Data/preprocessed/y_valid.csv\").iloc[:, 0]\n",
    "\n",
    "X_test = pd.read_csv(\"Data/preprocessed/X_test_enc.csv\")\n",
    "y_test = pd.read_csv(\"Data/preprocessed/y_test.csv\").iloc[:, 0]\n",
    "\n",
    "print(f\"Training Set (oversampled): {X_train_res.shape}\")\n",
    "print(f\"Validation Set: {X_valid.shape}\")\n",
    "print(f\"Test Set: {X_test.shape}\")\n",
    "\n",
    "print(\"Train classes:\", np.unique(y_train_res))\n",
    "print(\"Valid classes:\", np.unique(y_valid))\n",
    "print(\"Test classes:\", np.unique(y_test))\n",
    "print(f\"Class distribution in training: {np.bincount(y_train_res)}\")\n",
    "\n",
    "# HYPOTHESE 4: max_depth × learning_rate × subsample\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9, 11, 15],\n",
    "    'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01], \n",
    "    'subsample': [1.0, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "}\n",
    "\n",
    "total_combinations = len(param_grid['max_depth']) * len(param_grid['learning_rate']) * len(param_grid['subsample'])\n",
    "print(f\"Teste {total_combinations} Parameterkombinationen\")\n",
    "\n",
    "def evaluate_params_native(max_depth, learning_rate, subsample, X_train, y_train):\n",
    "    \"\"\"Evaluiert Parameter mit nativem XGBoost (ohne sklearn)\"\"\"\n",
    "    \n",
    "    # Manual 3-Fold Cross Validation\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        dtrain_fold = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dval_fold = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 3,\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'max_depth': max_depth,\n",
    "            'learning_rate': learning_rate,\n",
    "            'subsample': subsample,\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Train model\n",
    "        model = xgb.train(params, dtrain_fold, num_boost_round=100, verbose_eval=False)\n",
    "        \n",
    "        # Predict and calculate F1\n",
    "        y_pred = model.predict(dval_fold)\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample,\n",
    "        'f1_mean': np.mean(f1_scores),\n",
    "        'f1_std': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "# Kombiniere Training + Validation für Cross-Validation\n",
    "X_train_val = pd.concat([X_train_res, X_valid])\n",
    "y_train_val = pd.concat([y_train_res, y_valid])\n",
    "\n",
    "print(\"Starte Native XGBoost Grid Search...\")\n",
    "start_time = time.time()\n",
    "\n",
    "results = []\n",
    "for i, (max_depth, learning_rate, subsample) in enumerate(\n",
    "    product(param_grid['max_depth'], \n",
    "            param_grid['learning_rate'], \n",
    "            param_grid['subsample']), 1):\n",
    "    \n",
    "    print(f\"[{i:3d}/{total_combinations}] max_depth={max_depth:2d}, lr={learning_rate:.3f}, subsample={subsample:.1f}\", end=\" \")\n",
    "    \n",
    "    result = evaluate_params_native(max_depth, learning_rate, subsample, X_train_val, y_train_val)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"→ F1: {result['f1_mean']:.4f}±{result['f1_std']:.3f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Ergebnisse analysieren\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nNative XGBoost Grid Search abgeschlossen in {total_time/60:.1f} Minuten\")\n",
    "\n",
    "# Beste Konfiguration finden\n",
    "best_result = results_df.loc[results_df['f1_mean'].idxmax()]\n",
    "print(f\"Best parameters: {{'max_depth': {best_result['max_depth']}, 'learning_rate': {best_result['learning_rate']:.3f}, 'subsample': {best_result['subsample']:.1f}}}\")\n",
    "print(f\"Best CV score: {best_result['f1_mean']:.4f}\")\n",
    "\n",
    "# Bestes Modell auf Test Set evaluieren (native XGBoost)\n",
    "dtrain_final = xgb.DMatrix(X_train_val, label=y_train_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "best_params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'max_depth': int(best_result['max_depth']),\n",
    "    'learning_rate': best_result['learning_rate'],\n",
    "    'subsample': best_result['subsample'],\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "best_model = xgb.train(best_params, dtrain_final, num_boost_round=100, verbose_eval=False)\n",
    "y_test_pred = best_model.predict(dtest)\n",
    "\n",
    "# Test Metriken\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion Matrix (für 3 Klassen)\n",
    "labels_all = [0, 1, 2]\n",
    "cm = confusion_matrix(y_test, y_test_pred, labels=labels_all)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Sehr schnell (0)\", \"Mittel (1-3)\", \"Gar nicht (4)\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Best Model (3 Klassen)\")\n",
    "plt.show()\n",
    "\n",
    "# Top 10 Konfigurationen\n",
    "print(f\"\\nTop 10 Konfigurationen:\")\n",
    "print(\"=\"*90)\n",
    "top_10 = results_df.nlargest(10, 'f1_mean')\n",
    "\n",
    "for idx, row in top_10.iterrows():\n",
    "    print(f\"F1: {row['f1_mean']:.4f}±{row['f1_std']:.3f} | \"\n",
    "          f\"max_depth={int(row['max_depth']):2d}, lr={row['learning_rate']:.3f}, subsample={row['subsample']:.1f}\")\n",
    "\n",
    "# Klassen-Verteilung anzeigen\n",
    "print(f\"\\nKlassen-Verteilung im Test Set:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    class_names = [\"Sehr schnell (0)\", \"Mittel (1-3)\", \"Gar nicht (4)\"]\n",
    "    print(f\"Klasse {cls} ({class_names[cls]}): {count} Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316de83a",
   "metadata": {},
   "source": [
    "### 5 Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174102a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Daten laden (bereits preprocessed)\n",
    "print(\"Lade vorverarbeitete Daten...\")\n",
    "\n",
    "X_train_res = pd.read_csv(\"Data/preprocessed/X_train_res.csv\")\n",
    "y_train_res = pd.read_csv(\"Data/preprocessed/y_train_res.csv\").iloc[:, 0]\n",
    "\n",
    "X_valid = pd.read_csv(\"Data/preprocessed/X_valid_enc.csv\")\n",
    "y_valid = pd.read_csv(\"Data/preprocessed/y_valid.csv\").iloc[:, 0]\n",
    "\n",
    "X_test = pd.read_csv(\"Data/preprocessed/X_test_enc.csv\")\n",
    "y_test = pd.read_csv(\"Data/preprocessed/y_test.csv\").iloc[:, 0]\n",
    "\n",
    "print(f\"Training Set (oversampled): {X_train_res.shape}\")\n",
    "print(f\"Validation Set: {X_valid.shape}\")\n",
    "print(f\"Test Set: {X_test.shape}\")\n",
    "\n",
    "print(\"Train classes:\", np.unique(y_train_res))\n",
    "print(\"Valid classes:\", np.unique(y_valid))\n",
    "print(\"Test classes:\", np.unique(y_test))\n",
    "print(f\"Class distribution in training: {np.bincount(y_train_res)}\")\n",
    "\n",
    "# HYPOTHESE 4: max_depth × learning_rate × subsample\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9, 11, 15],\n",
    "    'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01], \n",
    "    'subsample': [1.0, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "}\n",
    "\n",
    "total_combinations = len(param_grid['max_depth']) * len(param_grid['learning_rate']) * len(param_grid['subsample'])\n",
    "print(f\"Teste {total_combinations} Parameterkombinationen\")\n",
    "\n",
    "def evaluate_params_native(max_depth, learning_rate, subsample, X_train, y_train):\n",
    "    \"\"\"Evaluiert Parameter mit nativem XGBoost (ohne sklearn)\"\"\"\n",
    "    \n",
    "    # Manual 3-Fold Cross Validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        dtrain_fold = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dval_fold = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 3,\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'max_depth': max_depth,\n",
    "            'learning_rate': learning_rate,\n",
    "            'subsample': subsample,\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Train model\n",
    "        model = xgb.train(params, dtrain_fold, num_boost_round=100, verbose_eval=False)\n",
    "        \n",
    "        # Predict and calculate F1\n",
    "        y_pred = model.predict(dval_fold)\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample,\n",
    "        'f1_mean': np.mean(f1_scores),\n",
    "        'f1_std': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "# Kombiniere Training + Validation für Cross-Validation\n",
    "X_train_val = pd.concat([X_train_res, X_valid])\n",
    "y_train_val = pd.concat([y_train_res, y_valid])\n",
    "\n",
    "print(\"Starte Native XGBoost Grid Search...\")\n",
    "start_time = time.time()\n",
    "\n",
    "results = []\n",
    "for i, (max_depth, learning_rate, subsample) in enumerate(\n",
    "    product(param_grid['max_depth'], \n",
    "            param_grid['learning_rate'], \n",
    "            param_grid['subsample']), 1):\n",
    "    \n",
    "    print(f\"[{i:3d}/{total_combinations}] max_depth={max_depth:2d}, lr={learning_rate:.3f}, subsample={subsample:.1f}\", end=\" \")\n",
    "    \n",
    "    result = evaluate_params_native(max_depth, learning_rate, subsample, X_train_val, y_train_val)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"→ F1: {result['f1_mean']:.4f}±{result['f1_std']:.3f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Ergebnisse analysieren\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nNative XGBoost Grid Search abgeschlossen in {total_time/60:.1f} Minuten\")\n",
    "\n",
    "# Beste Konfiguration finden\n",
    "best_result = results_df.loc[results_df['f1_mean'].idxmax()]\n",
    "print(f\"Best parameters: {{'max_depth': {best_result['max_depth']}, 'learning_rate': {best_result['learning_rate']:.3f}, 'subsample': {best_result['subsample']:.1f}}}\")\n",
    "print(f\"Best CV score: {best_result['f1_mean']:.4f}\")\n",
    "\n",
    "# Bestes Modell auf Test Set evaluieren (native XGBoost)\n",
    "dtrain_final = xgb.DMatrix(X_train_val, label=y_train_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "best_params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'max_depth': int(best_result['max_depth']),\n",
    "    'learning_rate': best_result['learning_rate'],\n",
    "    'subsample': best_result['subsample'],\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "best_model = xgb.train(best_params, dtrain_final, num_boost_round=100, verbose_eval=False)\n",
    "y_test_pred = best_model.predict(dtest)\n",
    "\n",
    "# Test Metriken\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion Matrix (für 3 Klassen)\n",
    "labels_all = [0, 1, 2]\n",
    "cm = confusion_matrix(y_test, y_test_pred, labels=labels_all)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Sehr schnell (0)\", \"Mittel (1-3)\", \"Gar nicht (4)\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Best Model (3 Klassen)\")\n",
    "plt.show()\n",
    "\n",
    "# Top 10 Konfigurationen\n",
    "print(f\"\\nTop 10 Konfigurationen:\")\n",
    "print(\"=\"*90)\n",
    "top_10 = results_df.nlargest(10, 'f1_mean')\n",
    "\n",
    "for idx, row in top_10.iterrows():\n",
    "    print(f\"F1: {row['f1_mean']:.4f}±{row['f1_std']:.3f} | \"\n",
    "          f\"max_depth={int(row['max_depth']):2d}, lr={row['learning_rate']:.3f}, subsample={row['subsample']:.1f}\")\n",
    "\n",
    "# Klassen-Verteilung anzeigen\n",
    "print(f\"\\nKlassen-Verteilung im Test Set:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    class_names = [\"Sehr schnell (0)\", \"Mittel (1-3)\", \"Gar nicht (4)\"]\n",
    "    print(f\"Klasse {cls} ({class_names[cls]}): {count} Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d2adc",
   "metadata": {},
   "source": [
    "### 10 Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62d795",
   "metadata": {},
   "source": [
    "# Hypothese 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ff590",
   "metadata": {},
   "source": [
    "## Hypothese 5 pre oversamling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Daten laden\n",
    "df = pd.read_csv(\"Data/preprocessed/combined_preprocessed.csv\")\n",
    "\n",
    "# Labels von 5 → 3 Klassen mappen\n",
    "def map_labels(x):\n",
    "    if x == 0:\n",
    "        return 0   # sehr schnell adoptiert\n",
    "    elif x == 4:\n",
    "        return 2   # gar nicht adoptiert\n",
    "    else:\n",
    "        return 1   # mittlere Geschwindigkeiten (1,2,3)\n",
    "\n",
    "df['target'] = df['AdoptionSpeed'].map(map_labels)\n",
    "\n",
    "# Features & Labels trennen\n",
    "X = df.drop(columns=['AdoptionSpeed', 'target'])\n",
    "y = df['target'].astype(int)\n",
    "\n",
    "# Stratified Split: Train / Temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Stratified Split: Temp → Valid / Test\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train classes:\", np.unique(y_train))\n",
    "print(\"Valid classes:\", np.unique(y_valid))\n",
    "print(\"Test classes:\", np.unique(y_test))\n",
    "\n",
    "# Kategorische Variablen in Kategorie-Typ umwandeln\n",
    "for df_ in [X_train, X_valid, X_test]:\n",
    "    for col in df_.select_dtypes(include=[\"object\"]).columns:\n",
    "        df_[col] = df_[col].astype(\"category\")\n",
    "\n",
    "# Oversampling nur auf Train\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original Training Set: {X_train.shape}\")\n",
    "print(f\"Oversampled Training Set: {X_train_res.shape}\")\n",
    "print(f\"Class distribution after oversampling: {np.bincount(y_train_res)}\")\n",
    "\n",
    "# Kategorische Spalten encodieren\n",
    "cat_cols = X_train_res.select_dtypes(include=[\"category\"]).columns\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    X_train_res[cat_cols] = oe.fit_transform(X_train_res[cat_cols])\n",
    "    X_valid[cat_cols] = oe.transform(X_valid[cat_cols])\n",
    "    X_test[cat_cols] = oe.transform(X_test[cat_cols])\n",
    "\n",
    "print(f\"Final Training Set: {X_train_res.shape}, Validation Set: {X_valid.shape}, Test Set: {X_test.shape}\")\n",
    "\n",
    "# HYPOTHESE 5: n_estimators × max_depth × subsample\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    'max_depth': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
    "    'subsample': [1.0, 0.8, 0.6, 0.4]\n",
    "}\n",
    "\n",
    "total_combinations = len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['subsample'])\n",
    "print(f\"Teste {total_combinations} Parameterkombinationen\")\n",
    "\n",
    "def evaluate_with_native_xgb(n_estimators, max_depth, subsample, X_data, y_data):\n",
    "    \"\"\"Native XGBoost ohne sklearn dependencies\"\"\"\n",
    "    \n",
    "    # Manual 3-Fold Cross Validation\n",
    "    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(X_data, y_data):\n",
    "        X_tr = X_data.iloc[train_idx]\n",
    "        X_val = X_data.iloc[val_idx] \n",
    "        y_tr = y_data.iloc[train_idx]\n",
    "        y_val = y_data.iloc[val_idx]\n",
    "        \n",
    "        # Native XGBoost Training\n",
    "        dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 3,\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'max_depth': max_depth,\n",
    "            'subsample': subsample,\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        model = xgb.train(params, dtrain, num_boost_round=n_estimators, verbose_eval=False)\n",
    "        predictions = model.predict(dval)\n",
    "        \n",
    "        f1 = f1_score(y_val, predictions, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'subsample': subsample,\n",
    "        'f1_mean': np.mean(f1_scores),\n",
    "        'f1_std': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "# Kombiniere Daten\n",
    "X_combined = pd.concat([X_train_res, X_valid])\n",
    "y_combined = pd.concat([y_train_res, y_valid])\n",
    "\n",
    "print(\"Starte Native XGBoost Grid Search...\")\n",
    "start_time = time.time()\n",
    "\n",
    "all_results = []\n",
    "for i, (n_est, max_d, sub) in enumerate(\n",
    "    product(param_grid['n_estimators'], \n",
    "            param_grid['max_depth'], \n",
    "            param_grid['subsample']), 1):\n",
    "    \n",
    "    print(f\"[{i:4d}/{total_combinations}] n_est={n_est:4d}, max_depth={max_d:2d}, subsample={sub:.1f}\", end=\" \")\n",
    "    \n",
    "    result = evaluate_with_native_xgb(n_est, max_d, sub, X_combined, y_combined)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"-> F1: {result['f1_mean']:.4f}±{result['f1_std']:.3f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nNative Grid Search abgeschlossen in {total_time/60:.1f} Minuten\")\n",
    "\n",
    "# Ergebnisse analysieren  \n",
    "results_df = pd.DataFrame(all_results)\n",
    "best_config = results_df.loc[results_df['f1_mean'].idxmax()]\n",
    "\n",
    "print(f\"Best parameters: {{'n_estimators': {best_config['n_estimators']}, 'max_depth': {best_config['max_depth']}, 'subsample': {best_config['subsample']:.1f}}}\")\n",
    "print(f\"Best CV score: {best_config['f1_mean']:.4f}\")\n",
    "\n",
    "# Finales Modell auf Test Set\n",
    "dtrain_final = xgb.DMatrix(X_combined, label=y_combined)\n",
    "dtest_final = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "final_params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'max_depth': int(best_config['max_depth']),\n",
    "    'subsample': best_config['subsample'],\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "final_model = xgb.train(final_params, dtrain_final, \n",
    "                       num_boost_round=int(best_config['n_estimators']), \n",
    "                       verbose_eval=False)\n",
    "\n",
    "y_final_pred = final_model.predict(dtest_final)\n",
    "\n",
    "# Test Metriken\n",
    "test_acc = accuracy_score(y_test, y_final_pred)\n",
    "test_f1 = f1_score(y_test, y_final_pred, average='weighted')\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_final_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_final_pred, labels=[0, 1, 2, 3, 4])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Klasse 0\", \"Klasse 1\", \"Klasse 2\", \"Klasse 3\", \"Klasse 4\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Native XGBoost Best Model (5 Klassen)\")\n",
    "plt.show()\n",
    "\n",
    "# Top 10\n",
    "print(f\"\\nTop 10 Konfigurationen:\")\n",
    "print(\"=\"*100)\n",
    "top_10_configs = results_df.nlargest(10, 'f1_mean')\n",
    "\n",
    "for idx, row in top_10_configs.iterrows():\n",
    "    print(f\"F1: {row['f1_mean']:.4f}±{row['f1_std']:.3f} | \"\n",
    "          f\"n_estimators={int(row['n_estimators']):4d}, max_depth={int(row['max_depth']):2d}, subsample={row['subsample']:.1f}\")\n",
    "\n",
    "# Klassen-Verteilung\n",
    "print(f\"\\nKlassen-Verteilung im Test Set:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "class_names = [\"Sehr schnell (0)\", \"Mittel (1-3)\", \"Gar nicht (4)\"]\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"Klasse {cls} ({class_names[cls]}): {count} Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04bd25",
   "metadata": {},
   "source": [
    "### 3 Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a760a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Daten laden\n",
    "df = pd.read_csv(\"Data/preprocessed/combined_preprocessed.csv\")\n",
    "\n",
    "# Labels von 5 → 3 Klassen mappen\n",
    "def map_labels(x):\n",
    "    if x == 0:\n",
    "        return 0   # sehr schnell adoptiert\n",
    "    elif x == 4:\n",
    "        return 2   # gar nicht adoptiert\n",
    "    else:\n",
    "        return 1   # mittlere Geschwindigkeiten (1,2,3)\n",
    "\n",
    "df['target'] = df['AdoptionSpeed'].map(map_labels)\n",
    "\n",
    "# Features & Labels trennen\n",
    "X = df.drop(columns=['AdoptionSpeed', 'target'])\n",
    "y = df['target'].astype(int)\n",
    "\n",
    "# Stratified Split: Train / Temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Stratified Split: Temp → Valid / Test\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train classes:\", np.unique(y_train))\n",
    "print(\"Valid classes:\", np.unique(y_valid))\n",
    "print(\"Test classes:\", np.unique(y_test))\n",
    "\n",
    "# Kategorische Variablen in Kategorie-Typ umwandeln\n",
    "for df_ in [X_train, X_valid, X_test]:\n",
    "    for col in df_.select_dtypes(include=[\"object\"]).columns:\n",
    "        df_[col] = df_[col].astype(\"category\")\n",
    "\n",
    "# Oversampling nur auf Train\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original Training Set: {X_train.shape}\")\n",
    "print(f\"Oversampled Training Set: {X_train_res.shape}\")\n",
    "print(f\"Class distribution after oversampling: {np.bincount(y_train_res)}\")\n",
    "\n",
    "# Kategorische Spalten encodieren\n",
    "cat_cols = X_train_res.select_dtypes(include=[\"category\"]).columns\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    X_train_res[cat_cols] = oe.fit_transform(X_train_res[cat_cols])\n",
    "    X_valid[cat_cols] = oe.transform(X_valid[cat_cols])\n",
    "    X_test[cat_cols] = oe.transform(X_test[cat_cols])\n",
    "\n",
    "print(f\"Final Training Set: {X_train_res.shape}, Validation Set: {X_valid.shape}, Test Set: {X_test.shape}\")\n",
    "\n",
    "# HYPOTHESE 5: n_estimators × max_depth × subsample\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500, 600, 700],\n",
    "    'max_depth': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "    'subsample': [1.0, 0.8, 0.6, 0.4]\n",
    "}\n",
    "\n",
    "total_combinations = len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['subsample'])\n",
    "print(f\"Teste {total_combinations} Parameterkombinationen\")\n",
    "\n",
    "def evaluate_with_native_xgb(n_estimators, max_depth, subsample, X_data, y_data):\n",
    "    \"\"\"Native XGBoost ohne sklearn dependencies\"\"\"\n",
    "    \n",
    "    # Manual 3-Fold Cross Validation\n",
    "    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(X_data, y_data):\n",
    "        X_tr = X_data.iloc[train_idx]\n",
    "        X_val = X_data.iloc[val_idx] \n",
    "        y_tr = y_data.iloc[train_idx]\n",
    "        y_val = y_data.iloc[val_idx]\n",
    "        \n",
    "        # Native XGBoost Training\n",
    "        dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 3,\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'max_depth': max_depth,\n",
    "            'subsample': subsample,\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        model = xgb.train(params, dtrain, num_boost_round=n_estimators, verbose_eval=False)\n",
    "        predictions = model.predict(dval)\n",
    "        \n",
    "        f1 = f1_score(y_val, predictions, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'subsample': subsample,\n",
    "        'f1_mean': np.mean(f1_scores),\n",
    "        'f1_std': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "# Kombiniere Daten\n",
    "X_combined = pd.concat([X_train_res, X_valid])\n",
    "y_combined = pd.concat([y_train_res, y_valid])\n",
    "\n",
    "print(\"Starte Native XGBoost Grid Search...\")\n",
    "start_time = time.time()\n",
    "\n",
    "all_results = []\n",
    "for i, (n_est, max_d, sub) in enumerate(\n",
    "    product(param_grid['n_estimators'], \n",
    "            param_grid['max_depth'], \n",
    "            param_grid['subsample']), 1):\n",
    "    \n",
    "    print(f\"[{i:4d}/{total_combinations}] n_est={n_est:4d}, max_depth={max_d:2d}, subsample={sub:.1f}\", end=\" \")\n",
    "    \n",
    "    result = evaluate_with_native_xgb(n_est, max_d, sub, X_combined, y_combined)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"-> F1: {result['f1_mean']:.4f}±{result['f1_std']:.3f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nNative Grid Search abgeschlossen in {total_time/60:.1f} Minuten\")\n",
    "\n",
    "# Ergebnisse analysieren  \n",
    "results_df = pd.DataFrame(all_results)\n",
    "best_config = results_df.loc[results_df['f1_mean'].idxmax()]\n",
    "\n",
    "print(f\"Best parameters: {{'n_estimators': {best_config['n_estimators']}, 'max_depth': {best_config['max_depth']}, 'subsample': {best_config['subsample']:.1f}}}\")\n",
    "print(f\"Best CV score: {best_config['f1_mean']:.4f}\")\n",
    "\n",
    "# Finales Modell auf Test Set\n",
    "dtrain_final = xgb.DMatrix(X_combined, label=y_combined)\n",
    "dtest_final = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "final_params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'max_depth': int(best_config['max_depth']),\n",
    "    'subsample': best_config['subsample'],\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "final_model = xgb.train(final_params, dtrain_final, \n",
    "                       num_boost_round=int(best_config['n_estimators']), \n",
    "                       verbose_eval=False)\n",
    "\n",
    "y_final_pred = final_model.predict(dtest_final)\n",
    "\n",
    "# Test Metriken\n",
    "test_acc = accuracy_score(y_test, y_final_pred)\n",
    "test_f1 = f1_score(y_test, y_final_pred, average='weighted')\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_final_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_final_pred, labels=[0, 1, 2])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Sehr schnell (0)\", \"Mittel (1-3)\", \"Gar nicht (4)\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Native XGBoost Best Model\")\n",
    "plt.show()\n",
    "\n",
    "# Top 10\n",
    "print(f\"\\nTop 10 Konfigurationen:\")\n",
    "print(\"=\"*100)\n",
    "top_10_configs = results_df.nlargest(10, 'f1_mean')\n",
    "\n",
    "for idx, row in top_10_configs.iterrows():\n",
    "    print(f\"F1: {row['f1_mean']:.4f}±{row['f1_std']:.3f} | \"\n",
    "          f\"n_estimators={int(row['n_estimators']):4d}, max_depth={int(row['max_depth']):2d}, subsample={row['subsample']:.1f}\")\n",
    "\n",
    "# Klassen-Verteilung\n",
    "print(f\"\\nKlassen-Verteilung im Test Set:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "class_names = [\"Sehr schnell (0)\", \"Mittel (1-3)\", \"Gar nicht (4)\"]\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"Klasse {cls} ({class_names[cls]}): {count} Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538a524",
   "metadata": {},
   "source": [
    "## Hypothese 5 post oversamling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ca3d3",
   "metadata": {},
   "source": [
    "### 3 Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92afe989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
